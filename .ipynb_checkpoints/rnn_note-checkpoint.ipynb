{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from core.gain import *\n",
    "from core.rnn_predic import *\n",
    "from core.models import *\n",
    "from core.util import *\n",
    "#from core.window import WindowGenerator, MissData, make_dataset_water, WaterDataGenerator\n",
    "from core.window import WindowGenerator, make_dataset_gain, make_dataset_water\n",
    "from core.file_open import make_dataframe\n",
    "from core.miss_data import MissData\n",
    "import json\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "parameters_dir = 'input'\n",
    "\n",
    "parameters_file = 'input.json'\n",
    "parameters_path = '{dir}/{file}'.format(dir=parameters_dir, file=parameters_file)\n",
    "\n",
    "with open(parameters_path, encoding='utf8') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "gain_parameters = parameters['gain']\n",
    "rnn_parameters = parameters['rnn']\n",
    "file_parameters = parameters['file']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input json name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_parameters['watershed'] = 'yeongsan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_path = parameters_dir+'/'+ file_parameters['watershed'] + '.json'\n",
    "with open(parameters_path, encoding='utf8') as json_file:\n",
    "    parameters = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parameters = parameters['data']\n",
    "\n",
    "interpolation_option = data_parameters['interpolation']\n",
    "colum_idx = data_parameters['columns']\n",
    "watershed = data_parameters['watershed']\n",
    "file_names = data_parameters['files']\n",
    "folder = data_parameters['directorys']\n",
    "for i in range(len(folder)):\n",
    "    folder[i] = watershed+folder[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "__GAIN_TRAINING__ = gain_parameters['train']\n",
    "gain_epochs = gain_parameters['max_epochs']\n",
    "gain_in_setps = gain_parameters['input_width']\n",
    "gain_out_setps = gain_parameters['label_width']\n",
    "gain_batch_size = gain_parameters['batch_size']\n",
    "gain_fill_no = gain_parameters['fill_width']\n",
    "gain_shift = gain_parameters['shift_width']\n",
    "gain_miss_rate = gain_parameters['miss_rate']\n",
    "\n",
    "__RNN_TRAINING__ = rnn_parameters['train']\n",
    "rnn_epochs = rnn_parameters['max_epochs']\n",
    "rnn_in_setps = rnn_parameters['input_width']\n",
    "rnn_out_steps = rnn_parameters['label_width']\n",
    "rnn_batch_size = rnn_parameters['batch_size']\n",
    "rnn_predict_day = rnn_parameters['predict_day']\n",
    "rnn_target_column = rnn_parameters['target_column']\n",
    "\n",
    "if rnn_predict_day < 3 or rnn_predict_day >5:\n",
    "    print('predict_day err')\n",
    "    exit(88)\n",
    "rnn_predict_day -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gain_epochs , rnn_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_num = range(len(folder))\n",
    "\n",
    "\n",
    "run_num = [0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "real_df_all = pd.DataFrame([])\n",
    "target_all = target_mean = target_std = 0\n",
    "\n",
    "gain_val_performance = {}\n",
    "gain_performance = {}\n",
    "\n",
    "length = len(run_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain Training Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "__GAIN_TRAINING__ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpol flag :  [False, False]\n",
      "folder :  data/yeong/자동/\n",
      "colum_idx :  :,[26,27,28,29,30,31,32,33]\n",
      "file_names[idx] :  [['나주_2015.xlsx', '나주_2016.xlsx', '나주_2017.xlsx', '나주_2018.xlsx', '나주_2019.xlsx', '나주_2020.xlsx'], ['지석천_2015.xlsx', '지석천_2016.xlsx', '지석천_2017.xlsx', '지석천_2018.xlsx', '지석천_2019.xlsx', '지석천_2020.xlsx'], ['용봉_2015.xlsx', '용봉_2016.xlsx', '용봉_2017.xlsx', '용봉_2018.xlsx', '용봉_2019.xlsx', '용봉_2020.xlsx'], ['우치_2015.xlsx', '우치_2016.xlsx', '우치_2017.xlsx', '우치_2018.xlsx', '우치_2019.xlsx', '우치_2020.xlsx']]\n",
      "data/yeong/자동/나주_2015.xlsx\n",
      "data/yeong/자동/나주_2016.xlsx\n",
      "data/yeong/자동/나주_2017.xlsx\n",
      "data/yeong/자동/나주_2018.xlsx\n",
      "data/yeong/자동/나주_2019.xlsx\n",
      "data/yeong/자동/나주_2020.xlsx\n",
      "time range in files :  2015-01-01 00:00  ~  2020-12-31 23:00\n",
      "data/yeong/자동/지석천_2015.xlsx\n",
      "data/yeong/자동/지석천_2016.xlsx\n",
      "data/yeong/자동/지석천_2017.xlsx\n",
      "data/yeong/자동/지석천_2018.xlsx\n",
      "data/yeong/자동/지석천_2019.xlsx\n",
      "data/yeong/자동/지석천_2020.xlsx\n",
      "time range in files :  2015-01-01 00:00  ~  2020-12-31 23:00\n",
      "data/yeong/자동/용봉_2015.xlsx\n",
      "data/yeong/자동/용봉_2016.xlsx\n",
      "data/yeong/자동/용봉_2017.xlsx\n",
      "data/yeong/자동/용봉_2018.xlsx\n",
      "data/yeong/자동/용봉_2019.xlsx\n",
      "data/yeong/자동/용봉_2020.xlsx\n",
      "time range in files :  2015-01-01 00:00  ~  2020-12-31 23:00\n",
      "data/yeong/자동/우치_2015.xlsx\n",
      "data/yeong/자동/우치_2016.xlsx\n",
      "data/yeong/자동/우치_2017.xlsx\n",
      "data/yeong/자동/우치_2018.xlsx\n",
      "data/yeong/자동/우치_2019.xlsx\n",
      "data/yeong/자동/우치_2020.xlsx\n",
      "time range in files :  2015-01-01 00:00  ~  2020-12-31 23:00\n",
      "MissData :  save/  miss :  (14780, 12)\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 0.2752\n",
      "MissData :  save/  miss :  (14780, 12)\n"
     ]
    }
   ],
   "source": [
    "for i in range(length):\n",
    "\n",
    "    idx = run_num[i]\n",
    "\n",
    "    print('interpol flag : ', interpolation_option[idx])\n",
    "    print('folder : ', data_path + folder[idx])\n",
    "    print('colum_idx : ', colum_idx[idx])\n",
    "    print('file_names[idx] : ', file_names[idx])\n",
    "\n",
    "    #start = time.time()\n",
    "\n",
    "    #if watershed == '한강_12days_test':\n",
    "    #    df, times = make_dataframe_temp_12days(folder[idx], file_names[idx], colum_idx[idx], interpolate=interpolation_option[idx])\n",
    "    #else:\n",
    "    df, times = make_dataframe(data_path+folder[idx], file_names[idx], colum_idx[idx], interpolation=interpolation_option[idx])\n",
    "\n",
    "    df_all, train_mean, train_std, df = normalize(df)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    if i == 0:\n",
    "        dfff = df\n",
    "        target_all = df_all\n",
    "        target_std = train_std\n",
    "        target_mean = train_mean\n",
    "        start_year = str(times.iloc[0].year)\n",
    "        end_year = str(times.iloc[-1].year)\n",
    "\n",
    "    if interpolation_option[idx][0] == False:\n",
    "\n",
    "        loadfiles = ['idx.npy', 'miss.npy', 'discriminator.h5', 'generator.h5']\n",
    "\n",
    "        gain_calc_falg = True\n",
    "\n",
    "        if __GAIN_TRAINING__ == True:\n",
    "            gain_calc_falg = MissData.save(pd.concat(df, axis=0).to_numpy(), max_tseq=24, save_dir='save/')\n",
    "            #print(folder[idx], ': training ', 'Miss date save : ', gain_calc_falg)\n",
    "        else:\n",
    "            for file in loadfiles:\n",
    "                if os.path.isfile('save/' + folder[idx]+file):\n",
    "                    shutil.copyfile('save/' + folder[idx]+file, 'save/'+file)\n",
    "                    #print('load file name : save/' + folder[idx]+file)\n",
    "                else:\n",
    "                    if file == 'miss.npy':\n",
    "                        gain_calc_falg = MissData.save(pd.concat(df, axis=0).to_numpy(), max_tseq=24, save_dir='save/')\n",
    "                        #print(folder[idx], ': is not miss.npy ', 'Miss date save : ', gain_calc_falg)\n",
    "\n",
    "        if gain_calc_falg == True:\n",
    "            #print('GainWindowGenerator in main')\n",
    "            WindowGenerator.make_dataset = make_dataset_gain\n",
    "            wide_window = WindowGenerator(input_width=gain_in_setps, label_width=gain_out_setps, shift=gain_shift,\n",
    "                                          fill_no=gain_fill_no, miss_rate=gain_miss_rate, batch_size=gain_batch_size,\n",
    "                                          train_df = df_all, val_df = df_all, test_df = df_all, df = df)\n",
    "\n",
    "            #gain = model_GAIN(shape=wide_window.dg.shape[1:], gen_sigmoid=False, epochs=gain_epochs, training_flag=__GAIN_TRAINING__, window=wide_window, model_save_path='save/')\n",
    "            gain = model_GAIN(shape=(gain_in_setps, df_all.shape[1]), gen_sigmoid=False, epochs=gain_epochs,\n",
    "                              training_flag=__GAIN_TRAINING__, window=wide_window, model_save_path='save/')\n",
    "\n",
    "            gain_val_performance[str(i)] = gain.evaluate(wide_window.val)\n",
    "            gain_performance[str(i)] = gain.evaluate(wide_window.test, verbose=0)\n",
    "\n",
    "            #print('file proc in main')\n",
    "            if __GAIN_TRAINING__ == True:\n",
    "                #dir = 'save/'+folder[i]\n",
    "                if not os.path.exists('save/' + folder[idx]):\n",
    "                    os.makedirs('save/'+folder[idx])\n",
    "                for file in loadfiles:\n",
    "                    shutil.copyfile('save/' + file, 'save/' + folder[idx] + file)\n",
    "\n",
    "            #print('create_dataset_with_gain in main')\n",
    "            #ori, gan = create_dataset_with_gain(gain=gain, window=wide_window, df=df)\n",
    "            ori, gan = create_dataset_with_gain(gain=gain, shape=(gain_in_setps, df_all.shape[1]), df=df)\n",
    "\n",
    "        else:\n",
    "            gan = create_dataset_interpol(window=gain_in_setps, df=df)\n",
    "    else:\n",
    "        gan = create_dataset_interpol(window=gain_in_setps, df=df)\n",
    "\n",
    "    if i == 0 :\n",
    "#        if i < length -1:\n",
    "#            gan = gan[:,:-4]  #맨마지막전까지 사인코사인삭제\n",
    "#            print(gan.shape)\n",
    "        real_df_all = pd.DataFrame(gan)\n",
    "    else:\n",
    "#        if i < length -1:\n",
    "#            gan = gan[:,:-4]  #맨마지막전까지 사인코사인삭제\n",
    "#            print(gan.shape)\n",
    "        real_df_all = pd.concat([real_df_all, pd.DataFrame(gan)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52608, 48)\n"
     ]
    }
   ],
   "source": [
    "print(real_df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_slice(df, train_ratio, val_ratio, test_ratio):\n",
    "    total_no = df.shape[0]\n",
    "\n",
    "    train_no = int(total_no * train_ratio)\n",
    "    #\n",
    "    val_no = int(total_no * (train_ratio + val_ratio))\n",
    "\n",
    "    # val_no = int(total_no*val_ratio)\n",
    "    # train_no =int(total_no * (train_ratio+val_ratio))\n",
    "    #\n",
    "    # val_slice = slice(0, val_no)\n",
    "    # train_slice = slice(val_no, train_no)\n",
    "    # test_slice = slice(train_no, None)\n",
    "\n",
    "    train_slice = slice(0, train_no)    #0.8\n",
    "    val_slice = slice(train_no, val_no)   #0.1\n",
    "    test_slice = slice(val_no, None)\n",
    "    # val_slice = slice(train_no, val_no)   #0.1\n",
    "    # test_slice = slice(val_no, None)\n",
    "#     train_slice = slice(0, train_no)  # 0.8\n",
    "#     val_slice = slice(train_no, None)  # 0.1\n",
    "#     test_slice = slice(val_no, None)\n",
    "\n",
    "    train = pd.DataFrame(df[train_slice])\n",
    "    val = pd.DataFrame(df[val_slice])\n",
    "    test = pd.DataFrame(df[test_slice])\n",
    "\n",
    "    test_slice2 = slice(total_no - 288, None)\n",
    "    test2 = pd.DataFrame(df[test_slice2])\n",
    "\n",
    "    #print('total_no :1111111111111111111111111 ')\n",
    "    #print('total_no : ', total_no)\n",
    "    #print('train : ', train.shape)\n",
    "    #print('val : ', val.shape)\n",
    "    #print('test : ', test.shape)\n",
    "\n",
    "\n",
    "    return train, val, test, test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# slice DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df=real_df_all.iloc[:-2000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, val_df, train_df, test_df2 = dataset_slice(fake_df, 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5061, 48), (40486, 48), (5061, 48))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape, test_df.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------prediction\n",
      "-------------------prediction\n",
      "-------------------prediction\n",
      "real_df_all.type :  <class 'pandas.core.frame.DataFrame'>\n",
      "train_df.type :  <class 'pandas.core.frame.DataFrame'>\n",
      "train_df.shape :  (5061, 48) val_df.shape :  (5061, 48) test_df.shape: (40486, 48)\n"
     ]
    }
   ],
   "source": [
    "print('-------------------prediction')\n",
    "print('-------------------prediction')\n",
    "print('-------------------prediction')\n",
    "\n",
    "print('real_df_all.type : ', type(real_df_all))\n",
    "print('train_df.type : ', type(train_df))\n",
    "print('train_df.shape : ', train_df.shape, 'val_df.shape : ', val_df.shape, 'test_df.shape:' ,test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_columns_indices:\n",
      "{'tmpr_value': 0, 'ph_value': 1, 'do_value': 2, 'ec_value': 3, 'toc_value': 4, '총질소_값': 5, '총인_값': 6, '클로로필-a_값': 7, 'Day sin': 8, 'Day cos': 9, 'Year sin': 10, 'Year cos': 11}\n",
      "target columns :  toc\n",
      "target_col_idx :  4\n",
      "out_num_features :  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_columns_indices = {name: i for i, name in enumerate(dfff[0])}\n",
    "\n",
    "print(\"label_columns_indices:\")\n",
    "print(label_columns_indices)\n",
    "\n",
    "\n",
    "target_dic = {\"do\":\"do_value\", \"toc\":\"toc_value\", \"tn\":\"총질소_값\", \"tp\":\"총인_값\", \"chl-a\":\"클로로필-a_값\"}\n",
    "\n",
    "rnn_target_column = \"toc\"\n",
    "\n",
    "print('target columns : ', rnn_target_column)\n",
    "num_features = dfff[0].shape[1]\n",
    "\n",
    "\n",
    "\n",
    "target_col_idx = label_columns_indices[target_dic[rnn_target_column]]\n",
    "out_features = [target_col_idx]\n",
    "out_num_features = len(out_features)\n",
    "\n",
    "print(\"target_col_idx : \", target_col_idx)\n",
    "print('out_num_features : ', out_num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model path :  save/yeong/models/toc/\n"
     ]
    }
   ],
   "source": [
    "val_nse = {}\n",
    "val_pbias = {}\n",
    "\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset_water\n",
    "multi_window = WindowGenerator(\n",
    "    input_width=rnn_in_setps,label_width=rnn_out_steps, shift=rnn_out_steps,out_features=out_features,\n",
    "    out_num_features=out_num_features,label_columns=dfff[0].columns, batch_size=rnn_batch_size,\n",
    "    train_df=train_df, val_df=val_df, test_df=test_df, test_df2=test_df2)\n",
    "\n",
    "if __RNN_TRAINING__:\n",
    "    if not os.path.exists('save/' + watershed):\n",
    "        os.makedirs('save/' + watershed)\n",
    "\n",
    "\n",
    "idx = [2, 4, 5, 6, 7]\n",
    "pa = [\"do/\", \"toc/\", \"nitrogen/\", \"phosphorus/\", \"chlorophyll-a/\"]\n",
    "\n",
    "indices = {name: i for i, name in enumerate(idx)}\n",
    "\n",
    "model_path = \"save/\" + watershed + \"models/\" + pa[indices[target_col_idx]]\n",
    "print(\"save model path : \", model_path)\n",
    "\n",
    "val_nse = {}\n",
    "val_pbias = {}\n",
    "\n",
    " # +\"gru.ckpt\" -- path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b73378368dd421da31cab5b85c6d79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(real_df_all.iloc[:,out_features[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027735ec8fe440848717d5544a3c54f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(test_df.iloc[:,out_features[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "__RNN_TRAINING__ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/250\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8882 - mean_absolute_error: 0.6565 - nse: -0.6175 - val_loss: 0.2214 - val_mean_absolute_error: 0.3678 - val_nse: -0.0141\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.22140, saving model to save/yeong/models/toc/gru.ckpt\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 1.1653 - mean_absolute_error: 0.7176 - nse: -0.5054 - val_loss: 0.2168 - val_mean_absolute_error: 0.3670 - val_nse: 0.0359\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.22140 to 0.21684, saving model to save/yeong/models/toc/gru.ckpt\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 0.8438 - mean_absolute_error: 0.6237 - nse: -0.5398 - val_loss: 0.2483 - val_mean_absolute_error: 0.3953 - val_nse: 0.0848\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.21684\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 1s 547ms/step - loss: 0.7538 - mean_absolute_error: 0.5623 - nse: -0.2355 - val_loss: 0.2066 - val_mean_absolute_error: 0.3605 - val_nse: 0.1224\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.21684 to 0.20661, saving model to save/yeong/models/toc/gru.ckpt\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.5487 - mean_absolute_error: 0.4424 - nse: 0.0287 - val_loss: 0.2124 - val_mean_absolute_error: 0.3566 - val_nse: 0.2403\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.20661\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.5426 - mean_absolute_error: 0.4614 - nse: 0.1676 - val_loss: 0.1988 - val_mean_absolute_error: 0.3530 - val_nse: 0.1662\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.20661 to 0.19880, saving model to save/yeong/models/toc/gru.ckpt\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.4773 - mean_absolute_error: 0.4120 - nse: 0.3254 - val_loss: 0.2323 - val_mean_absolute_error: 0.3739 - val_nse: -0.1212\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.19880\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 1s 558ms/step - loss: 0.5993 - mean_absolute_error: 0.5677 - nse: 0.1468 - val_loss: 0.2473 - val_mean_absolute_error: 0.3806 - val_nse: -0.0628\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.19880\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 1s 537ms/step - loss: 0.5072 - mean_absolute_error: 0.5335 - nse: 0.2001 - val_loss: 0.3372 - val_mean_absolute_error: 0.4666 - val_nse: -0.2039\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.19880\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 0.4470 - mean_absolute_error: 0.4744 - nse: 0.3668 - val_loss: 0.4170 - val_mean_absolute_error: 0.5382 - val_nse: -0.8119\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.19880\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.3660 - mean_absolute_error: 0.4496 - nse: 0.3540 - val_loss: 0.3094 - val_mean_absolute_error: 0.4564 - val_nse: -0.3835\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.19880\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 1s 508ms/step - loss: 0.3837 - mean_absolute_error: 0.4203 - nse: 0.3383 - val_loss: 0.2884 - val_mean_absolute_error: 0.4436 - val_nse: -0.2885\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.19880\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 1s 557ms/step - loss: 0.3878 - mean_absolute_error: 0.4093 - nse: 0.3734 - val_loss: 0.2809 - val_mean_absolute_error: 0.4350 - val_nse: -0.2679\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.19880\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 0.3972 - mean_absolute_error: 0.4050 - nse: 0.3576 - val_loss: 0.2382 - val_mean_absolute_error: 0.3968 - val_nse: 0.0610\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.19880\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.3961 - mean_absolute_error: 0.3835 - nse: 0.3351 - val_loss: 0.2345 - val_mean_absolute_error: 0.3960 - val_nse: -0.0482\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.19880\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 1s 510ms/step - loss: 0.4522 - mean_absolute_error: 0.4035 - nse: 0.3610 - val_loss: 0.2372 - val_mean_absolute_error: 0.4019 - val_nse: -0.0808\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.19880\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.3179 - mean_absolute_error: 0.3377 - nse: 0.4457 - val_loss: 0.2181 - val_mean_absolute_error: 0.3781 - val_nse: -0.0102\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.19880\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 1s 529ms/step - loss: 0.2386 - mean_absolute_error: 0.3130 - nse: 0.4258 - val_loss: 0.2789 - val_mean_absolute_error: 0.4314 - val_nse: -0.1250\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.19880\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.3760 - mean_absolute_error: 0.3934 - nse: 0.3746 - val_loss: 0.2538 - val_mean_absolute_error: 0.4020 - val_nse: -0.0991\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.19880\n",
      "Epoch 20/250\n",
      "1/1 [==============================] - 1s 567ms/step - loss: 0.3605 - mean_absolute_error: 0.3854 - nse: 0.4022 - val_loss: 0.2749 - val_mean_absolute_error: 0.4309 - val_nse: -0.0448\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.19880\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.3719 - mean_absolute_error: 0.4047 - nse: 0.4463 - val_loss: 0.2544 - val_mean_absolute_error: 0.4131 - val_nse: -0.1000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.19880\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 1s 547ms/step - loss: 0.2915 - mean_absolute_error: 0.3490 - nse: 0.4665 - val_loss: 0.2168 - val_mean_absolute_error: 0.3796 - val_nse: 0.1857\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.19880\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.3265 - mean_absolute_error: 0.3499 - nse: 0.4643 - val_loss: 0.2054 - val_mean_absolute_error: 0.3681 - val_nse: 0.0840\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.19880\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 0.2602 - mean_absolute_error: 0.2968 - nse: 0.5201 - val_loss: 0.2129 - val_mean_absolute_error: 0.3723 - val_nse: -0.1145\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.19880\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 1s 573ms/step - loss: 0.2909 - mean_absolute_error: 0.3171 - nse: 0.5119 - val_loss: 0.1715 - val_mean_absolute_error: 0.3358 - val_nse: 0.2111\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.19880 to 0.17146, saving model to save/yeong/models/toc/gru.ckpt\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 1s 510ms/step - loss: 0.2478 - mean_absolute_error: 0.3046 - nse: 0.5298 - val_loss: 0.1892 - val_mean_absolute_error: 0.3476 - val_nse: 0.2466\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.17146\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 1s 586ms/step - loss: 0.3228 - mean_absolute_error: 0.3518 - nse: 0.4486 - val_loss: 0.1807 - val_mean_absolute_error: 0.3354 - val_nse: 0.1853\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.17146\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 0.3640 - mean_absolute_error: 0.3630 - nse: 0.4746 - val_loss: 0.1560 - val_mean_absolute_error: 0.3168 - val_nse: 0.4165\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.17146 to 0.15596, saving model to save/yeong/models/toc/gru.ckpt\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 0.2447 - mean_absolute_error: 0.3219 - nse: 0.5255 - val_loss: 0.1845 - val_mean_absolute_error: 0.3514 - val_nse: 0.1785\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.15596\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 1s 577ms/step - loss: 0.2629 - mean_absolute_error: 0.3342 - nse: 0.4573 - val_loss: 0.1737 - val_mean_absolute_error: 0.3255 - val_nse: 0.2768\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.15596\n",
      "Epoch 31/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 581ms/step - loss: 0.2362 - mean_absolute_error: 0.3081 - nse: 0.5035 - val_loss: 0.1254 - val_mean_absolute_error: 0.2843 - val_nse: 0.3468\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.15596 to 0.12538, saving model to save/yeong/models/toc/gru.ckpt\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 0.3636 - mean_absolute_error: 0.3483 - nse: 0.4660 - val_loss: 0.1698 - val_mean_absolute_error: 0.3238 - val_nse: 0.2669\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.12538\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 0.4261 - mean_absolute_error: 0.3839 - nse: 0.4675 - val_loss: 0.1813 - val_mean_absolute_error: 0.3452 - val_nse: 0.1938\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.12538\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 1s 515ms/step - loss: 0.3473 - mean_absolute_error: 0.3588 - nse: 0.4761 - val_loss: 0.1743 - val_mean_absolute_error: 0.3315 - val_nse: 0.2522\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.12538\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 500ms/step - loss: 0.2352 - mean_absolute_error: 0.3180 - nse: 0.5442 - val_loss: 0.1740 - val_mean_absolute_error: 0.3367 - val_nse: 0.2445\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.12538\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.3072 - mean_absolute_error: 0.3435 - nse: 0.5270 - val_loss: 0.1822 - val_mean_absolute_error: 0.3408 - val_nse: 0.2470\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.12538\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 1s 581ms/step - loss: 0.2808 - mean_absolute_error: 0.3457 - nse: 0.4866 - val_loss: 0.1704 - val_mean_absolute_error: 0.3371 - val_nse: 0.3548\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.12538\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.3056 - mean_absolute_error: 0.3344 - nse: 0.5237 - val_loss: 0.1840 - val_mean_absolute_error: 0.3490 - val_nse: 0.2447\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.12538\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 1s 583ms/step - loss: 0.2010 - mean_absolute_error: 0.2799 - nse: 0.6247 - val_loss: 0.1860 - val_mean_absolute_error: 0.3485 - val_nse: 0.1650\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.12538\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 0.3550 - mean_absolute_error: 0.3458 - nse: 0.4900 - val_loss: 0.1912 - val_mean_absolute_error: 0.3599 - val_nse: 0.2668\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.12538\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 1s 520ms/step - loss: 0.2646 - mean_absolute_error: 0.3031 - nse: 0.5500 - val_loss: 0.2412 - val_mean_absolute_error: 0.4011 - val_nse: 0.0641\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.12538\n",
      "Epoch 42/250\n",
      "1/1 [==============================] - 1s 536ms/step - loss: 0.2702 - mean_absolute_error: 0.3174 - nse: 0.5497 - val_loss: 0.2462 - val_mean_absolute_error: 0.4114 - val_nse: 0.0411\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.12538\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 1s 556ms/step - loss: 0.3079 - mean_absolute_error: 0.3488 - nse: 0.5341 - val_loss: 0.2226 - val_mean_absolute_error: 0.3883 - val_nse: 0.0411\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.12538\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 1s 558ms/step - loss: 0.3000 - mean_absolute_error: 0.3414 - nse: 0.5541 - val_loss: 0.2062 - val_mean_absolute_error: 0.3707 - val_nse: 0.1170\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.12538\n",
      "Epoch 45/250\n",
      "1/1 [==============================] - 1s 558ms/step - loss: 0.3205 - mean_absolute_error: 0.3386 - nse: 0.5783 - val_loss: 0.2240 - val_mean_absolute_error: 0.3762 - val_nse: -0.0148\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.12538\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 1s 569ms/step - loss: 0.2838 - mean_absolute_error: 0.3226 - nse: 0.5433 - val_loss: 0.2401 - val_mean_absolute_error: 0.4041 - val_nse: 0.0523\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.12538\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.2815 - mean_absolute_error: 0.3477 - nse: 0.5872 - val_loss: 0.2482 - val_mean_absolute_error: 0.3931 - val_nse: -0.0164\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.12538\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.2682 - mean_absolute_error: 0.3416 - nse: 0.5492 - val_loss: 0.2531 - val_mean_absolute_error: 0.4055 - val_nse: -0.0405\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.12538\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 1s 520ms/step - loss: 0.2437 - mean_absolute_error: 0.3126 - nse: 0.5566 - val_loss: 0.2102 - val_mean_absolute_error: 0.3722 - val_nse: 0.0288\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.12538\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 1s 536ms/step - loss: 0.2559 - mean_absolute_error: 0.3159 - nse: 0.6011 - val_loss: 0.2351 - val_mean_absolute_error: 0.3842 - val_nse: 0.0017\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.12538\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 1s 514ms/step - loss: 0.2652 - mean_absolute_error: 0.3151 - nse: 0.5679 - val_loss: 0.1840 - val_mean_absolute_error: 0.3438 - val_nse: 0.1559\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.12538\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.2519 - mean_absolute_error: 0.3132 - nse: 0.5592 - val_loss: 0.1957 - val_mean_absolute_error: 0.3625 - val_nse: 0.2517\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.12538\n",
      "Epoch 53/250\n",
      "1/1 [==============================] - 1s 537ms/step - loss: 0.2628 - mean_absolute_error: 0.3195 - nse: 0.5817 - val_loss: 0.2029 - val_mean_absolute_error: 0.3688 - val_nse: 0.0112\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.12538\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 1s 518ms/step - loss: 0.2407 - mean_absolute_error: 0.3083 - nse: 0.6004 - val_loss: 0.1828 - val_mean_absolute_error: 0.3451 - val_nse: 0.1805\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.12538\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 0.2395 - mean_absolute_error: 0.3184 - nse: 0.5357 - val_loss: 0.2080 - val_mean_absolute_error: 0.3687 - val_nse: 0.1356\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.12538\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 498ms/step - loss: 0.2460 - mean_absolute_error: 0.3210 - nse: 0.5748 - val_loss: 0.2232 - val_mean_absolute_error: 0.3818 - val_nse: 0.0092\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.12538\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 1s 562ms/step - loss: 0.2657 - mean_absolute_error: 0.3254 - nse: 0.5649 - val_loss: 0.1922 - val_mean_absolute_error: 0.3571 - val_nse: 0.1975\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.12538\n",
      "Epoch 58/250\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 0.2582 - mean_absolute_error: 0.3305 - nse: 0.5883 - val_loss: 0.1830 - val_mean_absolute_error: 0.3513 - val_nse: 0.2687\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.12538\n",
      "Epoch 59/250\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 0.2187 - mean_absolute_error: 0.2974 - nse: 0.6092 - val_loss: 0.1849 - val_mean_absolute_error: 0.3501 - val_nse: 0.3216\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.12538\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 1s 553ms/step - loss: 0.2629 - mean_absolute_error: 0.3153 - nse: 0.5699 - val_loss: 0.1847 - val_mean_absolute_error: 0.3469 - val_nse: 0.2065\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.12538\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 1s 518ms/step - loss: 0.2422 - mean_absolute_error: 0.2992 - nse: 0.6170 - val_loss: 0.2153 - val_mean_absolute_error: 0.3815 - val_nse: -0.1173\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.12538\n",
      "Epoch 62/250\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.2125 - mean_absolute_error: 0.2902 - nse: 0.6030 - val_loss: 0.1852 - val_mean_absolute_error: 0.3500 - val_nse: 0.1960\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.12538\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 1s 556ms/step - loss: 0.2136 - mean_absolute_error: 0.2992 - nse: 0.5724 - val_loss: 0.1930 - val_mean_absolute_error: 0.3602 - val_nse: 0.1671\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.12538\n",
      "Epoch 64/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 570ms/step - loss: 0.2351 - mean_absolute_error: 0.3122 - nse: 0.6070 - val_loss: 0.2142 - val_mean_absolute_error: 0.3769 - val_nse: 0.0092\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.12538\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 0.2448 - mean_absolute_error: 0.3089 - nse: 0.6004 - val_loss: 0.1879 - val_mean_absolute_error: 0.3548 - val_nse: 0.2890\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.12538\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 1s 623ms/step - loss: 0.2387 - mean_absolute_error: 0.3177 - nse: 0.5801 - val_loss: 0.1865 - val_mean_absolute_error: 0.3560 - val_nse: 0.2338\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.12538\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.2205 - mean_absolute_error: 0.3062 - nse: 0.5836 - val_loss: 0.1892 - val_mean_absolute_error: 0.3484 - val_nse: 0.1817\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.12538\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.1906 - mean_absolute_error: 0.2864 - nse: 0.6009 - val_loss: 0.1589 - val_mean_absolute_error: 0.3240 - val_nse: 0.2545\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.12538\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 1s 501ms/step - loss: 0.2873 - mean_absolute_error: 0.3214 - nse: 0.5536 - val_loss: 0.1725 - val_mean_absolute_error: 0.3314 - val_nse: 0.2022\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.12538\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 0.3405 - mean_absolute_error: 0.3489 - nse: 0.5688 - val_loss: 0.2243 - val_mean_absolute_error: 0.3831 - val_nse: 0.0152\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.12538\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 1s 527ms/step - loss: 0.2721 - mean_absolute_error: 0.3219 - nse: 0.5975 - val_loss: 0.2307 - val_mean_absolute_error: 0.3919 - val_nse: -0.0531\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.12538\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 1s 550ms/step - loss: 0.2704 - mean_absolute_error: 0.3490 - nse: 0.5315 - val_loss: 0.2300 - val_mean_absolute_error: 0.3936 - val_nse: 0.0293\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.12538\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.2196 - mean_absolute_error: 0.3077 - nse: 0.6189 - val_loss: 0.2396 - val_mean_absolute_error: 0.3969 - val_nse: -0.0692\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.12538\n",
      "Epoch 74/250\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 0.2508 - mean_absolute_error: 0.3206 - nse: 0.5906 - val_loss: 0.2122 - val_mean_absolute_error: 0.3684 - val_nse: 0.2402\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.12538\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 1s 566ms/step - loss: 0.2492 - mean_absolute_error: 0.3085 - nse: 0.5965 - val_loss: 0.2076 - val_mean_absolute_error: 0.3666 - val_nse: 0.1581\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.12538\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 1s 535ms/step - loss: 0.1644 - mean_absolute_error: 0.2690 - nse: 0.6744 - val_loss: 0.2136 - val_mean_absolute_error: 0.3700 - val_nse: 0.0936\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.12538\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 0.2967 - mean_absolute_error: 0.3311 - nse: 0.6022 - val_loss: 0.1990 - val_mean_absolute_error: 0.3658 - val_nse: 0.1682\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.12538\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 1s 607ms/step - loss: 0.2053 - mean_absolute_error: 0.2778 - nse: 0.5796 - val_loss: 0.2431 - val_mean_absolute_error: 0.3992 - val_nse: 0.0672\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.12538\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 1s 537ms/step - loss: 0.2747 - mean_absolute_error: 0.3164 - nse: 0.6402 - val_loss: 0.2335 - val_mean_absolute_error: 0.3887 - val_nse: 0.0266\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.12538\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 493ms/step - loss: 0.2576 - mean_absolute_error: 0.3191 - nse: 0.5687 - val_loss: 0.2360 - val_mean_absolute_error: 0.3937 - val_nse: 0.0809\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.12538\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 0.2278 - mean_absolute_error: 0.3078 - nse: 0.6281 - val_loss: 0.2157 - val_mean_absolute_error: 0.3732 - val_nse: 0.0378\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.12538\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 0.2399 - mean_absolute_error: 0.2869 - nse: 0.6511 - val_loss: 0.2318 - val_mean_absolute_error: 0.3859 - val_nse: -0.0094\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.12538\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 1s 562ms/step - loss: 0.3041 - mean_absolute_error: 0.3304 - nse: 0.6062 - val_loss: 0.2201 - val_mean_absolute_error: 0.3797 - val_nse: 0.0551\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.12538\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 1s 542ms/step - loss: 0.2092 - mean_absolute_error: 0.2975 - nse: 0.6471 - val_loss: 0.2879 - val_mean_absolute_error: 0.4285 - val_nse: -0.0568\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.12538\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.2287 - mean_absolute_error: 0.3211 - nse: 0.6093 - val_loss: 0.2282 - val_mean_absolute_error: 0.3824 - val_nse: -0.0244\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.12538\n",
      "Epoch 86/250\n",
      "1/1 [==============================] - 0s 491ms/step - loss: 0.1655 - mean_absolute_error: 0.2648 - nse: 0.6743 - val_loss: 0.2336 - val_mean_absolute_error: 0.3950 - val_nse: -0.0122\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.12538\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 1s 611ms/step - loss: 0.2635 - mean_absolute_error: 0.3164 - nse: 0.6277 - val_loss: 0.2228 - val_mean_absolute_error: 0.3718 - val_nse: -0.0448\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.12538\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 0.2182 - mean_absolute_error: 0.2840 - nse: 0.6374 - val_loss: 0.2387 - val_mean_absolute_error: 0.3898 - val_nse: -0.0399\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.12538\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 1s 527ms/step - loss: 0.2124 - mean_absolute_error: 0.2829 - nse: 0.6262 - val_loss: 0.2224 - val_mean_absolute_error: 0.3842 - val_nse: 0.1579\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.12538\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 1s 591ms/step - loss: 0.2221 - mean_absolute_error: 0.2924 - nse: 0.6235 - val_loss: 0.2596 - val_mean_absolute_error: 0.4194 - val_nse: -0.2464\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.12538\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 1s 530ms/step - loss: 0.2325 - mean_absolute_error: 0.3070 - nse: 0.6606 - val_loss: 0.2177 - val_mean_absolute_error: 0.3769 - val_nse: -0.0197\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.12538\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 0.2105 - mean_absolute_error: 0.3009 - nse: 0.6110 - val_loss: 0.2500 - val_mean_absolute_error: 0.4039 - val_nse: -0.0534\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.12538\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 1s 519ms/step - loss: 0.1813 - mean_absolute_error: 0.2712 - nse: 0.6293 - val_loss: 0.3005 - val_mean_absolute_error: 0.4370 - val_nse: -0.2462\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.12538\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 0.2597 - mean_absolute_error: 0.3172 - nse: 0.6266 - val_loss: 0.2591 - val_mean_absolute_error: 0.4221 - val_nse: -0.1344\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.12538\n",
      "Epoch 95/250\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 0.1950 - mean_absolute_error: 0.2763 - nse: 0.6693 - val_loss: 0.2336 - val_mean_absolute_error: 0.3979 - val_nse: 0.1002\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.12538\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 1s 545ms/step - loss: 0.1991 - mean_absolute_error: 0.2939 - nse: 0.6535 - val_loss: 0.2633 - val_mean_absolute_error: 0.4213 - val_nse: -0.1338\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.12538\n",
      "Epoch 97/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 539ms/step - loss: 0.2415 - mean_absolute_error: 0.2981 - nse: 0.6264 - val_loss: 0.2747 - val_mean_absolute_error: 0.4263 - val_nse: -0.0243\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.12538\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 0.1813 - mean_absolute_error: 0.2733 - nse: 0.6629 - val_loss: 0.3456 - val_mean_absolute_error: 0.4844 - val_nse: -0.6814\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.12538\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 0.1784 - mean_absolute_error: 0.2690 - nse: 0.6643 - val_loss: 0.3509 - val_mean_absolute_error: 0.4836 - val_nse: -0.6639\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.12538\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 1s 524ms/step - loss: 0.1967 - mean_absolute_error: 0.2864 - nse: 0.6315 - val_loss: 0.3238 - val_mean_absolute_error: 0.4661 - val_nse: -0.3992\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.12538\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 0.2100 - mean_absolute_error: 0.2820 - nse: 0.6617 - val_loss: 0.3892 - val_mean_absolute_error: 0.5111 - val_nse: -0.7677\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.12538\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.1643 - mean_absolute_error: 0.2623 - nse: 0.6708 - val_loss: 0.3261 - val_mean_absolute_error: 0.4635 - val_nse: -0.2781\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.12538\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 1s 524ms/step - loss: 0.2601 - mean_absolute_error: 0.3202 - nse: 0.6371 - val_loss: 0.3165 - val_mean_absolute_error: 0.4605 - val_nse: -0.1914\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.12538\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 1s 567ms/step - loss: 0.1830 - mean_absolute_error: 0.2811 - nse: 0.6394 - val_loss: 0.3234 - val_mean_absolute_error: 0.4588 - val_nse: -0.5468\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.12538\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 1s 508ms/step - loss: 0.1520 - mean_absolute_error: 0.2504 - nse: 0.6794 - val_loss: 0.2983 - val_mean_absolute_error: 0.4390 - val_nse: -0.3762\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.12538\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 0.1862 - mean_absolute_error: 0.2657 - nse: 0.6047 - val_loss: 0.3294 - val_mean_absolute_error: 0.4571 - val_nse: -0.4188\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.12538\n",
      "Epoch 107/250\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 0.3231 - mean_absolute_error: 0.3407 - nse: 0.6306 - val_loss: 0.4130 - val_mean_absolute_error: 0.5191 - val_nse: -0.8522\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.12538\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.2316 - mean_absolute_error: 0.2990 - nse: 0.6657 - val_loss: 0.5672 - val_mean_absolute_error: 0.6059 - val_nse: -1.6150\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.12538\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 1s 536ms/step - loss: 0.2306 - mean_absolute_error: 0.3177 - nse: 0.6214 - val_loss: 0.4852 - val_mean_absolute_error: 0.5723 - val_nse: -1.1464\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.12538\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 1s 509ms/step - loss: 0.1832 - mean_absolute_error: 0.2839 - nse: 0.6674 - val_loss: 0.4394 - val_mean_absolute_error: 0.5354 - val_nse: -0.9445\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.12538\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 0.2314 - mean_absolute_error: 0.2974 - nse: 0.6487 - val_loss: 0.3729 - val_mean_absolute_error: 0.4890 - val_nse: -0.2559\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.12538\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.1979 - mean_absolute_error: 0.2796 - nse: 0.6407 - val_loss: 0.3781 - val_mean_absolute_error: 0.4856 - val_nse: -0.5987\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.12538\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.1718 - mean_absolute_error: 0.2632 - nse: 0.6857 - val_loss: 0.3407 - val_mean_absolute_error: 0.4639 - val_nse: -0.4642\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.12538\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 0.2478 - mean_absolute_error: 0.3031 - nse: 0.6690 - val_loss: 0.4286 - val_mean_absolute_error: 0.5268 - val_nse: -0.9745\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.12538\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.1833 - mean_absolute_error: 0.2696 - nse: 0.6592 - val_loss: 0.4285 - val_mean_absolute_error: 0.5230 - val_nse: -0.5697\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.12538\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 1s 556ms/step - loss: 0.2045 - mean_absolute_error: 0.2742 - nse: 0.6952 - val_loss: 0.4940 - val_mean_absolute_error: 0.5713 - val_nse: -1.0563\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.12538\n",
      "Epoch 117/250\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.1949 - mean_absolute_error: 0.2897 - nse: 0.6355 - val_loss: 0.4345 - val_mean_absolute_error: 0.5290 - val_nse: -0.5633\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.12538\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.2167 - mean_absolute_error: 0.2998 - nse: 0.6507 - val_loss: 0.3953 - val_mean_absolute_error: 0.5068 - val_nse: -0.7665\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.12538\n",
      "Epoch 119/250\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.2118 - mean_absolute_error: 0.2784 - nse: 0.7039 - val_loss: 0.4461 - val_mean_absolute_error: 0.5256 - val_nse: -1.0829\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.12538\n",
      "Epoch 120/250\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 0.2416 - mean_absolute_error: 0.2867 - nse: 0.6683 - val_loss: 0.4007 - val_mean_absolute_error: 0.5048 - val_nse: -0.6821\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.12538\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 492ms/step - loss: 0.1948 - mean_absolute_error: 0.2787 - nse: 0.6872 - val_loss: 0.4605 - val_mean_absolute_error: 0.5367 - val_nse: -0.6640\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.12538\n",
      "Epoch 122/250\n",
      "1/1 [==============================] - 1s 579ms/step - loss: 0.2271 - mean_absolute_error: 0.3005 - nse: 0.6656 - val_loss: 0.5710 - val_mean_absolute_error: 0.5850 - val_nse: -1.6104\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.12538\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.1744 - mean_absolute_error: 0.2802 - nse: 0.6910 - val_loss: 0.5449 - val_mean_absolute_error: 0.5836 - val_nse: -1.2902\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.12538\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 1s 505ms/step - loss: 0.1942 - mean_absolute_error: 0.2852 - nse: 0.6561 - val_loss: 0.6165 - val_mean_absolute_error: 0.6065 - val_nse: -1.7843\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.12538\n",
      "Epoch 125/250\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.2127 - mean_absolute_error: 0.2848 - nse: 0.7018 - val_loss: 0.6414 - val_mean_absolute_error: 0.6221 - val_nse: -1.9287\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.12538\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 0.1792 - mean_absolute_error: 0.2698 - nse: 0.6633 - val_loss: 0.4927 - val_mean_absolute_error: 0.5593 - val_nse: -0.8933\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.12538\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 1s 570ms/step - loss: 0.1925 - mean_absolute_error: 0.2585 - nse: 0.6811 - val_loss: 0.4621 - val_mean_absolute_error: 0.5442 - val_nse: -1.1386\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.12538\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 1s 508ms/step - loss: 0.2478 - mean_absolute_error: 0.3185 - nse: 0.6535 - val_loss: 0.4769 - val_mean_absolute_error: 0.5518 - val_nse: -1.1348\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.12538\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 1s 574ms/step - loss: 0.1640 - mean_absolute_error: 0.2582 - nse: 0.6965 - val_loss: 0.4844 - val_mean_absolute_error: 0.5470 - val_nse: -1.2907\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.12538\n",
      "Epoch 130/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 562ms/step - loss: 0.1728 - mean_absolute_error: 0.2669 - nse: 0.6419 - val_loss: 0.4536 - val_mean_absolute_error: 0.5221 - val_nse: -0.8203\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.12538\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 1s 536ms/step - loss: 0.1882 - mean_absolute_error: 0.2748 - nse: 0.6754 - val_loss: 0.4774 - val_mean_absolute_error: 0.5435 - val_nse: -0.9794\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.12538\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.1777 - mean_absolute_error: 0.2610 - nse: 0.6937 - val_loss: 0.4819 - val_mean_absolute_error: 0.5413 - val_nse: -0.8404\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.12538\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.1998 - mean_absolute_error: 0.2876 - nse: 0.7017 - val_loss: 0.5439 - val_mean_absolute_error: 0.5748 - val_nse: -1.4839\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.12538\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 1s 515ms/step - loss: 0.1687 - mean_absolute_error: 0.2624 - nse: 0.7030 - val_loss: 0.5209 - val_mean_absolute_error: 0.5665 - val_nse: -0.8641\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.12538\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.1777 - mean_absolute_error: 0.2669 - nse: 0.6912 - val_loss: 0.6493 - val_mean_absolute_error: 0.6381 - val_nse: -2.1078\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.12538\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 1s 529ms/step - loss: 0.1727 - mean_absolute_error: 0.2567 - nse: 0.7022 - val_loss: 0.8098 - val_mean_absolute_error: 0.7189 - val_nse: -3.0149\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.12538\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 1s 535ms/step - loss: 0.1795 - mean_absolute_error: 0.2696 - nse: 0.6844 - val_loss: 0.5408 - val_mean_absolute_error: 0.5735 - val_nse: -1.4019\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.12538\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.1575 - mean_absolute_error: 0.2559 - nse: 0.7174 - val_loss: 0.6515 - val_mean_absolute_error: 0.6522 - val_nse: -1.7874\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.12538\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 1s 536ms/step - loss: 0.1689 - mean_absolute_error: 0.2617 - nse: 0.6909 - val_loss: 0.5902 - val_mean_absolute_error: 0.6117 - val_nse: -1.5771\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.12538\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 1s 509ms/step - loss: 0.2242 - mean_absolute_error: 0.2859 - nse: 0.6816 - val_loss: 0.5302 - val_mean_absolute_error: 0.5723 - val_nse: -0.9478\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.12538\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 1s 602ms/step - loss: 0.1612 - mean_absolute_error: 0.2595 - nse: 0.6936 - val_loss: 0.5742 - val_mean_absolute_error: 0.6050 - val_nse: -1.6095\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.12538\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.1319 - mean_absolute_error: 0.2400 - nse: 0.7051 - val_loss: 0.5263 - val_mean_absolute_error: 0.5620 - val_nse: -1.2255\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.12538\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 1s 566ms/step - loss: 0.1448 - mean_absolute_error: 0.2391 - nse: 0.6872 - val_loss: 0.4975 - val_mean_absolute_error: 0.5498 - val_nse: -1.5194\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.12538\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.2391 - mean_absolute_error: 0.2909 - nse: 0.6825 - val_loss: 0.5926 - val_mean_absolute_error: 0.6011 - val_nse: -1.5186\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.12538\n",
      "Epoch 145/250\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 0.2439 - mean_absolute_error: 0.2977 - nse: 0.6931 - val_loss: 0.6375 - val_mean_absolute_error: 0.6129 - val_nse: -2.0822\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.12538\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.1984 - mean_absolute_error: 0.2838 - nse: 0.6724 - val_loss: 0.5114 - val_mean_absolute_error: 0.5781 - val_nse: -1.0836\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.12538\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 1s 520ms/step - loss: 0.1471 - mean_absolute_error: 0.2510 - nse: 0.7198 - val_loss: 0.5379 - val_mean_absolute_error: 0.5685 - val_nse: -1.3627\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.12538\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.1923 - mean_absolute_error: 0.2659 - nse: 0.6999 - val_loss: 0.4739 - val_mean_absolute_error: 0.5266 - val_nse: -0.9130\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.12538\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 1s 542ms/step - loss: 0.1642 - mean_absolute_error: 0.2598 - nse: 0.7019 - val_loss: 0.5125 - val_mean_absolute_error: 0.5537 - val_nse: -0.9411\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.12538\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 0.1864 - mean_absolute_error: 0.2686 - nse: 0.7172 - val_loss: 0.4632 - val_mean_absolute_error: 0.5310 - val_nse: -1.0025\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.12538\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 0.1449 - mean_absolute_error: 0.2428 - nse: 0.7382 - val_loss: 0.6504 - val_mean_absolute_error: 0.6252 - val_nse: -1.8900\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.12538\n",
      "Epoch 152/250\n",
      "1/1 [==============================] - 1s 599ms/step - loss: 0.2376 - mean_absolute_error: 0.2977 - nse: 0.6747 - val_loss: 0.4667 - val_mean_absolute_error: 0.5371 - val_nse: -0.6601\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.12538\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.1531 - mean_absolute_error: 0.2454 - nse: 0.7370 - val_loss: 0.5841 - val_mean_absolute_error: 0.5888 - val_nse: -1.4395\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.12538\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 0.1701 - mean_absolute_error: 0.2481 - nse: 0.7033 - val_loss: 0.6553 - val_mean_absolute_error: 0.6301 - val_nse: -1.4041\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.12538\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.1735 - mean_absolute_error: 0.2683 - nse: 0.7202 - val_loss: 0.6893 - val_mean_absolute_error: 0.6689 - val_nse: -1.9826\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.12538\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 0.1999 - mean_absolute_error: 0.2831 - nse: 0.6943 - val_loss: 0.7469 - val_mean_absolute_error: 0.6785 - val_nse: -2.3867\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.12538\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.2081 - mean_absolute_error: 0.2742 - nse: 0.7254 - val_loss: 0.6571 - val_mean_absolute_error: 0.6251 - val_nse: -1.9129\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.12538\n",
      "Epoch 158/250\n",
      "1/1 [==============================] - 1s 512ms/step - loss: 0.1917 - mean_absolute_error: 0.2700 - nse: 0.7088 - val_loss: 0.5101 - val_mean_absolute_error: 0.5544 - val_nse: -1.0148\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.12538\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 500ms/step - loss: 0.2014 - mean_absolute_error: 0.2842 - nse: 0.7108 - val_loss: 0.5182 - val_mean_absolute_error: 0.5524 - val_nse: -1.1522\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.12538\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 1s 529ms/step - loss: 0.1812 - mean_absolute_error: 0.2727 - nse: 0.6925 - val_loss: 0.5302 - val_mean_absolute_error: 0.5669 - val_nse: -1.1717\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.12538\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 1s 563ms/step - loss: 0.1686 - mean_absolute_error: 0.2580 - nse: 0.6933 - val_loss: 0.5574 - val_mean_absolute_error: 0.5728 - val_nse: -1.4643\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.12538\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.1703 - mean_absolute_error: 0.2648 - nse: 0.7200 - val_loss: 0.6382 - val_mean_absolute_error: 0.6018 - val_nse: -1.9335\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.12538\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 539ms/step - loss: 0.1773 - mean_absolute_error: 0.2575 - nse: 0.7178 - val_loss: 0.4515 - val_mean_absolute_error: 0.5174 - val_nse: -0.9618\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.12538\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.1755 - mean_absolute_error: 0.2624 - nse: 0.6890 - val_loss: 0.4205 - val_mean_absolute_error: 0.5119 - val_nse: -0.7070\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.12538\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 1s 535ms/step - loss: 0.2071 - mean_absolute_error: 0.2910 - nse: 0.6866 - val_loss: 0.4555 - val_mean_absolute_error: 0.5375 - val_nse: -1.1894\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.12538\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 0.1846 - mean_absolute_error: 0.2591 - nse: 0.7049 - val_loss: 0.3767 - val_mean_absolute_error: 0.4938 - val_nse: -0.6067\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.12538\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 1s 542ms/step - loss: 0.1510 - mean_absolute_error: 0.2587 - nse: 0.6717 - val_loss: 0.4495 - val_mean_absolute_error: 0.5281 - val_nse: -0.8547\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.12538\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.1876 - mean_absolute_error: 0.2695 - nse: 0.6948 - val_loss: 0.4710 - val_mean_absolute_error: 0.5308 - val_nse: -1.1651\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.12538\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 1s 594ms/step - loss: 0.1619 - mean_absolute_error: 0.2463 - nse: 0.7149 - val_loss: 0.4180 - val_mean_absolute_error: 0.5064 - val_nse: -0.7060\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.12538\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 0.1788 - mean_absolute_error: 0.2578 - nse: 0.7485 - val_loss: 0.4297 - val_mean_absolute_error: 0.5137 - val_nse: -0.7535\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.12538\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 1s 569ms/step - loss: 0.1301 - mean_absolute_error: 0.2356 - nse: 0.7177 - val_loss: 0.4074 - val_mean_absolute_error: 0.5047 - val_nse: -0.4421\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.12538\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 0.1810 - mean_absolute_error: 0.2656 - nse: 0.7145 - val_loss: 0.4091 - val_mean_absolute_error: 0.5028 - val_nse: -0.8557\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.12538\n",
      "Epoch 173/250\n",
      "1/1 [==============================] - 1s 557ms/step - loss: 0.1679 - mean_absolute_error: 0.2470 - nse: 0.7280 - val_loss: 0.5069 - val_mean_absolute_error: 0.5601 - val_nse: -1.6025\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.12538\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 1s 557ms/step - loss: 0.1545 - mean_absolute_error: 0.2514 - nse: 0.7112 - val_loss: 0.4436 - val_mean_absolute_error: 0.5126 - val_nse: -0.9092\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.12538\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.1588 - mean_absolute_error: 0.2543 - nse: 0.7115 - val_loss: 0.4551 - val_mean_absolute_error: 0.5279 - val_nse: -0.9550\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.12538\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.1533 - mean_absolute_error: 0.2492 - nse: 0.7480 - val_loss: 0.4866 - val_mean_absolute_error: 0.5424 - val_nse: -1.2269\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.12538\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 1s 542ms/step - loss: 0.1736 - mean_absolute_error: 0.2569 - nse: 0.7006 - val_loss: 0.3790 - val_mean_absolute_error: 0.4867 - val_nse: -0.3901\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.12538\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.1591 - mean_absolute_error: 0.2537 - nse: 0.7066 - val_loss: 0.4814 - val_mean_absolute_error: 0.5517 - val_nse: -1.0588\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.12538\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 1s 566ms/step - loss: 0.1554 - mean_absolute_error: 0.2441 - nse: 0.7312 - val_loss: 0.4459 - val_mean_absolute_error: 0.5122 - val_nse: -0.9779\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.12538\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 1s 601ms/step - loss: 0.1201 - mean_absolute_error: 0.2277 - nse: 0.7220 - val_loss: 0.4079 - val_mean_absolute_error: 0.5047 - val_nse: -0.9312\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.12538\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 0.2094 - mean_absolute_error: 0.2726 - nse: 0.6982 - val_loss: 0.4669 - val_mean_absolute_error: 0.5340 - val_nse: -1.0104\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.12538\n",
      "Epoch 182/250\n",
      "1/1 [==============================] - 1s 556ms/step - loss: 0.2219 - mean_absolute_error: 0.2845 - nse: 0.7231 - val_loss: 0.5338 - val_mean_absolute_error: 0.5661 - val_nse: -1.3699\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.12538\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 0.1898 - mean_absolute_error: 0.2716 - nse: 0.7127 - val_loss: 0.5049 - val_mean_absolute_error: 0.5605 - val_nse: -1.2619\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.12538\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 1s 538ms/step - loss: 0.1464 - mean_absolute_error: 0.2428 - nse: 0.7238 - val_loss: 0.4678 - val_mean_absolute_error: 0.5427 - val_nse: -1.0769\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.12538\n",
      "Epoch 185/250\n",
      "1/1 [==============================] - 1s 569ms/step - loss: 0.1869 - mean_absolute_error: 0.2709 - nse: 0.7240 - val_loss: 0.4706 - val_mean_absolute_error: 0.5302 - val_nse: -0.9662\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.12538\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.1565 - mean_absolute_error: 0.2488 - nse: 0.7143 - val_loss: 0.3818 - val_mean_absolute_error: 0.4923 - val_nse: -0.3898\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.12538\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 1s 575ms/step - loss: 0.1691 - mean_absolute_error: 0.2540 - nse: 0.7226 - val_loss: 0.3828 - val_mean_absolute_error: 0.4986 - val_nse: -0.5175\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.12538\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 1s 519ms/step - loss: 0.1174 - mean_absolute_error: 0.2237 - nse: 0.7685 - val_loss: 0.4994 - val_mean_absolute_error: 0.5480 - val_nse: -1.3113\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.12538\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 1s 553ms/step - loss: 0.2325 - mean_absolute_error: 0.3002 - nse: 0.6951 - val_loss: 0.4459 - val_mean_absolute_error: 0.5326 - val_nse: -0.7676\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.12538\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 1s 536ms/step - loss: 0.1371 - mean_absolute_error: 0.2367 - nse: 0.7365 - val_loss: 0.4603 - val_mean_absolute_error: 0.5283 - val_nse: -0.7740\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.12538\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.1626 - mean_absolute_error: 0.2350 - nse: 0.7569 - val_loss: 0.5152 - val_mean_absolute_error: 0.5613 - val_nse: -1.0321\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.12538\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.1792 - mean_absolute_error: 0.2644 - nse: 0.7224 - val_loss: 0.4192 - val_mean_absolute_error: 0.5100 - val_nse: -0.8159\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.12538\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.1554 - mean_absolute_error: 0.2514 - nse: 0.7498 - val_loss: 0.4842 - val_mean_absolute_error: 0.5410 - val_nse: -1.1304\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.12538\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 0.1667 - mean_absolute_error: 0.2432 - nse: 0.7634 - val_loss: 0.4919 - val_mean_absolute_error: 0.5361 - val_nse: -1.1136\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.12538\n",
      "Epoch 195/250\n",
      "1/1 [==============================] - 1s 510ms/step - loss: 0.1877 - mean_absolute_error: 0.2594 - nse: 0.7328 - val_loss: 0.4279 - val_mean_absolute_error: 0.5093 - val_nse: -0.6710\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.12538\n",
      "Epoch 196/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 526ms/step - loss: 0.1636 - mean_absolute_error: 0.2589 - nse: 0.7314 - val_loss: 0.4516 - val_mean_absolute_error: 0.5190 - val_nse: -0.8088\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.12538\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 0.1492 - mean_absolute_error: 0.2479 - nse: 0.7304 - val_loss: 0.4746 - val_mean_absolute_error: 0.5261 - val_nse: -1.0571\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.12538\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 1s 506ms/step - loss: 0.1349 - mean_absolute_error: 0.2359 - nse: 0.7678 - val_loss: 0.5524 - val_mean_absolute_error: 0.5731 - val_nse: -1.3817\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.12538\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 0.1799 - mean_absolute_error: 0.2485 - nse: 0.7297 - val_loss: 0.5578 - val_mean_absolute_error: 0.5617 - val_nse: -1.5016\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.12538\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 1s 586ms/step - loss: 0.1684 - mean_absolute_error: 0.2490 - nse: 0.7356 - val_loss: 0.5078 - val_mean_absolute_error: 0.5537 - val_nse: -1.2962\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.12538\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 0.1393 - mean_absolute_error: 0.2321 - nse: 0.7410 - val_loss: 0.4488 - val_mean_absolute_error: 0.5335 - val_nse: -0.7475\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.12538\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.1731 - mean_absolute_error: 0.2526 - nse: 0.7444 - val_loss: 0.5324 - val_mean_absolute_error: 0.5842 - val_nse: -1.5898\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.12538\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.1615 - mean_absolute_error: 0.2451 - nse: 0.7218 - val_loss: 0.4190 - val_mean_absolute_error: 0.5075 - val_nse: -0.9847\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.12538\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.1391 - mean_absolute_error: 0.2349 - nse: 0.7406 - val_loss: 0.4546 - val_mean_absolute_error: 0.5266 - val_nse: -0.8650\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.12538\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 1s 524ms/step - loss: 0.1595 - mean_absolute_error: 0.2460 - nse: 0.7277 - val_loss: 0.4887 - val_mean_absolute_error: 0.5352 - val_nse: -1.0364\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.12538\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.1715 - mean_absolute_error: 0.2516 - nse: 0.7220 - val_loss: 0.5081 - val_mean_absolute_error: 0.5491 - val_nse: -1.1278\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.12538\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 0.1406 - mean_absolute_error: 0.2293 - nse: 0.7712 - val_loss: 0.4497 - val_mean_absolute_error: 0.5240 - val_nse: -0.8199\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.12538\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 1s 568ms/step - loss: 0.1340 - mean_absolute_error: 0.2376 - nse: 0.7556 - val_loss: 0.4481 - val_mean_absolute_error: 0.5309 - val_nse: -0.8336\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.12538\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 0.1666 - mean_absolute_error: 0.2499 - nse: 0.7463 - val_loss: 0.4506 - val_mean_absolute_error: 0.5258 - val_nse: -0.7507\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.12538\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 1s 578ms/step - loss: 0.1302 - mean_absolute_error: 0.2197 - nse: 0.7651 - val_loss: 0.5350 - val_mean_absolute_error: 0.5640 - val_nse: -1.6695\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.12538\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 1s 529ms/step - loss: 0.1373 - mean_absolute_error: 0.2280 - nse: 0.7459 - val_loss: 0.5772 - val_mean_absolute_error: 0.5854 - val_nse: -1.5666\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.12538\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.1279 - mean_absolute_error: 0.2270 - nse: 0.7478 - val_loss: 0.4588 - val_mean_absolute_error: 0.5268 - val_nse: -1.0392\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.12538\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.1615 - mean_absolute_error: 0.2415 - nse: 0.7527 - val_loss: 0.5847 - val_mean_absolute_error: 0.5909 - val_nse: -1.6307\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.12538\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 0.1576 - mean_absolute_error: 0.2463 - nse: 0.7136 - val_loss: 0.4752 - val_mean_absolute_error: 0.5322 - val_nse: -0.8781\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.12538\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 1s 587ms/step - loss: 0.1648 - mean_absolute_error: 0.2522 - nse: 0.7365 - val_loss: 0.4994 - val_mean_absolute_error: 0.5646 - val_nse: -0.9261\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.12538\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.1333 - mean_absolute_error: 0.2278 - nse: 0.7573 - val_loss: 0.5432 - val_mean_absolute_error: 0.5702 - val_nse: -1.5496\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.12538\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 0.0974 - mean_absolute_error: 0.2091 - nse: 0.7635 - val_loss: 0.4429 - val_mean_absolute_error: 0.5167 - val_nse: -0.9683\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.12538\n",
      "Epoch 218/250\n",
      "1/1 [==============================] - 1s 547ms/step - loss: 0.1675 - mean_absolute_error: 0.2381 - nse: 0.7191 - val_loss: 0.4944 - val_mean_absolute_error: 0.5373 - val_nse: -1.1623\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.12538\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.2081 - mean_absolute_error: 0.2766 - nse: 0.7386 - val_loss: 0.5413 - val_mean_absolute_error: 0.5671 - val_nse: -1.4606\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.12538\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.1934 - mean_absolute_error: 0.2670 - nse: 0.7305 - val_loss: 0.6166 - val_mean_absolute_error: 0.5963 - val_nse: -1.8746\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.12538\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 0.1411 - mean_absolute_error: 0.2468 - nse: 0.7489 - val_loss: 0.5281 - val_mean_absolute_error: 0.5735 - val_nse: -1.2097\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.12538\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 1s 519ms/step - loss: 0.1379 - mean_absolute_error: 0.2343 - nse: 0.7579 - val_loss: 0.4775 - val_mean_absolute_error: 0.5356 - val_nse: -1.1642\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.12538\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 0.1720 - mean_absolute_error: 0.2439 - nse: 0.7451 - val_loss: 0.4539 - val_mean_absolute_error: 0.5342 - val_nse: -0.5051\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.12538\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 1s 556ms/step - loss: 0.1494 - mean_absolute_error: 0.2403 - nse: 0.7375 - val_loss: 0.4023 - val_mean_absolute_error: 0.4943 - val_nse: -0.7470\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.12538\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 1s 550ms/step - loss: 0.1124 - mean_absolute_error: 0.2103 - nse: 0.7815 - val_loss: 0.4294 - val_mean_absolute_error: 0.5180 - val_nse: -0.7868\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.12538\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 1s 569ms/step - loss: 0.2014 - mean_absolute_error: 0.2669 - nse: 0.7447 - val_loss: 0.5228 - val_mean_absolute_error: 0.5541 - val_nse: -1.4390\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.12538\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 1s 535ms/step - loss: 0.1321 - mean_absolute_error: 0.2289 - nse: 0.7281 - val_loss: 0.4830 - val_mean_absolute_error: 0.5416 - val_nse: -0.7932\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.12538\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 0.1630 - mean_absolute_error: 0.2385 - nse: 0.7736 - val_loss: 0.5146 - val_mean_absolute_error: 0.5649 - val_nse: -1.1297\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.12538\n",
      "Epoch 229/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 527ms/step - loss: 0.1292 - mean_absolute_error: 0.2309 - nse: 0.7363 - val_loss: 0.4683 - val_mean_absolute_error: 0.5331 - val_nse: -0.6984\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.12538\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 1s 573ms/step - loss: 0.1616 - mean_absolute_error: 0.2515 - nse: 0.7527 - val_loss: 0.4433 - val_mean_absolute_error: 0.5170 - val_nse: -1.0139\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.12538\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.1491 - mean_absolute_error: 0.2309 - nse: 0.7728 - val_loss: 0.4609 - val_mean_absolute_error: 0.5230 - val_nse: -1.1207\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.12538\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.2020 - mean_absolute_error: 0.2559 - nse: 0.7559 - val_loss: 0.4070 - val_mean_absolute_error: 0.4994 - val_nse: -0.6855\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.12538\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 1s 621ms/step - loss: 0.1497 - mean_absolute_error: 0.2386 - nse: 0.7507 - val_loss: 0.3957 - val_mean_absolute_error: 0.4933 - val_nse: -0.4058\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.12538\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.1596 - mean_absolute_error: 0.2534 - nse: 0.7483 - val_loss: 0.5165 - val_mean_absolute_error: 0.5384 - val_nse: -1.3819\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.12538\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 0.1262 - mean_absolute_error: 0.2285 - nse: 0.7717 - val_loss: 0.5054 - val_mean_absolute_error: 0.5366 - val_nse: -1.2283\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.12538\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 1s 514ms/step - loss: 0.1559 - mean_absolute_error: 0.2454 - nse: 0.7404 - val_loss: 0.5916 - val_mean_absolute_error: 0.5568 - val_nse: -1.6316\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.12538\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.1668 - mean_absolute_error: 0.2530 - nse: 0.7509 - val_loss: 0.4993 - val_mean_absolute_error: 0.5363 - val_nse: -1.2590\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.12538\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 1s 537ms/step - loss: 0.1392 - mean_absolute_error: 0.2305 - nse: 0.7515 - val_loss: 0.4085 - val_mean_absolute_error: 0.5107 - val_nse: -0.4962\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.12538\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 1s 515ms/step - loss: 0.1464 - mean_absolute_error: 0.2260 - nse: 0.7386 - val_loss: 0.4577 - val_mean_absolute_error: 0.5433 - val_nse: -1.3291\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.12538\n",
      "Epoch 240/250\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.1946 - mean_absolute_error: 0.2711 - nse: 0.7363 - val_loss: 0.3499 - val_mean_absolute_error: 0.4718 - val_nse: -0.5346\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.12538\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.1262 - mean_absolute_error: 0.2165 - nse: 0.7517 - val_loss: 0.3996 - val_mean_absolute_error: 0.4982 - val_nse: -0.7739\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.12538\n",
      "Epoch 242/250\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 0.1447 - mean_absolute_error: 0.2302 - nse: 0.7230 - val_loss: 0.5017 - val_mean_absolute_error: 0.5349 - val_nse: -1.0524\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.12538\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 1s 557ms/step - loss: 0.1610 - mean_absolute_error: 0.2488 - nse: 0.7529 - val_loss: 0.4370 - val_mean_absolute_error: 0.5046 - val_nse: -0.8296\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.12538\n",
      "Epoch 244/250\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.1091 - mean_absolute_error: 0.2113 - nse: 0.7895 - val_loss: 0.5394 - val_mean_absolute_error: 0.5604 - val_nse: -1.0734\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.12538\n",
      "Epoch 245/250\n",
      "1/1 [==============================] - 1s 558ms/step - loss: 0.1724 - mean_absolute_error: 0.2629 - nse: 0.7418 - val_loss: 0.4173 - val_mean_absolute_error: 0.5062 - val_nse: -0.8632\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.12538\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 1s 562ms/step - loss: 0.1163 - mean_absolute_error: 0.2165 - nse: 0.7749 - val_loss: 0.4242 - val_mean_absolute_error: 0.5122 - val_nse: -0.5652\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.12538\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 0.1615 - mean_absolute_error: 0.2411 - nse: 0.7572 - val_loss: 0.4048 - val_mean_absolute_error: 0.5076 - val_nse: -0.8794\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.12538\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 1s 568ms/step - loss: 0.1240 - mean_absolute_error: 0.2141 - nse: 0.7600 - val_loss: 0.5334 - val_mean_absolute_error: 0.5541 - val_nse: -1.7353\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.12538\n",
      "Epoch 249/250\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 0.1332 - mean_absolute_error: 0.2291 - nse: 0.7530 - val_loss: 0.4079 - val_mean_absolute_error: 0.4949 - val_nse: -0.7479\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.12538\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 1s 520ms/step - loss: 0.1401 - mean_absolute_error: 0.2320 - nse: 0.7706 - val_loss: 0.5036 - val_mean_absolute_error: 0.5510 - val_nse: -1.2874\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.12538\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/250\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8808 - mean_absolute_error: 0.6318 - nse: -0.5691 - val_loss: 0.2573 - val_mean_absolute_error: 0.4056 - val_nse: 0.0043\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25734, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.9967 - mean_absolute_error: 0.6865 - nse: -0.6454 - val_loss: 0.2802 - val_mean_absolute_error: 0.4243 - val_nse: 0.0119\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.25734\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 1.0321 - mean_absolute_error: 0.7034 - nse: -0.6364 - val_loss: 0.2287 - val_mean_absolute_error: 0.3796 - val_nse: 0.0020\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.25734 to 0.22874, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.8552 - mean_absolute_error: 0.6160 - nse: -0.4889 - val_loss: 0.2188 - val_mean_absolute_error: 0.3601 - val_nse: 0.0292\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.22874 to 0.21884, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 1.0582 - mean_absolute_error: 0.7040 - nse: -0.4951 - val_loss: 0.2059 - val_mean_absolute_error: 0.3644 - val_nse: 0.0409\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.21884 to 0.20587, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.9489 - mean_absolute_error: 0.6479 - nse: -0.3510 - val_loss: 0.2400 - val_mean_absolute_error: 0.3954 - val_nse: 0.0436\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.20587\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.6211 - mean_absolute_error: 0.4700 - nse: -0.1373 - val_loss: 0.2190 - val_mean_absolute_error: 0.3664 - val_nse: 0.1059\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.20587\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.6539 - mean_absolute_error: 0.5078 - nse: -0.1134 - val_loss: 0.2083 - val_mean_absolute_error: 0.3574 - val_nse: 0.1345\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.20587\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.6745 - mean_absolute_error: 0.5025 - nse: 0.0699 - val_loss: 0.2163 - val_mean_absolute_error: 0.3674 - val_nse: 0.0820\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.20587\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.4926 - mean_absolute_error: 0.4329 - nse: 0.1435 - val_loss: 0.1955 - val_mean_absolute_error: 0.3470 - val_nse: 0.1076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_loss improved from 0.20587 to 0.19548, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.4654 - mean_absolute_error: 0.4362 - nse: 0.1745 - val_loss: 0.2872 - val_mean_absolute_error: 0.4371 - val_nse: -0.1257\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.19548\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.5084 - mean_absolute_error: 0.4618 - nse: 0.2104 - val_loss: 0.2454 - val_mean_absolute_error: 0.3991 - val_nse: 0.0318\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.19548\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.5013 - mean_absolute_error: 0.4782 - nse: 0.1775 - val_loss: 0.2025 - val_mean_absolute_error: 0.3618 - val_nse: 0.1685\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.19548\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.3701 - mean_absolute_error: 0.4194 - nse: 0.2572 - val_loss: 0.2104 - val_mean_absolute_error: 0.3671 - val_nse: 0.0177\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.19548\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.5835 - mean_absolute_error: 0.4902 - nse: 0.2955 - val_loss: 0.1844 - val_mean_absolute_error: 0.3457 - val_nse: 0.1877\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.19548 to 0.18444, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.4429 - mean_absolute_error: 0.4215 - nse: 0.3230 - val_loss: 0.1851 - val_mean_absolute_error: 0.3441 - val_nse: 0.2506\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.18444\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.3206 - mean_absolute_error: 0.3451 - nse: 0.4021 - val_loss: 0.2060 - val_mean_absolute_error: 0.3567 - val_nse: 0.1444\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.18444\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.3835 - mean_absolute_error: 0.3800 - nse: 0.4424 - val_loss: 0.1996 - val_mean_absolute_error: 0.3624 - val_nse: 0.1673\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.18444\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.3551 - mean_absolute_error: 0.3685 - nse: 0.4324 - val_loss: 0.1957 - val_mean_absolute_error: 0.3570 - val_nse: 0.1340\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.18444\n",
      "Epoch 20/250\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.2945 - mean_absolute_error: 0.3578 - nse: 0.4020 - val_loss: 0.2142 - val_mean_absolute_error: 0.3785 - val_nse: -0.0152\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.18444\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.2837 - mean_absolute_error: 0.3573 - nse: 0.3812 - val_loss: 0.2145 - val_mean_absolute_error: 0.3712 - val_nse: 0.0741\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.18444\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.2906 - mean_absolute_error: 0.3764 - nse: 0.4303 - val_loss: 0.2014 - val_mean_absolute_error: 0.3667 - val_nse: 0.0822\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.18444\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.3152 - mean_absolute_error: 0.3775 - nse: 0.4404 - val_loss: 0.1758 - val_mean_absolute_error: 0.3384 - val_nse: 0.1728\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.18444 to 0.17576, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.3876 - mean_absolute_error: 0.4030 - nse: 0.3765 - val_loss: 0.1788 - val_mean_absolute_error: 0.3427 - val_nse: 0.0925\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.17576\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.3357 - mean_absolute_error: 0.3756 - nse: 0.4273 - val_loss: 0.1887 - val_mean_absolute_error: 0.3471 - val_nse: 0.1560\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.17576\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.2992 - mean_absolute_error: 0.3491 - nse: 0.5025 - val_loss: 0.2045 - val_mean_absolute_error: 0.3571 - val_nse: 0.0102\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.17576\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.2772 - mean_absolute_error: 0.3532 - nse: 0.4059 - val_loss: 0.1637 - val_mean_absolute_error: 0.3178 - val_nse: 0.3750\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.17576 to 0.16370, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.3585 - mean_absolute_error: 0.3553 - nse: 0.4976 - val_loss: 0.1733 - val_mean_absolute_error: 0.3313 - val_nse: 0.2605\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.16370\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.2860 - mean_absolute_error: 0.3266 - nse: 0.5200 - val_loss: 0.1746 - val_mean_absolute_error: 0.3393 - val_nse: 0.2271\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.16370\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.3004 - mean_absolute_error: 0.3333 - nse: 0.4801 - val_loss: 0.1995 - val_mean_absolute_error: 0.3529 - val_nse: 0.1575\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.16370\n",
      "Epoch 31/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.2864 - mean_absolute_error: 0.3370 - nse: 0.5008 - val_loss: 0.1801 - val_mean_absolute_error: 0.3376 - val_nse: 0.2841\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.16370\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.3823 - mean_absolute_error: 0.3695 - nse: 0.4676 - val_loss: 0.1713 - val_mean_absolute_error: 0.3324 - val_nse: 0.2247\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.16370\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.3176 - mean_absolute_error: 0.3502 - nse: 0.4879 - val_loss: 0.1658 - val_mean_absolute_error: 0.3235 - val_nse: 0.3337\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.16370\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.3815 - mean_absolute_error: 0.3863 - nse: 0.4844 - val_loss: 0.1614 - val_mean_absolute_error: 0.3181 - val_nse: 0.3502\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.16370 to 0.16137, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.3286 - mean_absolute_error: 0.3509 - nse: 0.5208 - val_loss: 0.1703 - val_mean_absolute_error: 0.3219 - val_nse: 0.3044\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.16137\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.2741 - mean_absolute_error: 0.3182 - nse: 0.5455 - val_loss: 0.1531 - val_mean_absolute_error: 0.3078 - val_nse: 0.3548\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.16137 to 0.15313, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.2406 - mean_absolute_error: 0.3210 - nse: 0.4526 - val_loss: 0.1791 - val_mean_absolute_error: 0.3321 - val_nse: 0.3081\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.15313\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.3063 - mean_absolute_error: 0.3264 - nse: 0.4995 - val_loss: 0.1773 - val_mean_absolute_error: 0.3380 - val_nse: 0.2716\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.15313\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.2710 - mean_absolute_error: 0.3158 - nse: 0.5481 - val_loss: 0.1763 - val_mean_absolute_error: 0.3228 - val_nse: 0.3814\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.15313\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.2379 - mean_absolute_error: 0.3015 - nse: 0.5666 - val_loss: 0.1759 - val_mean_absolute_error: 0.3216 - val_nse: 0.2968\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.15313\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.2842 - mean_absolute_error: 0.3146 - nse: 0.5293 - val_loss: 0.1774 - val_mean_absolute_error: 0.3231 - val_nse: 0.2277\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.15313\n",
      "Epoch 42/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 272ms/step - loss: 0.3518 - mean_absolute_error: 0.3653 - nse: 0.5297 - val_loss: 0.1605 - val_mean_absolute_error: 0.3043 - val_nse: 0.2783\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.15313\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.3110 - mean_absolute_error: 0.3567 - nse: 0.5437 - val_loss: 0.1580 - val_mean_absolute_error: 0.3064 - val_nse: 0.3498\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.15313\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.2103 - mean_absolute_error: 0.2867 - nse: 0.5787 - val_loss: 0.1499 - val_mean_absolute_error: 0.3040 - val_nse: 0.3637\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.15313 to 0.14986, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 45/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.2602 - mean_absolute_error: 0.3107 - nse: 0.5758 - val_loss: 0.1707 - val_mean_absolute_error: 0.3123 - val_nse: 0.2841\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.14986\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.3308 - mean_absolute_error: 0.3561 - nse: 0.5477 - val_loss: 0.1408 - val_mean_absolute_error: 0.2862 - val_nse: 0.3753\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.14986 to 0.14078, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.2314 - mean_absolute_error: 0.2819 - nse: 0.6241 - val_loss: 0.1443 - val_mean_absolute_error: 0.2860 - val_nse: 0.4082\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.14078\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.2256 - mean_absolute_error: 0.2899 - nse: 0.5607 - val_loss: 0.1592 - val_mean_absolute_error: 0.3095 - val_nse: 0.3663\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.14078\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.2452 - mean_absolute_error: 0.2939 - nse: 0.6066 - val_loss: 0.1758 - val_mean_absolute_error: 0.3156 - val_nse: 0.2906\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.14078\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.2226 - mean_absolute_error: 0.2948 - nse: 0.6012 - val_loss: 0.1676 - val_mean_absolute_error: 0.3162 - val_nse: 0.3425\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.14078\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.2824 - mean_absolute_error: 0.3115 - nse: 0.5552 - val_loss: 0.1524 - val_mean_absolute_error: 0.2945 - val_nse: 0.2923\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.14078\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.2880 - mean_absolute_error: 0.3134 - nse: 0.6162 - val_loss: 0.1535 - val_mean_absolute_error: 0.3024 - val_nse: 0.3138\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.14078\n",
      "Epoch 53/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.2938 - mean_absolute_error: 0.3414 - nse: 0.5667 - val_loss: 0.1572 - val_mean_absolute_error: 0.3088 - val_nse: 0.3763\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.14078\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.2516 - mean_absolute_error: 0.3025 - nse: 0.5573 - val_loss: 0.1432 - val_mean_absolute_error: 0.2889 - val_nse: 0.3657\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.14078\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.2391 - mean_absolute_error: 0.2882 - nse: 0.6527 - val_loss: 0.1486 - val_mean_absolute_error: 0.2942 - val_nse: 0.3804\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.14078\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.2672 - mean_absolute_error: 0.3005 - nse: 0.5945 - val_loss: 0.1464 - val_mean_absolute_error: 0.2903 - val_nse: 0.3629\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.14078\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.1583 - mean_absolute_error: 0.2469 - nse: 0.6499 - val_loss: 0.1381 - val_mean_absolute_error: 0.2881 - val_nse: 0.3181\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.14078 to 0.13807, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 58/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.2092 - mean_absolute_error: 0.2766 - nse: 0.6123 - val_loss: 0.1516 - val_mean_absolute_error: 0.3046 - val_nse: 0.3666\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.13807\n",
      "Epoch 59/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.1678 - mean_absolute_error: 0.2641 - nse: 0.6018 - val_loss: 0.1466 - val_mean_absolute_error: 0.2965 - val_nse: 0.3755\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.13807\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.2069 - mean_absolute_error: 0.2830 - nse: 0.6265 - val_loss: 0.1397 - val_mean_absolute_error: 0.2872 - val_nse: 0.3130\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.13807\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.2700 - mean_absolute_error: 0.3134 - nse: 0.5622 - val_loss: 0.1386 - val_mean_absolute_error: 0.2870 - val_nse: 0.2822\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.13807\n",
      "Epoch 62/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.2341 - mean_absolute_error: 0.3000 - nse: 0.6097 - val_loss: 0.1373 - val_mean_absolute_error: 0.2827 - val_nse: 0.3772\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.13807 to 0.13726, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.2021 - mean_absolute_error: 0.2734 - nse: 0.6772 - val_loss: 0.1559 - val_mean_absolute_error: 0.2983 - val_nse: 0.2735\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.13726\n",
      "Epoch 64/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.1682 - mean_absolute_error: 0.2649 - nse: 0.6141 - val_loss: 0.1533 - val_mean_absolute_error: 0.2997 - val_nse: 0.3867\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.13726\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.2443 - mean_absolute_error: 0.2957 - nse: 0.6170 - val_loss: 0.1731 - val_mean_absolute_error: 0.3187 - val_nse: 0.2856\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.13726\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.2049 - mean_absolute_error: 0.2713 - nse: 0.6461 - val_loss: 0.1472 - val_mean_absolute_error: 0.2970 - val_nse: 0.3620\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.13726\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.2833 - mean_absolute_error: 0.3119 - nse: 0.6020 - val_loss: 0.1598 - val_mean_absolute_error: 0.3092 - val_nse: 0.3274\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.13726\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.1781 - mean_absolute_error: 0.2675 - nse: 0.6248 - val_loss: 0.1655 - val_mean_absolute_error: 0.3128 - val_nse: 0.2902\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.13726\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.3103 - mean_absolute_error: 0.3395 - nse: 0.5741 - val_loss: 0.1631 - val_mean_absolute_error: 0.3130 - val_nse: 0.3401\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.13726\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.2892 - mean_absolute_error: 0.3171 - nse: 0.5832 - val_loss: 0.1436 - val_mean_absolute_error: 0.2897 - val_nse: 0.3664\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.13726\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.2286 - mean_absolute_error: 0.2941 - nse: 0.6130 - val_loss: 0.1590 - val_mean_absolute_error: 0.3153 - val_nse: 0.4115\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.13726\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.2986 - mean_absolute_error: 0.3137 - nse: 0.6217 - val_loss: 0.1515 - val_mean_absolute_error: 0.2963 - val_nse: 0.3768\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.13726\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.2393 - mean_absolute_error: 0.2883 - nse: 0.6273 - val_loss: 0.1485 - val_mean_absolute_error: 0.2956 - val_nse: 0.3595\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.13726\n",
      "Epoch 74/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 254ms/step - loss: 0.1578 - mean_absolute_error: 0.2527 - nse: 0.6251 - val_loss: 0.1635 - val_mean_absolute_error: 0.3156 - val_nse: 0.3745\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.13726\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.2380 - mean_absolute_error: 0.2824 - nse: 0.6175 - val_loss: 0.1754 - val_mean_absolute_error: 0.3303 - val_nse: 0.2555\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.13726\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.2200 - mean_absolute_error: 0.2901 - nse: 0.5992 - val_loss: 0.1844 - val_mean_absolute_error: 0.3373 - val_nse: 0.3236\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.13726\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1946 - mean_absolute_error: 0.2685 - nse: 0.6214 - val_loss: 0.1742 - val_mean_absolute_error: 0.3150 - val_nse: 0.3562\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.13726\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.2557 - mean_absolute_error: 0.2927 - nse: 0.6250 - val_loss: 0.1547 - val_mean_absolute_error: 0.3008 - val_nse: 0.2400\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.13726\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.2529 - mean_absolute_error: 0.3061 - nse: 0.5984 - val_loss: 0.1789 - val_mean_absolute_error: 0.3267 - val_nse: 0.2735\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.13726\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.2704 - mean_absolute_error: 0.3262 - nse: 0.6173 - val_loss: 0.1634 - val_mean_absolute_error: 0.3165 - val_nse: 0.2989\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.13726\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.2004 - mean_absolute_error: 0.2626 - nse: 0.6487 - val_loss: 0.1536 - val_mean_absolute_error: 0.3054 - val_nse: 0.3708\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.13726\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.2381 - mean_absolute_error: 0.2772 - nse: 0.6433 - val_loss: 0.1605 - val_mean_absolute_error: 0.3088 - val_nse: 0.2901\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.13726\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.2477 - mean_absolute_error: 0.2945 - nse: 0.6172 - val_loss: 0.1703 - val_mean_absolute_error: 0.3207 - val_nse: 0.2831\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.13726\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.2202 - mean_absolute_error: 0.2813 - nse: 0.6647 - val_loss: 0.1526 - val_mean_absolute_error: 0.3028 - val_nse: 0.3736\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.13726\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.1774 - mean_absolute_error: 0.2509 - nse: 0.6529 - val_loss: 0.1662 - val_mean_absolute_error: 0.3209 - val_nse: 0.2565\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.13726\n",
      "Epoch 86/250\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.1965 - mean_absolute_error: 0.2723 - nse: 0.6613 - val_loss: 0.1785 - val_mean_absolute_error: 0.3260 - val_nse: 0.3051\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.13726\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.2244 - mean_absolute_error: 0.2890 - nse: 0.6502 - val_loss: 0.1869 - val_mean_absolute_error: 0.3415 - val_nse: 0.2367\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.13726\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.2197 - mean_absolute_error: 0.2835 - nse: 0.6286 - val_loss: 0.1511 - val_mean_absolute_error: 0.2995 - val_nse: 0.3411\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.13726\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.2091 - mean_absolute_error: 0.2662 - nse: 0.6594 - val_loss: 0.1781 - val_mean_absolute_error: 0.3310 - val_nse: 0.2548\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.13726\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.2932 - mean_absolute_error: 0.3201 - nse: 0.6393 - val_loss: 0.1461 - val_mean_absolute_error: 0.3008 - val_nse: 0.3799\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.13726\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.2172 - mean_absolute_error: 0.2717 - nse: 0.6469 - val_loss: 0.1630 - val_mean_absolute_error: 0.3180 - val_nse: 0.3114\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.13726\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1817 - mean_absolute_error: 0.2487 - nse: 0.7020 - val_loss: 0.1414 - val_mean_absolute_error: 0.2922 - val_nse: 0.3491\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.13726\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.2139 - mean_absolute_error: 0.2612 - nse: 0.6555 - val_loss: 0.1682 - val_mean_absolute_error: 0.3225 - val_nse: 0.3264\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.13726\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.1829 - mean_absolute_error: 0.2495 - nse: 0.6752 - val_loss: 0.1472 - val_mean_absolute_error: 0.3043 - val_nse: 0.3094\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.13726\n",
      "Epoch 95/250\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.1762 - mean_absolute_error: 0.2554 - nse: 0.6812 - val_loss: 0.1547 - val_mean_absolute_error: 0.3143 - val_nse: 0.2715\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.13726\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1356 - mean_absolute_error: 0.2372 - nse: 0.6598 - val_loss: 0.1605 - val_mean_absolute_error: 0.3188 - val_nse: 0.3500\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.13726\n",
      "Epoch 97/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.1715 - mean_absolute_error: 0.2636 - nse: 0.6709 - val_loss: 0.1528 - val_mean_absolute_error: 0.3121 - val_nse: 0.2821\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.13726\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.2212 - mean_absolute_error: 0.2797 - nse: 0.6611 - val_loss: 0.1496 - val_mean_absolute_error: 0.3062 - val_nse: 0.2719\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.13726\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.1855 - mean_absolute_error: 0.2649 - nse: 0.6404 - val_loss: 0.1480 - val_mean_absolute_error: 0.3005 - val_nse: 0.2842\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.13726\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.1857 - mean_absolute_error: 0.2605 - nse: 0.7073 - val_loss: 0.1622 - val_mean_absolute_error: 0.3149 - val_nse: 0.2547\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.13726\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1736 - mean_absolute_error: 0.2600 - nse: 0.6849 - val_loss: 0.1648 - val_mean_absolute_error: 0.3175 - val_nse: 0.3153\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.13726\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1891 - mean_absolute_error: 0.2651 - nse: 0.6819 - val_loss: 0.1594 - val_mean_absolute_error: 0.3069 - val_nse: 0.3332\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.13726\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.1748 - mean_absolute_error: 0.2515 - nse: 0.6989 - val_loss: 0.1776 - val_mean_absolute_error: 0.3373 - val_nse: 0.3046\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.13726\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.2362 - mean_absolute_error: 0.2874 - nse: 0.6736 - val_loss: 0.1484 - val_mean_absolute_error: 0.3015 - val_nse: 0.2794\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.13726\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1703 - mean_absolute_error: 0.2580 - nse: 0.6557 - val_loss: 0.1697 - val_mean_absolute_error: 0.3259 - val_nse: 0.2832\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.13726\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.2112 - mean_absolute_error: 0.2785 - nse: 0.6719 - val_loss: 0.1834 - val_mean_absolute_error: 0.3366 - val_nse: 0.2873\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.13726\n",
      "Epoch 107/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 267ms/step - loss: 0.2677 - mean_absolute_error: 0.2940 - nse: 0.6365 - val_loss: 0.1339 - val_mean_absolute_error: 0.2902 - val_nse: 0.3698\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.13726 to 0.13390, saving model to save/yeong/models/toc/multi_lstm.ckpt\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.1470 - mean_absolute_error: 0.2435 - nse: 0.6903 - val_loss: 0.1748 - val_mean_absolute_error: 0.3371 - val_nse: 0.3585\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.13390\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.2578 - mean_absolute_error: 0.2956 - nse: 0.6769 - val_loss: 0.1504 - val_mean_absolute_error: 0.3042 - val_nse: 0.3811\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.13390\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.2257 - mean_absolute_error: 0.2819 - nse: 0.6779 - val_loss: 0.1646 - val_mean_absolute_error: 0.3186 - val_nse: 0.3086\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.13390\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1614 - mean_absolute_error: 0.2539 - nse: 0.6809 - val_loss: 0.1550 - val_mean_absolute_error: 0.3092 - val_nse: 0.3811\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.13390\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1753 - mean_absolute_error: 0.2484 - nse: 0.6866 - val_loss: 0.1996 - val_mean_absolute_error: 0.3590 - val_nse: 0.1856\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.13390\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1504 - mean_absolute_error: 0.2386 - nse: 0.7063 - val_loss: 0.1889 - val_mean_absolute_error: 0.3505 - val_nse: 0.2885\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.13390\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.2002 - mean_absolute_error: 0.2771 - nse: 0.6694 - val_loss: 0.1704 - val_mean_absolute_error: 0.3156 - val_nse: 0.3664\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.13390\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.2140 - mean_absolute_error: 0.2779 - nse: 0.6844 - val_loss: 0.1654 - val_mean_absolute_error: 0.3179 - val_nse: 0.2580\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.13390\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1700 - mean_absolute_error: 0.2526 - nse: 0.6819 - val_loss: 0.1761 - val_mean_absolute_error: 0.3283 - val_nse: 0.2254\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.13390\n",
      "Epoch 117/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.2296 - mean_absolute_error: 0.2932 - nse: 0.6767 - val_loss: 0.1504 - val_mean_absolute_error: 0.3126 - val_nse: 0.2984\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.13390\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.2095 - mean_absolute_error: 0.2775 - nse: 0.6903 - val_loss: 0.1789 - val_mean_absolute_error: 0.3381 - val_nse: 0.2922\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.13390\n",
      "Epoch 119/250\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.1852 - mean_absolute_error: 0.2559 - nse: 0.7015 - val_loss: 0.1565 - val_mean_absolute_error: 0.3108 - val_nse: 0.3357\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.13390\n",
      "Epoch 120/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1881 - mean_absolute_error: 0.2643 - nse: 0.6755 - val_loss: 0.1827 - val_mean_absolute_error: 0.3418 - val_nse: 0.2438\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.13390\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.2016 - mean_absolute_error: 0.2718 - nse: 0.7064 - val_loss: 0.1695 - val_mean_absolute_error: 0.3258 - val_nse: 0.2937\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.13390\n",
      "Epoch 122/250\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.1598 - mean_absolute_error: 0.2421 - nse: 0.7075 - val_loss: 0.1543 - val_mean_absolute_error: 0.3155 - val_nse: 0.3186\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.13390\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.1785 - mean_absolute_error: 0.2520 - nse: 0.7030 - val_loss: 0.1950 - val_mean_absolute_error: 0.3515 - val_nse: 0.2350\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.13390\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1924 - mean_absolute_error: 0.2583 - nse: 0.7085 - val_loss: 0.1970 - val_mean_absolute_error: 0.3566 - val_nse: 0.2369\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.13390\n",
      "Epoch 125/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1959 - mean_absolute_error: 0.2685 - nse: 0.6845 - val_loss: 0.1630 - val_mean_absolute_error: 0.3248 - val_nse: 0.2912\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.13390\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.1588 - mean_absolute_error: 0.2508 - nse: 0.6836 - val_loss: 0.1726 - val_mean_absolute_error: 0.3325 - val_nse: 0.2010\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.13390\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.2198 - mean_absolute_error: 0.2822 - nse: 0.7117 - val_loss: 0.1701 - val_mean_absolute_error: 0.3331 - val_nse: 0.2473\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.13390\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.2075 - mean_absolute_error: 0.2783 - nse: 0.6825 - val_loss: 0.1887 - val_mean_absolute_error: 0.3494 - val_nse: 0.2758\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.13390\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.1685 - mean_absolute_error: 0.2505 - nse: 0.7007 - val_loss: 0.1599 - val_mean_absolute_error: 0.3161 - val_nse: 0.2953\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.13390\n",
      "Epoch 130/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1729 - mean_absolute_error: 0.2502 - nse: 0.7424 - val_loss: 0.1659 - val_mean_absolute_error: 0.3277 - val_nse: 0.3241\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.13390\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.1805 - mean_absolute_error: 0.2488 - nse: 0.7087 - val_loss: 0.1589 - val_mean_absolute_error: 0.3128 - val_nse: 0.2907\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.13390\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1625 - mean_absolute_error: 0.2349 - nse: 0.7107 - val_loss: 0.1561 - val_mean_absolute_error: 0.3194 - val_nse: 0.2353\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.13390\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.1223 - mean_absolute_error: 0.2207 - nse: 0.7166 - val_loss: 0.1632 - val_mean_absolute_error: 0.3218 - val_nse: 0.2986\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.13390\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.1401 - mean_absolute_error: 0.2438 - nse: 0.7081 - val_loss: 0.1540 - val_mean_absolute_error: 0.3197 - val_nse: 0.2879\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.13390\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.1712 - mean_absolute_error: 0.2603 - nse: 0.7079 - val_loss: 0.1664 - val_mean_absolute_error: 0.3256 - val_nse: 0.2391\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.13390\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.2027 - mean_absolute_error: 0.2764 - nse: 0.6808 - val_loss: 0.1515 - val_mean_absolute_error: 0.3075 - val_nse: 0.2209\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.13390\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1452 - mean_absolute_error: 0.2387 - nse: 0.7292 - val_loss: 0.1686 - val_mean_absolute_error: 0.3291 - val_nse: 0.2678\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.13390\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1611 - mean_absolute_error: 0.2449 - nse: 0.7376 - val_loss: 0.1703 - val_mean_absolute_error: 0.3234 - val_nse: 0.1963\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.13390\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1493 - mean_absolute_error: 0.2426 - nse: 0.6876 - val_loss: 0.1712 - val_mean_absolute_error: 0.3294 - val_nse: 0.3447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00139: val_loss did not improve from 0.13390\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.2001 - mean_absolute_error: 0.2595 - nse: 0.7216 - val_loss: 0.1868 - val_mean_absolute_error: 0.3454 - val_nse: 0.2003\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.13390\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 0.1756 - mean_absolute_error: 0.2497 - nse: 0.7241 - val_loss: 0.1781 - val_mean_absolute_error: 0.3428 - val_nse: 0.2373\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.13390\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1567 - mean_absolute_error: 0.2481 - nse: 0.6931 - val_loss: 0.1860 - val_mean_absolute_error: 0.3474 - val_nse: 0.1666\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.13390\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.1741 - mean_absolute_error: 0.2632 - nse: 0.7145 - val_loss: 0.2025 - val_mean_absolute_error: 0.3611 - val_nse: 0.2147\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.13390\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.2246 - mean_absolute_error: 0.2869 - nse: 0.6712 - val_loss: 0.1682 - val_mean_absolute_error: 0.3312 - val_nse: 0.2435\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.13390\n",
      "Epoch 145/250\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.1754 - mean_absolute_error: 0.2505 - nse: 0.7247 - val_loss: 0.1845 - val_mean_absolute_error: 0.3504 - val_nse: 0.2747\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.13390\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.2010 - mean_absolute_error: 0.2633 - nse: 0.7211 - val_loss: 0.1706 - val_mean_absolute_error: 0.3338 - val_nse: 0.3216\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.13390\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.2314 - mean_absolute_error: 0.2851 - nse: 0.6958 - val_loss: 0.1793 - val_mean_absolute_error: 0.3392 - val_nse: 0.2317\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.13390\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.1393 - mean_absolute_error: 0.2333 - nse: 0.7295 - val_loss: 0.1691 - val_mean_absolute_error: 0.3311 - val_nse: 0.3001\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.13390\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.1230 - mean_absolute_error: 0.2228 - nse: 0.7221 - val_loss: 0.2117 - val_mean_absolute_error: 0.3724 - val_nse: 0.1345\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.13390\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1629 - mean_absolute_error: 0.2384 - nse: 0.7293 - val_loss: 0.2024 - val_mean_absolute_error: 0.3654 - val_nse: 0.1969\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.13390\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.1678 - mean_absolute_error: 0.2482 - nse: 0.7249 - val_loss: 0.2024 - val_mean_absolute_error: 0.3556 - val_nse: 0.2761\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.13390\n",
      "Epoch 152/250\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.1606 - mean_absolute_error: 0.2460 - nse: 0.7371 - val_loss: 0.2021 - val_mean_absolute_error: 0.3581 - val_nse: 0.2293\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.13390\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.1534 - mean_absolute_error: 0.2357 - nse: 0.7280 - val_loss: 0.1926 - val_mean_absolute_error: 0.3457 - val_nse: 0.1112\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.13390\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.2245 - mean_absolute_error: 0.2903 - nse: 0.7059 - val_loss: 0.1769 - val_mean_absolute_error: 0.3388 - val_nse: 0.2122\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.13390\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.1672 - mean_absolute_error: 0.2586 - nse: 0.7331 - val_loss: 0.1922 - val_mean_absolute_error: 0.3532 - val_nse: 0.2155\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.13390\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.1443 - mean_absolute_error: 0.2334 - nse: 0.7434 - val_loss: 0.1742 - val_mean_absolute_error: 0.3345 - val_nse: 0.2597\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.13390\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1730 - mean_absolute_error: 0.2440 - nse: 0.7209 - val_loss: 0.1946 - val_mean_absolute_error: 0.3525 - val_nse: 0.2114\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.13390\n",
      "Epoch 158/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1822 - mean_absolute_error: 0.2556 - nse: 0.7318 - val_loss: 0.1868 - val_mean_absolute_error: 0.3461 - val_nse: 0.1792\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.13390\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1471 - mean_absolute_error: 0.2278 - nse: 0.7584 - val_loss: 0.1737 - val_mean_absolute_error: 0.3358 - val_nse: 0.2434\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.13390\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.1330 - mean_absolute_error: 0.2185 - nse: 0.7424 - val_loss: 0.1953 - val_mean_absolute_error: 0.3548 - val_nse: 0.2380\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.13390\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1891 - mean_absolute_error: 0.2558 - nse: 0.7463 - val_loss: 0.2009 - val_mean_absolute_error: 0.3560 - val_nse: 0.2169\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.13390\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1194 - mean_absolute_error: 0.2226 - nse: 0.7437 - val_loss: 0.1932 - val_mean_absolute_error: 0.3567 - val_nse: 0.2029\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.13390\n",
      "Epoch 163/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1723 - mean_absolute_error: 0.2475 - nse: 0.7182 - val_loss: 0.1813 - val_mean_absolute_error: 0.3393 - val_nse: 0.1712\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.13390\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.2045 - mean_absolute_error: 0.2676 - nse: 0.7451 - val_loss: 0.1880 - val_mean_absolute_error: 0.3536 - val_nse: 0.1535\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.13390\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1669 - mean_absolute_error: 0.2572 - nse: 0.7296 - val_loss: 0.1948 - val_mean_absolute_error: 0.3579 - val_nse: 0.2190\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.13390\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1558 - mean_absolute_error: 0.2283 - nse: 0.7373 - val_loss: 0.1754 - val_mean_absolute_error: 0.3343 - val_nse: 0.2496\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.13390\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.1520 - mean_absolute_error: 0.2319 - nse: 0.7755 - val_loss: 0.1840 - val_mean_absolute_error: 0.3444 - val_nse: 0.2382\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.13390\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1739 - mean_absolute_error: 0.2385 - nse: 0.7403 - val_loss: 0.1845 - val_mean_absolute_error: 0.3455 - val_nse: 0.2128\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.13390\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.1071 - mean_absolute_error: 0.2057 - nse: 0.7652 - val_loss: 0.1676 - val_mean_absolute_error: 0.3270 - val_nse: 0.1540\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.13390\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1315 - mean_absolute_error: 0.2171 - nse: 0.7414 - val_loss: 0.1907 - val_mean_absolute_error: 0.3566 - val_nse: 0.1801\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.13390\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1102 - mean_absolute_error: 0.2100 - nse: 0.7561 - val_loss: 0.1719 - val_mean_absolute_error: 0.3393 - val_nse: 0.2259\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.13390\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1314 - mean_absolute_error: 0.2332 - nse: 0.7638 - val_loss: 0.1898 - val_mean_absolute_error: 0.3512 - val_nse: 0.1184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00172: val_loss did not improve from 0.13390\n",
      "Epoch 173/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1738 - mean_absolute_error: 0.2455 - nse: 0.7304 - val_loss: 0.1731 - val_mean_absolute_error: 0.3348 - val_nse: 0.0840\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.13390\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.1324 - mean_absolute_error: 0.2282 - nse: 0.7571 - val_loss: 0.1810 - val_mean_absolute_error: 0.3444 - val_nse: 0.1761\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.13390\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1385 - mean_absolute_error: 0.2269 - nse: 0.7835 - val_loss: 0.1893 - val_mean_absolute_error: 0.3454 - val_nse: 0.1396\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.13390\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1086 - mean_absolute_error: 0.2094 - nse: 0.7433 - val_loss: 0.1849 - val_mean_absolute_error: 0.3462 - val_nse: 0.2728\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.13390\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1770 - mean_absolute_error: 0.2532 - nse: 0.7600 - val_loss: 0.2010 - val_mean_absolute_error: 0.3582 - val_nse: 0.1216\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.13390\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1155 - mean_absolute_error: 0.2062 - nse: 0.7839 - val_loss: 0.1933 - val_mean_absolute_error: 0.3615 - val_nse: 0.1710\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.13390\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.1740 - mean_absolute_error: 0.2528 - nse: 0.7401 - val_loss: 0.2129 - val_mean_absolute_error: 0.3743 - val_nse: 0.0861\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.13390\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.1208 - mean_absolute_error: 0.2206 - nse: 0.7542 - val_loss: 0.1973 - val_mean_absolute_error: 0.3568 - val_nse: 0.1993\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.13390\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.2162 - mean_absolute_error: 0.2697 - nse: 0.7208 - val_loss: 0.1925 - val_mean_absolute_error: 0.3526 - val_nse: 0.1813\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.13390\n",
      "Epoch 182/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1671 - mean_absolute_error: 0.2373 - nse: 0.7427 - val_loss: 0.1933 - val_mean_absolute_error: 0.3591 - val_nse: 0.1649\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.13390\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1671 - mean_absolute_error: 0.2383 - nse: 0.7627 - val_loss: 0.1942 - val_mean_absolute_error: 0.3643 - val_nse: 0.2773\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.13390\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.1696 - mean_absolute_error: 0.2454 - nse: 0.7521 - val_loss: 0.1858 - val_mean_absolute_error: 0.3463 - val_nse: 0.2077\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.13390\n",
      "Epoch 185/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.1479 - mean_absolute_error: 0.2271 - nse: 0.7633 - val_loss: 0.1827 - val_mean_absolute_error: 0.3477 - val_nse: 0.2308\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.13390\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0973 - mean_absolute_error: 0.2076 - nse: 0.7554 - val_loss: 0.2048 - val_mean_absolute_error: 0.3660 - val_nse: 0.2227\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.13390\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1480 - mean_absolute_error: 0.2276 - nse: 0.7655 - val_loss: 0.2202 - val_mean_absolute_error: 0.3839 - val_nse: 0.0958\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.13390\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1296 - mean_absolute_error: 0.2242 - nse: 0.7719 - val_loss: 0.2111 - val_mean_absolute_error: 0.3702 - val_nse: 0.2102\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.13390\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1283 - mean_absolute_error: 0.2171 - nse: 0.7646 - val_loss: 0.2034 - val_mean_absolute_error: 0.3586 - val_nse: 0.2577\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.13390\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1619 - mean_absolute_error: 0.2340 - nse: 0.7539 - val_loss: 0.1930 - val_mean_absolute_error: 0.3487 - val_nse: 0.0858\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.13390\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1624 - mean_absolute_error: 0.2402 - nse: 0.7549 - val_loss: 0.1954 - val_mean_absolute_error: 0.3530 - val_nse: 0.1327\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.13390\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.1655 - mean_absolute_error: 0.2522 - nse: 0.7567 - val_loss: 0.2012 - val_mean_absolute_error: 0.3651 - val_nse: 0.1898\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.13390\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.1337 - mean_absolute_error: 0.2139 - nse: 0.7736 - val_loss: 0.1934 - val_mean_absolute_error: 0.3564 - val_nse: 0.1922\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.13390\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.1565 - mean_absolute_error: 0.2286 - nse: 0.7520 - val_loss: 0.1916 - val_mean_absolute_error: 0.3505 - val_nse: 0.1414\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.13390\n",
      "Epoch 195/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1705 - mean_absolute_error: 0.2428 - nse: 0.7511 - val_loss: 0.2090 - val_mean_absolute_error: 0.3689 - val_nse: 0.1299\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.13390\n",
      "Epoch 196/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.1453 - mean_absolute_error: 0.2342 - nse: 0.7861 - val_loss: 0.1911 - val_mean_absolute_error: 0.3519 - val_nse: 0.1885\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.13390\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.0968 - mean_absolute_error: 0.1865 - nse: 0.7698 - val_loss: 0.1986 - val_mean_absolute_error: 0.3595 - val_nse: 0.1495\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.13390\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1312 - mean_absolute_error: 0.2238 - nse: 0.7837 - val_loss: 0.2100 - val_mean_absolute_error: 0.3674 - val_nse: 0.1717\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.13390\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.1426 - mean_absolute_error: 0.2294 - nse: 0.7747 - val_loss: 0.2123 - val_mean_absolute_error: 0.3740 - val_nse: 0.1790\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.13390\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.1459 - mean_absolute_error: 0.2311 - nse: 0.7479 - val_loss: 0.1961 - val_mean_absolute_error: 0.3558 - val_nse: 0.0913\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.13390\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.1647 - mean_absolute_error: 0.2319 - nse: 0.7801 - val_loss: 0.2112 - val_mean_absolute_error: 0.3751 - val_nse: 0.1269\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.13390\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1734 - mean_absolute_error: 0.2485 - nse: 0.7624 - val_loss: 0.1904 - val_mean_absolute_error: 0.3529 - val_nse: 0.1993\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.13390\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1401 - mean_absolute_error: 0.2209 - nse: 0.7585 - val_loss: 0.2052 - val_mean_absolute_error: 0.3646 - val_nse: 0.1508\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.13390\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1266 - mean_absolute_error: 0.2149 - nse: 0.7962 - val_loss: 0.1666 - val_mean_absolute_error: 0.3260 - val_nse: 0.2065\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.13390\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1525 - mean_absolute_error: 0.2234 - nse: 0.7771 - val_loss: 0.2077 - val_mean_absolute_error: 0.3668 - val_nse: 0.1825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00205: val_loss did not improve from 0.13390\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.1092 - mean_absolute_error: 0.1986 - nse: 0.7860 - val_loss: 0.1883 - val_mean_absolute_error: 0.3451 - val_nse: 0.0960\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.13390\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1231 - mean_absolute_error: 0.2137 - nse: 0.7703 - val_loss: 0.1979 - val_mean_absolute_error: 0.3642 - val_nse: 0.1188\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.13390\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0865 - mean_absolute_error: 0.1901 - nse: 0.7787 - val_loss: 0.1789 - val_mean_absolute_error: 0.3421 - val_nse: 0.2355\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.13390\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.1169 - mean_absolute_error: 0.2179 - nse: 0.7720 - val_loss: 0.2017 - val_mean_absolute_error: 0.3618 - val_nse: 0.0418\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.13390\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.1651 - mean_absolute_error: 0.2439 - nse: 0.7584 - val_loss: 0.1898 - val_mean_absolute_error: 0.3530 - val_nse: 0.0532\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.13390\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1137 - mean_absolute_error: 0.2136 - nse: 0.7792 - val_loss: 0.1849 - val_mean_absolute_error: 0.3445 - val_nse: 0.1387\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.13390\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1335 - mean_absolute_error: 0.2161 - nse: 0.8001 - val_loss: 0.2078 - val_mean_absolute_error: 0.3640 - val_nse: 0.0693\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.13390\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.1098 - mean_absolute_error: 0.2130 - nse: 0.7794 - val_loss: 0.1794 - val_mean_absolute_error: 0.3388 - val_nse: 0.2480\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.13390\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.1358 - mean_absolute_error: 0.2254 - nse: 0.7779 - val_loss: 0.2015 - val_mean_absolute_error: 0.3556 - val_nse: 0.1272\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.13390\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.1118 - mean_absolute_error: 0.2053 - nse: 0.8000 - val_loss: 0.2207 - val_mean_absolute_error: 0.3856 - val_nse: 0.1225\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.13390\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1715 - mean_absolute_error: 0.2441 - nse: 0.7716 - val_loss: 0.2031 - val_mean_absolute_error: 0.3659 - val_nse: 0.0872\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.13390\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.1075 - mean_absolute_error: 0.2049 - nse: 0.7600 - val_loss: 0.2023 - val_mean_absolute_error: 0.3594 - val_nse: 0.1364\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.13390\n",
      "Epoch 218/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1533 - mean_absolute_error: 0.2309 - nse: 0.7597 - val_loss: 0.2176 - val_mean_absolute_error: 0.3767 - val_nse: 0.1211\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.13390\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1828 - mean_absolute_error: 0.2437 - nse: 0.7573 - val_loss: 0.1830 - val_mean_absolute_error: 0.3492 - val_nse: 0.1799\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.13390\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1187 - mean_absolute_error: 0.2095 - nse: 0.7726 - val_loss: 0.2062 - val_mean_absolute_error: 0.3697 - val_nse: 0.2347\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.13390\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1659 - mean_absolute_error: 0.2362 - nse: 0.7924 - val_loss: 0.1987 - val_mean_absolute_error: 0.3620 - val_nse: 0.2144\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.13390\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1440 - mean_absolute_error: 0.2248 - nse: 0.7814 - val_loss: 0.1939 - val_mean_absolute_error: 0.3554 - val_nse: 0.1459\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.13390\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1078 - mean_absolute_error: 0.2076 - nse: 0.7803 - val_loss: 0.1941 - val_mean_absolute_error: 0.3577 - val_nse: 0.2587\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.13390\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.1245 - mean_absolute_error: 0.2098 - nse: 0.7883 - val_loss: 0.2332 - val_mean_absolute_error: 0.3905 - val_nse: 0.0103\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.13390\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.1114 - mean_absolute_error: 0.2079 - nse: 0.7914 - val_loss: 0.2290 - val_mean_absolute_error: 0.3884 - val_nse: 0.1377\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.13390\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.1366 - mean_absolute_error: 0.2214 - nse: 0.7820 - val_loss: 0.1999 - val_mean_absolute_error: 0.3531 - val_nse: 0.2596\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.13390\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.1300 - mean_absolute_error: 0.2083 - nse: 0.7856 - val_loss: 0.2108 - val_mean_absolute_error: 0.3679 - val_nse: 0.0378\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.13390\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1350 - mean_absolute_error: 0.2206 - nse: 0.7809 - val_loss: 0.2123 - val_mean_absolute_error: 0.3621 - val_nse: 0.0874\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.13390\n",
      "Epoch 229/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.1612 - mean_absolute_error: 0.2428 - nse: 0.7773 - val_loss: 0.1978 - val_mean_absolute_error: 0.3614 - val_nse: 0.1270\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.13390\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.1201 - mean_absolute_error: 0.2134 - nse: 0.7985 - val_loss: 0.2155 - val_mean_absolute_error: 0.3781 - val_nse: 0.1645\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.13390\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1463 - mean_absolute_error: 0.2240 - nse: 0.7772 - val_loss: 0.1915 - val_mean_absolute_error: 0.3438 - val_nse: 0.1402\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.13390\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1319 - mean_absolute_error: 0.2179 - nse: 0.7645 - val_loss: 0.2201 - val_mean_absolute_error: 0.3787 - val_nse: 0.0826\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.13390\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.1409 - mean_absolute_error: 0.2210 - nse: 0.7959 - val_loss: 0.2154 - val_mean_absolute_error: 0.3699 - val_nse: 0.0985\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.13390\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.1062 - mean_absolute_error: 0.1926 - nse: 0.7990 - val_loss: 0.2091 - val_mean_absolute_error: 0.3668 - val_nse: 0.0806\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.13390\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.1318 - mean_absolute_error: 0.2142 - nse: 0.7936 - val_loss: 0.2121 - val_mean_absolute_error: 0.3647 - val_nse: 0.1636\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.13390\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.1237 - mean_absolute_error: 0.2069 - nse: 0.7996 - val_loss: 0.2181 - val_mean_absolute_error: 0.3781 - val_nse: 0.1228\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.13390\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.1394 - mean_absolute_error: 0.2248 - nse: 0.7754 - val_loss: 0.2074 - val_mean_absolute_error: 0.3667 - val_nse: 0.0768\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.13390\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.1021 - mean_absolute_error: 0.1950 - nse: 0.8007 - val_loss: 0.2119 - val_mean_absolute_error: 0.3707 - val_nse: 0.1077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00238: val_loss did not improve from 0.13390\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1728 - mean_absolute_error: 0.2417 - nse: 0.7913 - val_loss: 0.2003 - val_mean_absolute_error: 0.3618 - val_nse: 0.0654\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.13390\n",
      "Epoch 240/250\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.1363 - mean_absolute_error: 0.2212 - nse: 0.7838 - val_loss: 0.2214 - val_mean_absolute_error: 0.3788 - val_nse: 0.1634\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.13390\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.1167 - mean_absolute_error: 0.2062 - nse: 0.8053 - val_loss: 0.1883 - val_mean_absolute_error: 0.3449 - val_nse: 0.1667\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.13390\n",
      "Epoch 242/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1198 - mean_absolute_error: 0.2011 - nse: 0.8081 - val_loss: 0.2117 - val_mean_absolute_error: 0.3709 - val_nse: 0.1069\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.13390\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1132 - mean_absolute_error: 0.1975 - nse: 0.8020 - val_loss: 0.2036 - val_mean_absolute_error: 0.3560 - val_nse: 0.1125\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.13390\n",
      "Epoch 244/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1238 - mean_absolute_error: 0.2043 - nse: 0.7906 - val_loss: 0.1930 - val_mean_absolute_error: 0.3553 - val_nse: 0.0563\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.13390\n",
      "Epoch 245/250\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0857 - mean_absolute_error: 0.1861 - nse: 0.7933 - val_loss: 0.1981 - val_mean_absolute_error: 0.3603 - val_nse: 0.1760\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.13390\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0946 - mean_absolute_error: 0.1991 - nse: 0.8017 - val_loss: 0.2045 - val_mean_absolute_error: 0.3662 - val_nse: 0.0619\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.13390\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1217 - mean_absolute_error: 0.2109 - nse: 0.7905 - val_loss: 0.2128 - val_mean_absolute_error: 0.3690 - val_nse: -0.0216\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.13390\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.1379 - mean_absolute_error: 0.2255 - nse: 0.7877 - val_loss: 0.1888 - val_mean_absolute_error: 0.3480 - val_nse: 0.0448\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.13390\n",
      "Epoch 249/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1063 - mean_absolute_error: 0.2004 - nse: 0.8114 - val_loss: 0.2122 - val_mean_absolute_error: 0.3685 - val_nse: 0.0621\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.13390\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1221 - mean_absolute_error: 0.2108 - nse: 0.7998 - val_loss: 0.2069 - val_mean_absolute_error: 0.3591 - val_nse: 0.0595\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.13390\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/250\n",
      "1/1 [==============================] - 1s 553ms/step - loss: 0.8872 - mean_absolute_error: 0.6600 - nse: -0.6183 - val_loss: 0.2780 - val_mean_absolute_error: 0.4273 - val_nse: -0.0571\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27800, saving model to save/yeong/models/toc/multi_conv.ckpt\n",
      "Epoch 2/250\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.7473 - mean_absolute_error: 0.5933 - nse: -0.5188 - val_loss: 0.2784 - val_mean_absolute_error: 0.4179 - val_nse: -0.1996\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.27800\n",
      "Epoch 3/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.6881 - mean_absolute_error: 0.5376 - nse: -0.0847 - val_loss: 0.3453 - val_mean_absolute_error: 0.4796 - val_nse: -0.3083\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.27800\n",
      "Epoch 4/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.4565 - mean_absolute_error: 0.4309 - nse: 0.2913 - val_loss: 0.4405 - val_mean_absolute_error: 0.5874 - val_nse: -1.1183\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.27800\n",
      "Epoch 5/250\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 0.5113 - mean_absolute_error: 0.5318 - nse: 0.2426 - val_loss: 0.5362 - val_mean_absolute_error: 0.6665 - val_nse: -1.1725\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.27800\n",
      "Epoch 6/250\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.4665 - mean_absolute_error: 0.4979 - nse: 0.1268 - val_loss: 0.5698 - val_mean_absolute_error: 0.6846 - val_nse: -1.4205\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.27800\n",
      "Epoch 7/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.3730 - mean_absolute_error: 0.4120 - nse: 0.4464 - val_loss: 0.5799 - val_mean_absolute_error: 0.6837 - val_nse: -1.8714\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.27800\n",
      "Epoch 8/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.3414 - mean_absolute_error: 0.3739 - nse: 0.4748 - val_loss: 0.4650 - val_mean_absolute_error: 0.6002 - val_nse: -0.9540\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.27800\n",
      "Epoch 9/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.3322 - mean_absolute_error: 0.3775 - nse: 0.4480 - val_loss: 0.3971 - val_mean_absolute_error: 0.5433 - val_nse: -0.6356\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.27800\n",
      "Epoch 10/250\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.3680 - mean_absolute_error: 0.3872 - nse: 0.4277 - val_loss: 0.3627 - val_mean_absolute_error: 0.5233 - val_nse: -0.5761\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.27800\n",
      "Epoch 11/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.3589 - mean_absolute_error: 0.4199 - nse: 0.3552 - val_loss: 0.3218 - val_mean_absolute_error: 0.4809 - val_nse: -0.3178\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.27800\n",
      "Epoch 12/250\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.3272 - mean_absolute_error: 0.3555 - nse: 0.4508 - val_loss: 0.3456 - val_mean_absolute_error: 0.5090 - val_nse: -0.4694\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.27800\n",
      "Epoch 13/250\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.2603 - mean_absolute_error: 0.3227 - nse: 0.4732 - val_loss: 0.2994 - val_mean_absolute_error: 0.4628 - val_nse: -0.3228\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.27800\n",
      "Epoch 14/250\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.2263 - mean_absolute_error: 0.2746 - nse: 0.5741 - val_loss: 0.2882 - val_mean_absolute_error: 0.4505 - val_nse: -0.1667\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.27800\n",
      "Epoch 15/250\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.1867 - mean_absolute_error: 0.2699 - nse: 0.6025 - val_loss: 0.2729 - val_mean_absolute_error: 0.4329 - val_nse: -0.1749\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.27800 to 0.27292, saving model to save/yeong/models/toc/multi_conv.ckpt\n",
      "Epoch 16/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.2532 - mean_absolute_error: 0.3227 - nse: 0.5731 - val_loss: 0.2914 - val_mean_absolute_error: 0.4592 - val_nse: -0.2385\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.27292\n",
      "Epoch 17/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.3494 - mean_absolute_error: 0.3955 - nse: 0.5542 - val_loss: 0.2576 - val_mean_absolute_error: 0.4172 - val_nse: -0.0981\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.27292 to 0.25760, saving model to save/yeong/models/toc/multi_conv.ckpt\n",
      "Epoch 18/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.3508 - mean_absolute_error: 0.3990 - nse: 0.5701 - val_loss: 0.2679 - val_mean_absolute_error: 0.4270 - val_nse: -0.1887\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.25760\n",
      "Epoch 19/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.2603 - mean_absolute_error: 0.3488 - nse: 0.5953 - val_loss: 0.2445 - val_mean_absolute_error: 0.4112 - val_nse: -0.0761\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.25760 to 0.24450, saving model to save/yeong/models/toc/multi_conv.ckpt\n",
      "Epoch 20/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 230ms/step - loss: 0.2917 - mean_absolute_error: 0.3406 - nse: 0.5983 - val_loss: 0.2680 - val_mean_absolute_error: 0.4313 - val_nse: -0.1261\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.24450\n",
      "Epoch 21/250\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.2285 - mean_absolute_error: 0.2999 - nse: 0.6374 - val_loss: 0.2472 - val_mean_absolute_error: 0.4044 - val_nse: -0.0696\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.24450\n",
      "Epoch 22/250\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.2321 - mean_absolute_error: 0.3034 - nse: 0.6217 - val_loss: 0.2575 - val_mean_absolute_error: 0.4101 - val_nse: -0.0759\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.24450\n",
      "Epoch 23/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.2587 - mean_absolute_error: 0.3154 - nse: 0.6451 - val_loss: 0.2765 - val_mean_absolute_error: 0.4230 - val_nse: -0.2525\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.24450\n",
      "Epoch 24/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.2903 - mean_absolute_error: 0.3395 - nse: 0.5933 - val_loss: 0.2740 - val_mean_absolute_error: 0.4206 - val_nse: -0.1714\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.24450\n",
      "Epoch 25/250\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.2005 - mean_absolute_error: 0.2862 - nse: 0.5882 - val_loss: 0.3172 - val_mean_absolute_error: 0.4517 - val_nse: -0.3925\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.24450\n",
      "Epoch 26/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1994 - mean_absolute_error: 0.2927 - nse: 0.6053 - val_loss: 0.2899 - val_mean_absolute_error: 0.4254 - val_nse: -0.2189\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.24450\n",
      "Epoch 27/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.2594 - mean_absolute_error: 0.3256 - nse: 0.6552 - val_loss: 0.3378 - val_mean_absolute_error: 0.4728 - val_nse: -0.3405\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.24450\n",
      "Epoch 28/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.2706 - mean_absolute_error: 0.3407 - nse: 0.6420 - val_loss: 0.3171 - val_mean_absolute_error: 0.4591 - val_nse: -0.2691\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.24450\n",
      "Epoch 29/250\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.2031 - mean_absolute_error: 0.2812 - nse: 0.6870 - val_loss: 0.3334 - val_mean_absolute_error: 0.4659 - val_nse: -0.3760\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.24450\n",
      "Epoch 30/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.1805 - mean_absolute_error: 0.2809 - nse: 0.6505 - val_loss: 0.3451 - val_mean_absolute_error: 0.4744 - val_nse: -0.3537\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.24450\n",
      "Epoch 31/250\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1893 - mean_absolute_error: 0.2921 - nse: 0.6238 - val_loss: 0.3429 - val_mean_absolute_error: 0.4701 - val_nse: -0.4936\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.24450\n",
      "Epoch 32/250\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.2118 - mean_absolute_error: 0.3066 - nse: 0.6583 - val_loss: 0.3017 - val_mean_absolute_error: 0.4442 - val_nse: -0.3284\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.24450\n",
      "Epoch 33/250\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.1966 - mean_absolute_error: 0.2932 - nse: 0.6008 - val_loss: 0.3059 - val_mean_absolute_error: 0.4454 - val_nse: -0.4712\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.24450\n",
      "Epoch 34/250\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.1836 - mean_absolute_error: 0.2846 - nse: 0.6337 - val_loss: 0.3122 - val_mean_absolute_error: 0.4504 - val_nse: -0.2342\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.24450\n",
      "Epoch 35/250\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.1620 - mean_absolute_error: 0.2530 - nse: 0.6567 - val_loss: 0.3103 - val_mean_absolute_error: 0.4509 - val_nse: -0.1682\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.24450\n",
      "Epoch 36/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1874 - mean_absolute_error: 0.2694 - nse: 0.6236 - val_loss: 0.3698 - val_mean_absolute_error: 0.4845 - val_nse: -0.7361\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.24450\n",
      "Epoch 37/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.2161 - mean_absolute_error: 0.2818 - nse: 0.6762 - val_loss: 0.3549 - val_mean_absolute_error: 0.4732 - val_nse: -0.6221\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.24450\n",
      "Epoch 38/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.2019 - mean_absolute_error: 0.2801 - nse: 0.6516 - val_loss: 0.4045 - val_mean_absolute_error: 0.5084 - val_nse: -0.6049\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.24450\n",
      "Epoch 39/250\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.1679 - mean_absolute_error: 0.2718 - nse: 0.6574 - val_loss: 0.4081 - val_mean_absolute_error: 0.5168 - val_nse: -0.6796\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.24450\n",
      "Epoch 40/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.1716 - mean_absolute_error: 0.2750 - nse: 0.6719 - val_loss: 0.4383 - val_mean_absolute_error: 0.5312 - val_nse: -0.7121\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.24450\n",
      "Epoch 41/250\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.2483 - mean_absolute_error: 0.3075 - nse: 0.6584 - val_loss: 0.4784 - val_mean_absolute_error: 0.5601 - val_nse: -1.1029\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.24450\n",
      "Epoch 42/250\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.2297 - mean_absolute_error: 0.2953 - nse: 0.6688 - val_loss: 0.4859 - val_mean_absolute_error: 0.5627 - val_nse: -1.1988\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.24450\n",
      "Epoch 43/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1833 - mean_absolute_error: 0.2818 - nse: 0.6328 - val_loss: 0.4347 - val_mean_absolute_error: 0.5297 - val_nse: -0.8308\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.24450\n",
      "Epoch 44/250\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.2084 - mean_absolute_error: 0.2939 - nse: 0.6663 - val_loss: 0.4978 - val_mean_absolute_error: 0.5666 - val_nse: -1.3330\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.24450\n",
      "Epoch 45/250\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.2208 - mean_absolute_error: 0.2796 - nse: 0.6833 - val_loss: 0.4033 - val_mean_absolute_error: 0.5130 - val_nse: -0.6232\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.24450\n",
      "Epoch 46/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.2359 - mean_absolute_error: 0.2877 - nse: 0.6481 - val_loss: 0.4579 - val_mean_absolute_error: 0.5266 - val_nse: -1.0030\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.24450\n",
      "Epoch 47/250\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.1658 - mean_absolute_error: 0.2549 - nse: 0.6683 - val_loss: 0.4391 - val_mean_absolute_error: 0.5136 - val_nse: -0.8202\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.24450\n",
      "Epoch 48/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.2259 - mean_absolute_error: 0.2952 - nse: 0.6680 - val_loss: 0.4675 - val_mean_absolute_error: 0.5367 - val_nse: -0.9768\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.24450\n",
      "Epoch 49/250\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.2192 - mean_absolute_error: 0.2982 - nse: 0.6414 - val_loss: 0.5223 - val_mean_absolute_error: 0.5775 - val_nse: -1.1635\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.24450\n",
      "Epoch 50/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1376 - mean_absolute_error: 0.2392 - nse: 0.6665 - val_loss: 0.5996 - val_mean_absolute_error: 0.6075 - val_nse: -1.6987\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.24450\n",
      "Epoch 51/250\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.1634 - mean_absolute_error: 0.2425 - nse: 0.7015 - val_loss: 0.6128 - val_mean_absolute_error: 0.6167 - val_nse: -1.5919\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.24450\n",
      "Epoch 52/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1617 - mean_absolute_error: 0.2558 - nse: 0.6815 - val_loss: 0.5305 - val_mean_absolute_error: 0.5659 - val_nse: -1.0652\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.24450\n",
      "Epoch 53/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1555 - mean_absolute_error: 0.2479 - nse: 0.6886 - val_loss: 0.6320 - val_mean_absolute_error: 0.6335 - val_nse: -1.8245\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.24450\n",
      "Epoch 54/250\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.2299 - mean_absolute_error: 0.3030 - nse: 0.7178 - val_loss: 0.6624 - val_mean_absolute_error: 0.6358 - val_nse: -1.8421\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.24450\n",
      "Epoch 55/250\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.2451 - mean_absolute_error: 0.3073 - nse: 0.6846 - val_loss: 0.5864 - val_mean_absolute_error: 0.6058 - val_nse: -1.4065\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.24450\n",
      "Epoch 56/250\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.2001 - mean_absolute_error: 0.2853 - nse: 0.7071 - val_loss: 0.5394 - val_mean_absolute_error: 0.5797 - val_nse: -1.3515\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.24450\n",
      "Epoch 57/250\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.2101 - mean_absolute_error: 0.2747 - nse: 0.7141 - val_loss: 0.6186 - val_mean_absolute_error: 0.6257 - val_nse: -1.6804\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.24450\n",
      "Epoch 58/250\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.1983 - mean_absolute_error: 0.2730 - nse: 0.6976 - val_loss: 0.5505 - val_mean_absolute_error: 0.5822 - val_nse: -1.5133\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.24450\n",
      "Epoch 59/250\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.1936 - mean_absolute_error: 0.2821 - nse: 0.6822 - val_loss: 0.5600 - val_mean_absolute_error: 0.5892 - val_nse: -1.2898\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.24450\n",
      "Epoch 60/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1846 - mean_absolute_error: 0.2685 - nse: 0.7367 - val_loss: 0.5183 - val_mean_absolute_error: 0.5567 - val_nse: -1.4225\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.24450\n",
      "Epoch 61/250\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.2066 - mean_absolute_error: 0.2786 - nse: 0.7041 - val_loss: 0.5219 - val_mean_absolute_error: 0.5679 - val_nse: -1.2286\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.24450\n",
      "Epoch 62/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1846 - mean_absolute_error: 0.2763 - nse: 0.6673 - val_loss: 0.5589 - val_mean_absolute_error: 0.5766 - val_nse: -1.4834\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.24450\n",
      "Epoch 63/250\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.1582 - mean_absolute_error: 0.2565 - nse: 0.6806 - val_loss: 0.5441 - val_mean_absolute_error: 0.5714 - val_nse: -1.3416\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.24450\n",
      "Epoch 64/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.2192 - mean_absolute_error: 0.2918 - nse: 0.7035 - val_loss: 0.5855 - val_mean_absolute_error: 0.6019 - val_nse: -1.1961\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.24450\n",
      "Epoch 65/250\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.1693 - mean_absolute_error: 0.2648 - nse: 0.7085 - val_loss: 0.5902 - val_mean_absolute_error: 0.5933 - val_nse: -1.4830\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.24450\n",
      "Epoch 66/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.2377 - mean_absolute_error: 0.2953 - nse: 0.7156 - val_loss: 0.6297 - val_mean_absolute_error: 0.6292 - val_nse: -1.5852\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.24450\n",
      "Epoch 67/250\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.1531 - mean_absolute_error: 0.2514 - nse: 0.6937 - val_loss: 0.6956 - val_mean_absolute_error: 0.6496 - val_nse: -1.7208\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.24450\n",
      "Epoch 68/250\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.1686 - mean_absolute_error: 0.2685 - nse: 0.6988 - val_loss: 0.6269 - val_mean_absolute_error: 0.6172 - val_nse: -1.7355\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.24450\n",
      "Epoch 69/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1567 - mean_absolute_error: 0.2583 - nse: 0.7286 - val_loss: 0.6660 - val_mean_absolute_error: 0.6339 - val_nse: -1.8279\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.24450\n",
      "Epoch 70/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.1664 - mean_absolute_error: 0.2663 - nse: 0.6974 - val_loss: 0.6814 - val_mean_absolute_error: 0.6534 - val_nse: -2.3236\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.24450\n",
      "Epoch 71/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1609 - mean_absolute_error: 0.2604 - nse: 0.6959 - val_loss: 0.5212 - val_mean_absolute_error: 0.5677 - val_nse: -0.9079\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.24450\n",
      "Epoch 72/250\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.1189 - mean_absolute_error: 0.2193 - nse: 0.7073 - val_loss: 0.6035 - val_mean_absolute_error: 0.5974 - val_nse: -1.5603\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.24450\n",
      "Epoch 73/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.1519 - mean_absolute_error: 0.2497 - nse: 0.7162 - val_loss: 0.7311 - val_mean_absolute_error: 0.6719 - val_nse: -2.1290\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.24450\n",
      "Epoch 74/250\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.1805 - mean_absolute_error: 0.2576 - nse: 0.7239 - val_loss: 0.7845 - val_mean_absolute_error: 0.6962 - val_nse: -2.7562\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.24450\n",
      "Epoch 75/250\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1558 - mean_absolute_error: 0.2484 - nse: 0.7484 - val_loss: 0.7436 - val_mean_absolute_error: 0.6780 - val_nse: -2.0053\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.24450\n",
      "Epoch 76/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1424 - mean_absolute_error: 0.2490 - nse: 0.6965 - val_loss: 0.7664 - val_mean_absolute_error: 0.6939 - val_nse: -2.1837\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.24450\n",
      "Epoch 77/250\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.1269 - mean_absolute_error: 0.2334 - nse: 0.7187 - val_loss: 0.7504 - val_mean_absolute_error: 0.6871 - val_nse: -1.8049\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.24450\n",
      "Epoch 78/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1905 - mean_absolute_error: 0.2718 - nse: 0.7402 - val_loss: 0.7998 - val_mean_absolute_error: 0.7066 - val_nse: -2.4189\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.24450\n",
      "Epoch 79/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1899 - mean_absolute_error: 0.2708 - nse: 0.7254 - val_loss: 0.7895 - val_mean_absolute_error: 0.7069 - val_nse: -2.6190\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.24450\n",
      "Epoch 80/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1600 - mean_absolute_error: 0.2546 - nse: 0.7197 - val_loss: 0.8065 - val_mean_absolute_error: 0.6968 - val_nse: -2.4974\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.24450\n",
      "Epoch 81/250\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.1554 - mean_absolute_error: 0.2649 - nse: 0.7131 - val_loss: 0.7397 - val_mean_absolute_error: 0.6786 - val_nse: -2.1916\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.24450\n",
      "Epoch 82/250\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1789 - mean_absolute_error: 0.2598 - nse: 0.7133 - val_loss: 0.7053 - val_mean_absolute_error: 0.6525 - val_nse: -2.4043\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.24450\n",
      "Epoch 83/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.2139 - mean_absolute_error: 0.2687 - nse: 0.7211 - val_loss: 0.7559 - val_mean_absolute_error: 0.6868 - val_nse: -1.9663\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.24450\n",
      "Epoch 84/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.1262 - mean_absolute_error: 0.2246 - nse: 0.7282 - val_loss: 0.6542 - val_mean_absolute_error: 0.6121 - val_nse: -1.7705\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.24450\n",
      "Epoch 85/250\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.2024 - mean_absolute_error: 0.2720 - nse: 0.7064 - val_loss: 0.7246 - val_mean_absolute_error: 0.6527 - val_nse: -2.1016\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.24450\n",
      "Epoch 86/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 245ms/step - loss: 0.1546 - mean_absolute_error: 0.2623 - nse: 0.7059 - val_loss: 0.8400 - val_mean_absolute_error: 0.7144 - val_nse: -2.3583\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.24450\n",
      "Epoch 87/250\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.1485 - mean_absolute_error: 0.2458 - nse: 0.7163 - val_loss: 0.9096 - val_mean_absolute_error: 0.7412 - val_nse: -2.9191\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.24450\n",
      "Epoch 88/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.1655 - mean_absolute_error: 0.2469 - nse: 0.7385 - val_loss: 0.9098 - val_mean_absolute_error: 0.7335 - val_nse: -3.0931\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.24450\n",
      "Epoch 89/250\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.1219 - mean_absolute_error: 0.2297 - nse: 0.7217 - val_loss: 0.7656 - val_mean_absolute_error: 0.6656 - val_nse: -2.0215\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.24450\n",
      "Epoch 90/250\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.1361 - mean_absolute_error: 0.2322 - nse: 0.7419 - val_loss: 0.8764 - val_mean_absolute_error: 0.7273 - val_nse: -3.0035\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.24450\n",
      "Epoch 91/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1626 - mean_absolute_error: 0.2507 - nse: 0.7784 - val_loss: 0.8123 - val_mean_absolute_error: 0.6975 - val_nse: -2.2179\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.24450\n",
      "Epoch 92/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.2111 - mean_absolute_error: 0.2900 - nse: 0.7287 - val_loss: 0.8297 - val_mean_absolute_error: 0.7015 - val_nse: -2.4476\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.24450\n",
      "Epoch 93/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1633 - mean_absolute_error: 0.2501 - nse: 0.7489 - val_loss: 0.8171 - val_mean_absolute_error: 0.7092 - val_nse: -2.8557\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.24450\n",
      "Epoch 94/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.1824 - mean_absolute_error: 0.2599 - nse: 0.7563 - val_loss: 0.7903 - val_mean_absolute_error: 0.6849 - val_nse: -2.2421\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.24450\n",
      "Epoch 95/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1570 - mean_absolute_error: 0.2424 - nse: 0.7342 - val_loss: 0.7674 - val_mean_absolute_error: 0.6888 - val_nse: -2.5254\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.24450\n",
      "Epoch 96/250\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.1698 - mean_absolute_error: 0.2632 - nse: 0.7300 - val_loss: 0.7387 - val_mean_absolute_error: 0.6586 - val_nse: -2.1668\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.24450\n",
      "Epoch 97/250\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.1739 - mean_absolute_error: 0.2626 - nse: 0.7664 - val_loss: 0.7067 - val_mean_absolute_error: 0.6393 - val_nse: -2.0870\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.24450\n",
      "Epoch 98/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1735 - mean_absolute_error: 0.2560 - nse: 0.7560 - val_loss: 0.7975 - val_mean_absolute_error: 0.6863 - val_nse: -2.6536\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.24450\n",
      "Epoch 99/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.1859 - mean_absolute_error: 0.2747 - nse: 0.7254 - val_loss: 0.7512 - val_mean_absolute_error: 0.6548 - val_nse: -2.2804\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.24450\n",
      "Epoch 100/250\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.1273 - mean_absolute_error: 0.2298 - nse: 0.7230 - val_loss: 0.7851 - val_mean_absolute_error: 0.6693 - val_nse: -2.2433\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.24450\n",
      "Epoch 101/250\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.1602 - mean_absolute_error: 0.2495 - nse: 0.7413 - val_loss: 0.7468 - val_mean_absolute_error: 0.6586 - val_nse: -2.1443\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.24450\n",
      "Epoch 102/250\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.1724 - mean_absolute_error: 0.2637 - nse: 0.7418 - val_loss: 0.7513 - val_mean_absolute_error: 0.6531 - val_nse: -2.0146\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.24450\n",
      "Epoch 103/250\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.2318 - mean_absolute_error: 0.2936 - nse: 0.7354 - val_loss: 0.8427 - val_mean_absolute_error: 0.7000 - val_nse: -2.5566\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.24450\n",
      "Epoch 104/250\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.1095 - mean_absolute_error: 0.2166 - nse: 0.7384 - val_loss: 0.8488 - val_mean_absolute_error: 0.7059 - val_nse: -2.0600\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.24450\n",
      "Epoch 105/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1500 - mean_absolute_error: 0.2484 - nse: 0.7455 - val_loss: 0.8824 - val_mean_absolute_error: 0.7276 - val_nse: -2.9168\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.24450\n",
      "Epoch 106/250\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.1358 - mean_absolute_error: 0.2433 - nse: 0.7396 - val_loss: 0.7991 - val_mean_absolute_error: 0.6802 - val_nse: -2.2811\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.24450\n",
      "Epoch 107/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1435 - mean_absolute_error: 0.2423 - nse: 0.7486 - val_loss: 0.9028 - val_mean_absolute_error: 0.7268 - val_nse: -3.1281\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.24450\n",
      "Epoch 108/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1676 - mean_absolute_error: 0.2620 - nse: 0.7156 - val_loss: 0.7434 - val_mean_absolute_error: 0.6620 - val_nse: -2.3560\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.24450\n",
      "Epoch 109/250\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0985 - mean_absolute_error: 0.2039 - nse: 0.7506 - val_loss: 0.8652 - val_mean_absolute_error: 0.7113 - val_nse: -2.4000\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.24450\n",
      "Epoch 110/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.1259 - mean_absolute_error: 0.2251 - nse: 0.7626 - val_loss: 0.8359 - val_mean_absolute_error: 0.6854 - val_nse: -2.4321\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.24450\n",
      "Epoch 111/250\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.1265 - mean_absolute_error: 0.2271 - nse: 0.7646 - val_loss: 0.9943 - val_mean_absolute_error: 0.7657 - val_nse: -3.7046\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.24450\n",
      "Epoch 112/250\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1644 - mean_absolute_error: 0.2495 - nse: 0.7785 - val_loss: 0.8366 - val_mean_absolute_error: 0.6965 - val_nse: -2.5603\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.24450\n",
      "Epoch 113/250\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.1208 - mean_absolute_error: 0.2279 - nse: 0.7503 - val_loss: 0.9885 - val_mean_absolute_error: 0.7700 - val_nse: -2.9482\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.24450\n",
      "Epoch 114/250\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.1065 - mean_absolute_error: 0.2180 - nse: 0.7540 - val_loss: 0.9163 - val_mean_absolute_error: 0.7422 - val_nse: -2.8339\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.24450\n",
      "Epoch 115/250\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.1602 - mean_absolute_error: 0.2457 - nse: 0.7601 - val_loss: 0.8454 - val_mean_absolute_error: 0.7045 - val_nse: -2.2895\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.24450\n",
      "Epoch 116/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1520 - mean_absolute_error: 0.2463 - nse: 0.7739 - val_loss: 0.9334 - val_mean_absolute_error: 0.7541 - val_nse: -3.5464\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.24450\n",
      "Epoch 117/250\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.1656 - mean_absolute_error: 0.2594 - nse: 0.7468 - val_loss: 0.8427 - val_mean_absolute_error: 0.7031 - val_nse: -2.3720\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.24450\n",
      "Epoch 118/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1255 - mean_absolute_error: 0.2341 - nse: 0.7423 - val_loss: 0.7496 - val_mean_absolute_error: 0.6647 - val_nse: -2.1934\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.24450\n",
      "Epoch 119/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 228ms/step - loss: 0.1638 - mean_absolute_error: 0.2518 - nse: 0.7547 - val_loss: 0.9027 - val_mean_absolute_error: 0.7349 - val_nse: -3.4572\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.24450\n",
      "Epoch 120/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1429 - mean_absolute_error: 0.2296 - nse: 0.7813 - val_loss: 0.8634 - val_mean_absolute_error: 0.7118 - val_nse: -2.5225\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.24450\n",
      "Epoch 121/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1570 - mean_absolute_error: 0.2426 - nse: 0.7449 - val_loss: 0.7397 - val_mean_absolute_error: 0.6452 - val_nse: -2.0840\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.24450\n",
      "Epoch 122/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1703 - mean_absolute_error: 0.2450 - nse: 0.7593 - val_loss: 0.8541 - val_mean_absolute_error: 0.6981 - val_nse: -2.7792\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.24450\n",
      "Epoch 123/250\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.1216 - mean_absolute_error: 0.2425 - nse: 0.7347 - val_loss: 0.8608 - val_mean_absolute_error: 0.6899 - val_nse: -2.5107\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.24450\n",
      "Epoch 124/250\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.1459 - mean_absolute_error: 0.2431 - nse: 0.7562 - val_loss: 1.0594 - val_mean_absolute_error: 0.7916 - val_nse: -3.5378\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.24450\n",
      "Epoch 125/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.1458 - mean_absolute_error: 0.2375 - nse: 0.7596 - val_loss: 0.9955 - val_mean_absolute_error: 0.7626 - val_nse: -3.5964\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.24450\n",
      "Epoch 126/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.1080 - mean_absolute_error: 0.2163 - nse: 0.7395 - val_loss: 0.8367 - val_mean_absolute_error: 0.6841 - val_nse: -2.2099\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.24450\n",
      "Epoch 127/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1027 - mean_absolute_error: 0.2044 - nse: 0.7901 - val_loss: 0.9159 - val_mean_absolute_error: 0.7352 - val_nse: -2.9071\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.24450\n",
      "Epoch 128/250\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.1334 - mean_absolute_error: 0.2267 - nse: 0.7784 - val_loss: 0.9041 - val_mean_absolute_error: 0.7316 - val_nse: -2.8386\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.24450\n",
      "Epoch 129/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1855 - mean_absolute_error: 0.2675 - nse: 0.7902 - val_loss: 0.9057 - val_mean_absolute_error: 0.7167 - val_nse: -2.9400\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.24450\n",
      "Epoch 130/250\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.1784 - mean_absolute_error: 0.2586 - nse: 0.7626 - val_loss: 0.8649 - val_mean_absolute_error: 0.7235 - val_nse: -2.7856\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.24450\n",
      "Epoch 131/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1342 - mean_absolute_error: 0.2314 - nse: 0.7837 - val_loss: 0.8617 - val_mean_absolute_error: 0.7069 - val_nse: -2.6591\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.24450\n",
      "Epoch 132/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.1628 - mean_absolute_error: 0.2441 - nse: 0.7615 - val_loss: 0.9097 - val_mean_absolute_error: 0.7413 - val_nse: -2.8253\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.24450\n",
      "Epoch 133/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.1457 - mean_absolute_error: 0.2461 - nse: 0.7747 - val_loss: 0.8674 - val_mean_absolute_error: 0.7052 - val_nse: -2.9181\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.24450\n",
      "Epoch 134/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1481 - mean_absolute_error: 0.2438 - nse: 0.7730 - val_loss: 0.8416 - val_mean_absolute_error: 0.6877 - val_nse: -2.4572\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.24450\n",
      "Epoch 135/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1416 - mean_absolute_error: 0.2346 - nse: 0.8090 - val_loss: 0.7725 - val_mean_absolute_error: 0.6633 - val_nse: -2.7054\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.24450\n",
      "Epoch 136/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1669 - mean_absolute_error: 0.2546 - nse: 0.7514 - val_loss: 0.7810 - val_mean_absolute_error: 0.6670 - val_nse: -2.3205\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.24450\n",
      "Epoch 137/250\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1197 - mean_absolute_error: 0.2175 - nse: 0.7504 - val_loss: 0.8740 - val_mean_absolute_error: 0.7092 - val_nse: -2.8039\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.24450\n",
      "Epoch 138/250\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.1144 - mean_absolute_error: 0.2135 - nse: 0.7732 - val_loss: 0.7348 - val_mean_absolute_error: 0.6391 - val_nse: -2.1139\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.24450\n",
      "Epoch 139/250\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.1804 - mean_absolute_error: 0.2578 - nse: 0.7634 - val_loss: 0.8661 - val_mean_absolute_error: 0.7130 - val_nse: -2.3043\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.24450\n",
      "Epoch 140/250\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.1995 - mean_absolute_error: 0.2755 - nse: 0.7421 - val_loss: 0.8268 - val_mean_absolute_error: 0.6854 - val_nse: -2.4400\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.24450\n",
      "Epoch 141/250\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.1330 - mean_absolute_error: 0.2283 - nse: 0.7814 - val_loss: 0.9357 - val_mean_absolute_error: 0.7283 - val_nse: -2.7878\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.24450\n",
      "Epoch 142/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.1265 - mean_absolute_error: 0.2309 - nse: 0.7758 - val_loss: 1.0312 - val_mean_absolute_error: 0.7821 - val_nse: -3.1403\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.24450\n",
      "Epoch 143/250\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.1103 - mean_absolute_error: 0.2178 - nse: 0.7502 - val_loss: 0.8865 - val_mean_absolute_error: 0.7081 - val_nse: -2.8352\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.24450\n",
      "Epoch 144/250\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.1413 - mean_absolute_error: 0.2388 - nse: 0.7830 - val_loss: 0.8759 - val_mean_absolute_error: 0.7041 - val_nse: -2.8509\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.24450\n",
      "Epoch 145/250\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1216 - mean_absolute_error: 0.2223 - nse: 0.7452 - val_loss: 0.9041 - val_mean_absolute_error: 0.7222 - val_nse: -3.2133\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.24450\n",
      "Epoch 146/250\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1259 - mean_absolute_error: 0.2206 - nse: 0.7621 - val_loss: 0.8342 - val_mean_absolute_error: 0.6874 - val_nse: -2.2491\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.24450\n",
      "Epoch 147/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0945 - mean_absolute_error: 0.1941 - nse: 0.7865 - val_loss: 0.7887 - val_mean_absolute_error: 0.6627 - val_nse: -2.2114\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.24450\n",
      "Epoch 148/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1113 - mean_absolute_error: 0.2151 - nse: 0.7779 - val_loss: 0.9994 - val_mean_absolute_error: 0.7578 - val_nse: -3.5045\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.24450\n",
      "Epoch 149/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1397 - mean_absolute_error: 0.2305 - nse: 0.7946 - val_loss: 0.8431 - val_mean_absolute_error: 0.6957 - val_nse: -2.8549\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.24450\n",
      "Epoch 150/250\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.1140 - mean_absolute_error: 0.2161 - nse: 0.7998 - val_loss: 0.9561 - val_mean_absolute_error: 0.7501 - val_nse: -2.6125\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.24450\n",
      "Epoch 151/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1029 - mean_absolute_error: 0.2102 - nse: 0.7815 - val_loss: 0.9582 - val_mean_absolute_error: 0.7487 - val_nse: -3.1644\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.24450\n",
      "Epoch 152/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1286 - mean_absolute_error: 0.2259 - nse: 0.7838 - val_loss: 0.8745 - val_mean_absolute_error: 0.7087 - val_nse: -2.4742\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.24450\n",
      "Epoch 153/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1496 - mean_absolute_error: 0.2413 - nse: 0.7952 - val_loss: 0.8846 - val_mean_absolute_error: 0.7256 - val_nse: -3.0189\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.24450\n",
      "Epoch 154/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1371 - mean_absolute_error: 0.2305 - nse: 0.7851 - val_loss: 0.8221 - val_mean_absolute_error: 0.6834 - val_nse: -2.5250\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.24450\n",
      "Epoch 155/250\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.1124 - mean_absolute_error: 0.2189 - nse: 0.7804 - val_loss: 0.8262 - val_mean_absolute_error: 0.6902 - val_nse: -2.4047\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.24450\n",
      "Epoch 156/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1541 - mean_absolute_error: 0.2466 - nse: 0.7640 - val_loss: 0.8892 - val_mean_absolute_error: 0.7336 - val_nse: -3.2572\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.24450\n",
      "Epoch 157/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.1356 - mean_absolute_error: 0.2229 - nse: 0.7981 - val_loss: 0.8367 - val_mean_absolute_error: 0.6923 - val_nse: -2.4589\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.24450\n",
      "Epoch 158/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.1380 - mean_absolute_error: 0.2285 - nse: 0.7858 - val_loss: 0.8356 - val_mean_absolute_error: 0.6931 - val_nse: -2.5456\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.24450\n",
      "Epoch 159/250\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.1427 - mean_absolute_error: 0.2273 - nse: 0.7709 - val_loss: 0.8030 - val_mean_absolute_error: 0.6733 - val_nse: -2.3630\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.24450\n",
      "Epoch 160/250\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.1169 - mean_absolute_error: 0.2329 - nse: 0.7978 - val_loss: 0.9318 - val_mean_absolute_error: 0.7144 - val_nse: -2.8342\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.24450\n",
      "Epoch 161/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1419 - mean_absolute_error: 0.2448 - nse: 0.7643 - val_loss: 0.9527 - val_mean_absolute_error: 0.7398 - val_nse: -2.8553\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.24450\n",
      "Epoch 162/250\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0947 - mean_absolute_error: 0.2022 - nse: 0.7757 - val_loss: 1.0344 - val_mean_absolute_error: 0.7674 - val_nse: -3.6809\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.24450\n",
      "Epoch 163/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.1200 - mean_absolute_error: 0.2174 - nse: 0.7721 - val_loss: 0.9419 - val_mean_absolute_error: 0.7220 - val_nse: -3.0198\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.24450\n",
      "Epoch 164/250\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.1036 - mean_absolute_error: 0.2040 - nse: 0.7918 - val_loss: 0.8502 - val_mean_absolute_error: 0.6984 - val_nse: -2.5500\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.24450\n",
      "Epoch 165/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.1199 - mean_absolute_error: 0.2129 - nse: 0.7927 - val_loss: 0.9213 - val_mean_absolute_error: 0.7434 - val_nse: -2.8552\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.24450\n",
      "Epoch 166/250\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1484 - mean_absolute_error: 0.2420 - nse: 0.8184 - val_loss: 0.9786 - val_mean_absolute_error: 0.7443 - val_nse: -3.3368\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.24450\n",
      "Epoch 167/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1617 - mean_absolute_error: 0.2508 - nse: 0.7888 - val_loss: 0.8919 - val_mean_absolute_error: 0.7251 - val_nse: -2.7124\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.24450\n",
      "Epoch 168/250\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.1396 - mean_absolute_error: 0.2364 - nse: 0.7828 - val_loss: 0.8728 - val_mean_absolute_error: 0.7125 - val_nse: -2.8793\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.24450\n",
      "Epoch 169/250\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.1598 - mean_absolute_error: 0.2391 - nse: 0.7948 - val_loss: 0.9703 - val_mean_absolute_error: 0.7596 - val_nse: -3.1406\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.24450\n",
      "Epoch 170/250\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.1206 - mean_absolute_error: 0.2202 - nse: 0.8022 - val_loss: 0.9236 - val_mean_absolute_error: 0.7201 - val_nse: -2.9906\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.24450\n",
      "Epoch 171/250\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.1417 - mean_absolute_error: 0.2431 - nse: 0.7724 - val_loss: 0.9159 - val_mean_absolute_error: 0.7127 - val_nse: -2.9683\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.24450\n",
      "Epoch 172/250\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.1318 - mean_absolute_error: 0.2301 - nse: 0.8162 - val_loss: 0.7863 - val_mean_absolute_error: 0.6727 - val_nse: -2.5249\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.24450\n",
      "Epoch 173/250\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1548 - mean_absolute_error: 0.2387 - nse: 0.7869 - val_loss: 0.7384 - val_mean_absolute_error: 0.6445 - val_nse: -2.1188\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.24450\n",
      "Epoch 174/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1068 - mean_absolute_error: 0.2108 - nse: 0.7819 - val_loss: 0.8571 - val_mean_absolute_error: 0.7011 - val_nse: -2.8777\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.24450\n",
      "Epoch 175/250\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.1176 - mean_absolute_error: 0.2106 - nse: 0.7853 - val_loss: 0.7818 - val_mean_absolute_error: 0.6696 - val_nse: -2.2670\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.24450\n",
      "Epoch 176/250\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.1521 - mean_absolute_error: 0.2387 - nse: 0.7889 - val_loss: 0.8597 - val_mean_absolute_error: 0.7142 - val_nse: -2.2961\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.24450\n",
      "Epoch 177/250\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.1342 - mean_absolute_error: 0.2255 - nse: 0.7742 - val_loss: 0.8395 - val_mean_absolute_error: 0.6830 - val_nse: -2.5326\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.24450\n",
      "Epoch 178/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1705 - mean_absolute_error: 0.2503 - nse: 0.7967 - val_loss: 0.9026 - val_mean_absolute_error: 0.7221 - val_nse: -2.6061\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.24450\n",
      "Epoch 179/250\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0989 - mean_absolute_error: 0.2037 - nse: 0.7832 - val_loss: 1.0447 - val_mean_absolute_error: 0.7685 - val_nse: -3.1618\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.24450\n",
      "Epoch 180/250\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.1150 - mean_absolute_error: 0.2198 - nse: 0.7852 - val_loss: 0.8890 - val_mean_absolute_error: 0.7020 - val_nse: -2.9562\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.24450\n",
      "Epoch 181/250\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.1173 - mean_absolute_error: 0.2191 - nse: 0.8022 - val_loss: 0.8906 - val_mean_absolute_error: 0.7037 - val_nse: -2.7340\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.24450\n",
      "Epoch 182/250\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.1239 - mean_absolute_error: 0.2234 - nse: 0.7712 - val_loss: 0.9337 - val_mean_absolute_error: 0.7328 - val_nse: -3.3985\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.24450\n",
      "Epoch 183/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1092 - mean_absolute_error: 0.2090 - nse: 0.7964 - val_loss: 0.8143 - val_mean_absolute_error: 0.6815 - val_nse: -2.0812\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.24450\n",
      "Epoch 184/250\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.1039 - mean_absolute_error: 0.1985 - nse: 0.7806 - val_loss: 0.7814 - val_mean_absolute_error: 0.6554 - val_nse: -2.1844\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.24450\n",
      "Epoch 185/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0940 - mean_absolute_error: 0.2012 - nse: 0.8048 - val_loss: 0.9441 - val_mean_absolute_error: 0.7260 - val_nse: -3.3273\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.24450\n",
      "Epoch 186/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1191 - mean_absolute_error: 0.2106 - nse: 0.8111 - val_loss: 0.9077 - val_mean_absolute_error: 0.7207 - val_nse: -3.2514\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.24450\n",
      "Epoch 187/250\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.1126 - mean_absolute_error: 0.2135 - nse: 0.8190 - val_loss: 0.9557 - val_mean_absolute_error: 0.7478 - val_nse: -2.7354\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.24450\n",
      "Epoch 188/250\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0948 - mean_absolute_error: 0.2016 - nse: 0.7937 - val_loss: 0.9544 - val_mean_absolute_error: 0.7374 - val_nse: -3.0727\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.24450\n",
      "Epoch 189/250\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0878 - mean_absolute_error: 0.1926 - nse: 0.8110 - val_loss: 0.8620 - val_mean_absolute_error: 0.7093 - val_nse: -2.1015\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.24450\n",
      "Epoch 190/250\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.1506 - mean_absolute_error: 0.2390 - nse: 0.8120 - val_loss: 0.8782 - val_mean_absolute_error: 0.7011 - val_nse: -2.7300\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.24450\n",
      "Epoch 191/250\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.1289 - mean_absolute_error: 0.2266 - nse: 0.8019 - val_loss: 0.9344 - val_mean_absolute_error: 0.7334 - val_nse: -3.6563\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.24450\n",
      "Epoch 192/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1013 - mean_absolute_error: 0.2063 - nse: 0.8090 - val_loss: 0.8653 - val_mean_absolute_error: 0.7007 - val_nse: -2.6504\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.24450\n",
      "Epoch 193/250\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1187 - mean_absolute_error: 0.2261 - nse: 0.7893 - val_loss: 0.8645 - val_mean_absolute_error: 0.7212 - val_nse: -2.7152\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.24450\n",
      "Epoch 194/250\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.1397 - mean_absolute_error: 0.2256 - nse: 0.7998 - val_loss: 0.8462 - val_mean_absolute_error: 0.6987 - val_nse: -2.7112\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.24450\n",
      "Epoch 195/250\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.1271 - mean_absolute_error: 0.2182 - nse: 0.8107 - val_loss: 0.9441 - val_mean_absolute_error: 0.7336 - val_nse: -2.9714\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.24450\n",
      "Epoch 196/250\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1112 - mean_absolute_error: 0.2045 - nse: 0.7922 - val_loss: 0.8539 - val_mean_absolute_error: 0.6868 - val_nse: -2.7529\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.24450\n",
      "Epoch 197/250\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.1317 - mean_absolute_error: 0.2323 - nse: 0.8015 - val_loss: 0.8684 - val_mean_absolute_error: 0.6975 - val_nse: -2.5329\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.24450\n",
      "Epoch 198/250\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.1219 - mean_absolute_error: 0.2365 - nse: 0.7961 - val_loss: 1.0045 - val_mean_absolute_error: 0.7447 - val_nse: -3.2276\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.24450\n",
      "Epoch 199/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0983 - mean_absolute_error: 0.2014 - nse: 0.7901 - val_loss: 1.0724 - val_mean_absolute_error: 0.7784 - val_nse: -3.5171\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.24450\n",
      "Epoch 200/250\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.1169 - mean_absolute_error: 0.2133 - nse: 0.7989 - val_loss: 1.0394 - val_mean_absolute_error: 0.7596 - val_nse: -3.5471\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.24450\n",
      "Epoch 201/250\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0918 - mean_absolute_error: 0.1953 - nse: 0.8097 - val_loss: 0.7549 - val_mean_absolute_error: 0.6543 - val_nse: -2.0526\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.24450\n",
      "Epoch 202/250\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0997 - mean_absolute_error: 0.1956 - nse: 0.8099 - val_loss: 0.9910 - val_mean_absolute_error: 0.7641 - val_nse: -3.2755\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.24450\n",
      "Epoch 203/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1202 - mean_absolute_error: 0.2228 - nse: 0.8367 - val_loss: 0.9219 - val_mean_absolute_error: 0.7350 - val_nse: -2.7977\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.24450\n",
      "Epoch 204/250\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.1512 - mean_absolute_error: 0.2468 - nse: 0.7970 - val_loss: 0.9487 - val_mean_absolute_error: 0.7383 - val_nse: -3.0541\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.24450\n",
      "Epoch 205/250\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.1319 - mean_absolute_error: 0.2237 - nse: 0.8168 - val_loss: 0.9501 - val_mean_absolute_error: 0.7425 - val_nse: -3.2707\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.24450\n",
      "Epoch 206/250\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.1393 - mean_absolute_error: 0.2312 - nse: 0.8167 - val_loss: 1.0561 - val_mean_absolute_error: 0.7795 - val_nse: -3.5754\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.24450\n",
      "Epoch 207/250\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.1113 - mean_absolute_error: 0.2065 - nse: 0.8196 - val_loss: 0.9243 - val_mean_absolute_error: 0.7297 - val_nse: -3.2684\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.24450\n",
      "Epoch 208/250\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.1121 - mean_absolute_error: 0.2210 - nse: 0.7907 - val_loss: 0.8986 - val_mean_absolute_error: 0.7115 - val_nse: -2.7697\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.24450\n",
      "Epoch 209/250\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1367 - mean_absolute_error: 0.2328 - nse: 0.8231 - val_loss: 0.8670 - val_mean_absolute_error: 0.6923 - val_nse: -2.9126\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.24450\n",
      "Epoch 210/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1370 - mean_absolute_error: 0.2265 - nse: 0.8142 - val_loss: 0.8060 - val_mean_absolute_error: 0.6813 - val_nse: -2.7437\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.24450\n",
      "Epoch 211/250\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.1266 - mean_absolute_error: 0.2205 - nse: 0.7886 - val_loss: 0.8445 - val_mean_absolute_error: 0.6931 - val_nse: -2.7292\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.24450\n",
      "Epoch 212/250\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0902 - mean_absolute_error: 0.1904 - nse: 0.8024 - val_loss: 0.8419 - val_mean_absolute_error: 0.6930 - val_nse: -2.4483\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.24450\n",
      "Epoch 213/250\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.1351 - mean_absolute_error: 0.2210 - nse: 0.8095 - val_loss: 0.8494 - val_mean_absolute_error: 0.7038 - val_nse: -2.4763\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.24450\n",
      "Epoch 214/250\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.1232 - mean_absolute_error: 0.2179 - nse: 0.8012 - val_loss: 0.8388 - val_mean_absolute_error: 0.6871 - val_nse: -2.3723\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.24450\n",
      "Epoch 215/250\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1615 - mean_absolute_error: 0.2466 - nse: 0.8134 - val_loss: 0.9546 - val_mean_absolute_error: 0.7411 - val_nse: -3.1494\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.24450\n",
      "Epoch 216/250\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0918 - mean_absolute_error: 0.1971 - nse: 0.7940 - val_loss: 0.9170 - val_mean_absolute_error: 0.7144 - val_nse: -2.3593\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.24450\n",
      "Epoch 217/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.1108 - mean_absolute_error: 0.2131 - nse: 0.8060 - val_loss: 0.9453 - val_mean_absolute_error: 0.7317 - val_nse: -3.1743\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.24450\n",
      "Epoch 218/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0970 - mean_absolute_error: 0.2033 - nse: 0.8177 - val_loss: 1.0249 - val_mean_absolute_error: 0.7528 - val_nse: -3.2135\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.24450\n",
      "Epoch 219/250\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1201 - mean_absolute_error: 0.2188 - nse: 0.8016 - val_loss: 0.9497 - val_mean_absolute_error: 0.7370 - val_nse: -3.4839\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.24450\n",
      "Epoch 220/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.1150 - mean_absolute_error: 0.2111 - nse: 0.7922 - val_loss: 0.7539 - val_mean_absolute_error: 0.6580 - val_nse: -2.0124\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.24450\n",
      "Epoch 221/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0739 - mean_absolute_error: 0.1766 - nse: 0.7967 - val_loss: 0.9267 - val_mean_absolute_error: 0.7121 - val_nse: -2.8695\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.24450\n",
      "Epoch 222/250\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0986 - mean_absolute_error: 0.2002 - nse: 0.8207 - val_loss: 0.8923 - val_mean_absolute_error: 0.7050 - val_nse: -2.7531\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.24450\n",
      "Epoch 223/250\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0992 - mean_absolute_error: 0.1984 - nse: 0.8292 - val_loss: 0.9768 - val_mean_absolute_error: 0.7455 - val_nse: -3.7537\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.24450\n",
      "Epoch 224/250\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.1181 - mean_absolute_error: 0.2124 - nse: 0.8358 - val_loss: 0.9510 - val_mean_absolute_error: 0.7274 - val_nse: -2.9477\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.24450\n",
      "Epoch 225/250\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0785 - mean_absolute_error: 0.1881 - nse: 0.8137 - val_loss: 0.9507 - val_mean_absolute_error: 0.7434 - val_nse: -2.7228\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.24450\n",
      "Epoch 226/250\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0802 - mean_absolute_error: 0.1901 - nse: 0.8223 - val_loss: 0.9265 - val_mean_absolute_error: 0.7270 - val_nse: -2.5844\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.24450\n",
      "Epoch 227/250\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.1297 - mean_absolute_error: 0.2222 - nse: 0.8161 - val_loss: 0.8942 - val_mean_absolute_error: 0.7062 - val_nse: -2.7365\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.24450\n",
      "Epoch 228/250\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1159 - mean_absolute_error: 0.2164 - nse: 0.8345 - val_loss: 0.9510 - val_mean_absolute_error: 0.7409 - val_nse: -3.8604\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.24450\n",
      "Epoch 229/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0994 - mean_absolute_error: 0.2033 - nse: 0.8234 - val_loss: 0.9219 - val_mean_absolute_error: 0.7250 - val_nse: -2.6198\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.24450\n",
      "Epoch 230/250\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1087 - mean_absolute_error: 0.2124 - nse: 0.8147 - val_loss: 0.8278 - val_mean_absolute_error: 0.6935 - val_nse: -2.4504\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.24450\n",
      "Epoch 231/250\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.1221 - mean_absolute_error: 0.2136 - nse: 0.8059 - val_loss: 1.0334 - val_mean_absolute_error: 0.7694 - val_nse: -4.2390\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.24450\n",
      "Epoch 232/250\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.1111 - mean_absolute_error: 0.2058 - nse: 0.8316 - val_loss: 0.9692 - val_mean_absolute_error: 0.7494 - val_nse: -2.9406\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.24450\n",
      "Epoch 233/250\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1074 - mean_absolute_error: 0.2043 - nse: 0.8219 - val_loss: 0.8406 - val_mean_absolute_error: 0.6834 - val_nse: -2.5537\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.24450\n",
      "Epoch 234/250\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.1248 - mean_absolute_error: 0.2167 - nse: 0.8191 - val_loss: 0.9315 - val_mean_absolute_error: 0.7188 - val_nse: -2.9988\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.24450\n",
      "Epoch 235/250\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0950 - mean_absolute_error: 0.2183 - nse: 0.8052 - val_loss: 0.8773 - val_mean_absolute_error: 0.6928 - val_nse: -2.4688\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.24450\n",
      "Epoch 236/250\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.1073 - mean_absolute_error: 0.2099 - nse: 0.8110 - val_loss: 1.0852 - val_mean_absolute_error: 0.7839 - val_nse: -3.8765\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.24450\n",
      "Epoch 237/250\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.1075 - mean_absolute_error: 0.2024 - nse: 0.8247 - val_loss: 0.9999 - val_mean_absolute_error: 0.7404 - val_nse: -3.5691\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.24450\n",
      "Epoch 238/250\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0813 - mean_absolute_error: 0.1867 - nse: 0.8090 - val_loss: 0.7806 - val_mean_absolute_error: 0.6645 - val_nse: -2.0119\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.24450\n",
      "Epoch 239/250\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0757 - mean_absolute_error: 0.1781 - nse: 0.8326 - val_loss: 0.9497 - val_mean_absolute_error: 0.7497 - val_nse: -3.2288\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.24450\n",
      "Epoch 240/250\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.1121 - mean_absolute_error: 0.2119 - nse: 0.8386 - val_loss: 0.9020 - val_mean_absolute_error: 0.7269 - val_nse: -2.6717\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.24450\n",
      "Epoch 241/250\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.1321 - mean_absolute_error: 0.2343 - nse: 0.8351 - val_loss: 0.9462 - val_mean_absolute_error: 0.7306 - val_nse: -3.0449\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.24450\n",
      "Epoch 242/250\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.1398 - mean_absolute_error: 0.2300 - nse: 0.8139 - val_loss: 0.9580 - val_mean_absolute_error: 0.7466 - val_nse: -3.3973\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.24450\n",
      "Epoch 243/250\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0983 - mean_absolute_error: 0.2021 - nse: 0.8411 - val_loss: 1.0086 - val_mean_absolute_error: 0.7564 - val_nse: -3.1640\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.24450\n",
      "Epoch 244/250\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.1213 - mean_absolute_error: 0.2139 - nse: 0.8244 - val_loss: 0.8899 - val_mean_absolute_error: 0.7243 - val_nse: -2.8087\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.24450\n",
      "Epoch 245/250\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.1097 - mean_absolute_error: 0.2167 - nse: 0.8286 - val_loss: 0.9192 - val_mean_absolute_error: 0.7106 - val_nse: -3.1595\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.24450\n",
      "Epoch 246/250\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.1197 - mean_absolute_error: 0.2165 - nse: 0.8192 - val_loss: 0.8531 - val_mean_absolute_error: 0.6905 - val_nse: -2.5919\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.24450\n",
      "Epoch 247/250\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.1151 - mean_absolute_error: 0.2086 - nse: 0.8542 - val_loss: 0.8657 - val_mean_absolute_error: 0.7016 - val_nse: -3.0025\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.24450\n",
      "Epoch 248/250\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.1245 - mean_absolute_error: 0.2145 - nse: 0.8041 - val_loss: 0.8573 - val_mean_absolute_error: 0.7061 - val_nse: -2.6606\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.24450\n",
      "Epoch 249/250\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0895 - mean_absolute_error: 0.1931 - nse: 0.8066 - val_loss: 0.9157 - val_mean_absolute_error: 0.7207 - val_nse: -3.0369\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.24450\n",
      "Epoch 250/250\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0973 - mean_absolute_error: 0.1932 - nse: 0.8187 - val_loss: 0.8203 - val_mean_absolute_error: 0.6821 - val_nse: -2.3759\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.24450\n"
     ]
    }
   ],
   "source": [
    "rnn_epochs = 250\n",
    "# multi_linear_model = model_multi_linear(\n",
    "#     window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "#     #training_flag=__RNN_TRAINING__, checkpoint_path=\"save/\"+watershed+\"models/multi_linear.ckpt\")\n",
    "#     training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"multi_linear.ckpt\")\n",
    "# elman_model = model_elman(\n",
    "#     window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "#     training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"elman.ckpt\")\n",
    "gru_model = model_gru(\n",
    "    window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "    training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"gru.ckpt\")\n",
    "multi_lstm_model = model_multi_lstm(\n",
    "    window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "    training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"multi_lstm.ckpt\")\n",
    "multi_conv_model = model_multi_conv(\n",
    "    window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "    training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"multi_conv.ckpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## core / window.py / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_to_day_mean(array):\n",
    "    time = 24\n",
    "    array = array.reshape((array.shape[0], array.shape[1] // time, time, array.shape[2]))\n",
    "    array = array.mean(2)\n",
    "    return array\n",
    "\n",
    "def compa(model=None,df = None, plot_col=0, input_width=7*24, label_width=5*24, target_std=None, target_mean=None, predict_day=4):\n",
    "    \n",
    "    print(df.shape)\n",
    "    print(plot_col)\n",
    "    \n",
    "    width = input_width + label_width\n",
    "    \n",
    "    length = df.shape[0]\n",
    "    length -= width\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "   \n",
    "    for i in range(0,length,24):\n",
    "#         print('i = ', i)\n",
    "        dataset = df.iloc[i:i+width].to_numpy()\n",
    "        input = dataset[:input_width]\n",
    "        label = dataset[input_width:, plot_col:plot_col+1]\n",
    "        \n",
    "        input = input.reshape((-1,)+input.shape)\n",
    "        label = label.reshape((-1,)+label.shape)\n",
    "        \n",
    "        inputs.append(input)\n",
    "        labels.append(label)\n",
    "        \n",
    "    inputs = np.concatenate(inputs, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "#     print('labels.mean(axis=1)')\n",
    "#     print(labels.mean(axis=1).shape)\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "\n",
    "    predictions = model(inputs)\n",
    "    print(predictions.shape)\n",
    "    \n",
    "    predictions = predictions.numpy() * target_std[plot_col] + target_mean[plot_col]\n",
    "    labels = labels * target_std[plot_col] + target_mean[plot_col]\n",
    "\n",
    "    print('predictions.shape =', predictions.shape)\n",
    "#     pred_day = predictions\n",
    "    pred_day = hour_to_day_mean(predictions)\n",
    "    print('pred_day.shape =', pred_day.shape)\n",
    "#     label_day= labels\n",
    "    label_day = hour_to_day_mean(labels)\n",
    "    \n",
    "    inputs_target = inputs[:,:,plot_col:plot_col+1]\n",
    "    inputs_target = inputs_target * target_std[plot_col] + target_mean[plot_col]\n",
    "#     inputs_day = inputs_target\n",
    "    inputs_day = hour_to_day_mean(inputs_target)\n",
    "    \n",
    "    #plt.figure(figsize=(10, 800))\n",
    "    \n",
    "#     input_index = np.array(range(0,length,24))\n",
    "#     label_index = input_index + 24*7\n",
    "    \n",
    "#     print(input_index)\n",
    "#     print(label_index)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(label_day[:, predict_day, :], label='label')\n",
    "    plt.plot(pred_day[:, predict_day, :], label='pred')\n",
    "#     plt.plot(input_index, inputs_day[:, 0, :], label='input')\n",
    "#     plt.plot(label_index, label_day[:, 0, :], label='label')\n",
    "#     plt.plot(label_index, pred_day[:, 0, :], label='pred')\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "        \n",
    "#     o1 = np.mean(labels)\n",
    "# #     print('labels =', labels)\n",
    "#     print('o1 =', o1)\n",
    "#     nse1 = ((labels - predictions)**2).sum()\n",
    "#     nse2 = ((labels - o1)**2).sum()\n",
    "#     nse3 = 1 - (nse1/nse2)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print('pred_day.shape =',pred_day.shape)\n",
    "#     print('label_day.shape =',label_day.shape)\n",
    "    #print('label_index =', label_index)\n",
    "    \n",
    "    o1 = np.mean(label_day[:,predict_day,0])\n",
    "#     o1 = np.mean(label_day)\n",
    "#     print('labels =', labels)\n",
    "#     o1 = target_mean[plot_col]\n",
    "    print('o1 =', o1)\n",
    "    nse1 = ((pred_day-label_day)**2).sum(axis=0)\n",
    "    nse2 = ((label_day - o1)**2).sum(axis=0)\n",
    "    nse3 = 1 - (nse1[predict_day]/nse2[predict_day])\n",
    "    print('nse3 =', nse3)\n",
    "    #mean = np.mean(label_day[predict_day])\n",
    "    print('label_day[predict_day].shape =', label_day[predict_day].shape)\n",
    "#     print(type(pred_day))\n",
    "#     print(pred_day[:20, 0, 0])\n",
    "#     print(label_day[:20, 0, 0])\n",
    "    o = label_day[:, 0, 0]\n",
    "    s = pred_day[:, 0, 0]\n",
    "    mean = np.mean(o)\n",
    "    print('mean =', mean)\n",
    "    nse = 1. - np.sum((o-s)**2)/np.sum((o-mean)**2)\n",
    "#     print('nse =', nse)\n",
    "    \n",
    "    \n",
    "#     pbias1 = (label_day - pred_day).sum(axis=0)\n",
    "#     pbias2 = (label_day).sum(axis=0)\n",
    "#     pbias3 = (pbias1[predict_day]/pbias2[predict_day])*100\n",
    "    \n",
    "    \n",
    "#     o1 = np.mean(label_day)\n",
    "# #     print('labels =', labels)\n",
    "#     print('o1 =', o1)\n",
    "#     nse1 = ((label_day - pred_day)**2).sum()\n",
    "#     nse2 = ((label_day - o1)**2).sum()\n",
    "#     nse3 = 1 - (nse1/nse2)\n",
    "#     print('nse3 =', nse3)\n",
    "    #mean = np.mean(label_day[predict_day])\n",
    "#     print(type(pred_day))\n",
    "#     print(pred_day[:20, 0, 0])\n",
    "#     print(label_day[:20, 0, 0])\n",
    "#     o = label_day[:, 0, 0]\n",
    "#     s = pred_day[:, 0, 0]\n",
    "#     mean = np.mean(o)\n",
    "#     print('mean =', mean)\n",
    "#     nse = 1. - np.sum((o-s)**2)/np.sum((o-mean)**2)\n",
    "#     print('nse =', nse)\n",
    "    \n",
    "    \n",
    "    pbias1 = (label_day - pred_day).sum(axis=0)\n",
    "    pbias2 = (label_day).sum(axis=0)\n",
    "    pbias3 = (pbias1[predict_day]/pbias2[predict_day])*100\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return nse3, np.abs(pbias3), pred_day, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.506584</td>\n",
       "      <td>-0.781585</td>\n",
       "      <td>0.609397</td>\n",
       "      <td>1.684159</td>\n",
       "      <td>-0.388460</td>\n",
       "      <td>0.914476</td>\n",
       "      <td>-0.137610</td>\n",
       "      <td>-0.525071</td>\n",
       "      <td>1.747686e-12</td>\n",
       "      <td>1.414210e+00</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>1.413681</td>\n",
       "      <td>-1.689783</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>0.994299</td>\n",
       "      <td>-0.463754</td>\n",
       "      <td>-1.254349</td>\n",
       "      <td>0.019940</td>\n",
       "      <td>-1.432135</td>\n",
       "      <td>-0.901498</td>\n",
       "      <td>1.747686e-12</td>\n",
       "      <td>1.414210e+00</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>1.413681</td>\n",
       "      <td>-1.356208</td>\n",
       "      <td>-0.588429</td>\n",
       "      <td>0.154454</td>\n",
       "      <td>1.470927</td>\n",
       "      <td>-1.231157</td>\n",
       "      <td>1.157017</td>\n",
       "      <td>-0.740015</td>\n",
       "      <td>-0.865323</td>\n",
       "      <td>1.747686e-12</td>\n",
       "      <td>1.414210e+00</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>1.413681</td>\n",
       "      <td>-1.507453</td>\n",
       "      <td>0.396912</td>\n",
       "      <td>0.801488</td>\n",
       "      <td>-0.042523</td>\n",
       "      <td>-1.445851</td>\n",
       "      <td>-0.430887</td>\n",
       "      <td>-0.021002</td>\n",
       "      <td>-0.837544</td>\n",
       "      <td>1.747686e-12</td>\n",
       "      <td>1.414210e+00</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>1.413681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.510089</td>\n",
       "      <td>-0.797194</td>\n",
       "      <td>0.583524</td>\n",
       "      <td>1.684159</td>\n",
       "      <td>-0.339509</td>\n",
       "      <td>0.940824</td>\n",
       "      <td>-0.116095</td>\n",
       "      <td>-0.537748</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>1.366022e+00</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>1.413679</td>\n",
       "      <td>-1.703138</td>\n",
       "      <td>-0.056581</td>\n",
       "      <td>0.980826</td>\n",
       "      <td>-0.440851</td>\n",
       "      <td>-1.252129</td>\n",
       "      <td>0.066557</td>\n",
       "      <td>-1.446334</td>\n",
       "      <td>-0.901628</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>1.366022e+00</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>1.413679</td>\n",
       "      <td>-1.367382</td>\n",
       "      <td>-0.588860</td>\n",
       "      <td>0.147153</td>\n",
       "      <td>1.470137</td>\n",
       "      <td>-1.172248</td>\n",
       "      <td>1.150839</td>\n",
       "      <td>-0.761529</td>\n",
       "      <td>-0.863685</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>1.366022e+00</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>1.413679</td>\n",
       "      <td>-1.530273</td>\n",
       "      <td>0.212962</td>\n",
       "      <td>0.685987</td>\n",
       "      <td>-0.009650</td>\n",
       "      <td>-1.419883</td>\n",
       "      <td>-0.440716</td>\n",
       "      <td>-0.030038</td>\n",
       "      <td>-0.838586</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>1.366022e+00</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>1.413679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.514180</td>\n",
       "      <td>-0.801723</td>\n",
       "      <td>0.557911</td>\n",
       "      <td>1.679321</td>\n",
       "      <td>-0.397348</td>\n",
       "      <td>0.916532</td>\n",
       "      <td>-0.116095</td>\n",
       "      <td>-0.548444</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>1.413677</td>\n",
       "      <td>-1.721294</td>\n",
       "      <td>-0.082758</td>\n",
       "      <td>0.978333</td>\n",
       "      <td>-0.447071</td>\n",
       "      <td>-1.250806</td>\n",
       "      <td>0.057947</td>\n",
       "      <td>-1.503778</td>\n",
       "      <td>-0.900590</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>1.413677</td>\n",
       "      <td>-1.375527</td>\n",
       "      <td>-0.588900</td>\n",
       "      <td>0.148847</td>\n",
       "      <td>1.459278</td>\n",
       "      <td>-1.273414</td>\n",
       "      <td>1.088776</td>\n",
       "      <td>-0.818973</td>\n",
       "      <td>-0.870471</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>1.413677</td>\n",
       "      <td>-1.551150</td>\n",
       "      <td>0.065689</td>\n",
       "      <td>0.593695</td>\n",
       "      <td>-0.020410</td>\n",
       "      <td>-1.389266</td>\n",
       "      <td>-0.424052</td>\n",
       "      <td>-0.042516</td>\n",
       "      <td>-0.839976</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>1.413677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.515415</td>\n",
       "      <td>-0.815917</td>\n",
       "      <td>0.552050</td>\n",
       "      <td>1.673497</td>\n",
       "      <td>-0.220514</td>\n",
       "      <td>0.915128</td>\n",
       "      <td>-0.116095</td>\n",
       "      <td>-0.551085</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>1.413674</td>\n",
       "      <td>-1.736147</td>\n",
       "      <td>-0.104176</td>\n",
       "      <td>0.983347</td>\n",
       "      <td>-0.488828</td>\n",
       "      <td>-1.255655</td>\n",
       "      <td>0.106997</td>\n",
       "      <td>-1.514535</td>\n",
       "      <td>-0.899293</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>1.413674</td>\n",
       "      <td>-1.381628</td>\n",
       "      <td>-0.588456</td>\n",
       "      <td>0.155937</td>\n",
       "      <td>1.467275</td>\n",
       "      <td>-1.280091</td>\n",
       "      <td>1.089618</td>\n",
       "      <td>-0.847587</td>\n",
       "      <td>-0.868599</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>1.413674</td>\n",
       "      <td>-1.573655</td>\n",
       "      <td>-0.036403</td>\n",
       "      <td>0.538223</td>\n",
       "      <td>-0.068585</td>\n",
       "      <td>-1.483433</td>\n",
       "      <td>-0.405517</td>\n",
       "      <td>-0.051552</td>\n",
       "      <td>-0.839976</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>1.413674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.518370</td>\n",
       "      <td>-0.796628</td>\n",
       "      <td>0.560990</td>\n",
       "      <td>1.676459</td>\n",
       "      <td>-0.406037</td>\n",
       "      <td>0.915223</td>\n",
       "      <td>-0.094581</td>\n",
       "      <td>-0.542766</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>1.413669</td>\n",
       "      <td>-1.745111</td>\n",
       "      <td>-0.117089</td>\n",
       "      <td>0.990270</td>\n",
       "      <td>-0.501366</td>\n",
       "      <td>-1.248377</td>\n",
       "      <td>0.123470</td>\n",
       "      <td>-1.424820</td>\n",
       "      <td>-0.899163</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>1.413669</td>\n",
       "      <td>-1.389343</td>\n",
       "      <td>-0.588900</td>\n",
       "      <td>0.160290</td>\n",
       "      <td>1.494817</td>\n",
       "      <td>-1.296875</td>\n",
       "      <td>1.167127</td>\n",
       "      <td>-0.847587</td>\n",
       "      <td>-0.866727</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>1.413669</td>\n",
       "      <td>-1.589207</td>\n",
       "      <td>-0.100847</td>\n",
       "      <td>0.504041</td>\n",
       "      <td>-0.044103</td>\n",
       "      <td>-1.431635</td>\n",
       "      <td>-0.403551</td>\n",
       "      <td>-0.064030</td>\n",
       "      <td>-0.839281</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>1.413669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40481</th>\n",
       "      <td>1.795294</td>\n",
       "      <td>0.428777</td>\n",
       "      <td>-0.563966</td>\n",
       "      <td>-1.302070</td>\n",
       "      <td>0.051775</td>\n",
       "      <td>-0.768629</td>\n",
       "      <td>1.540518</td>\n",
       "      <td>0.377217</td>\n",
       "      <td>-1.366022e+00</td>\n",
       "      <td>-3.660245e-01</td>\n",
       "      <td>-0.957083</td>\n",
       "      <td>-1.041478</td>\n",
       "      <td>1.581170</td>\n",
       "      <td>0.409410</td>\n",
       "      <td>0.070066</td>\n",
       "      <td>-1.128744</td>\n",
       "      <td>-0.457672</td>\n",
       "      <td>-0.733757</td>\n",
       "      <td>0.627694</td>\n",
       "      <td>0.195259</td>\n",
       "      <td>-1.366022e+00</td>\n",
       "      <td>-3.660245e-01</td>\n",
       "      <td>-0.957083</td>\n",
       "      <td>-1.041478</td>\n",
       "      <td>1.613790</td>\n",
       "      <td>-0.761879</td>\n",
       "      <td>0.284050</td>\n",
       "      <td>-0.947474</td>\n",
       "      <td>-0.089689</td>\n",
       "      <td>-0.463087</td>\n",
       "      <td>0.984799</td>\n",
       "      <td>0.779378</td>\n",
       "      <td>-1.366022e+00</td>\n",
       "      <td>-3.660245e-01</td>\n",
       "      <td>-0.957083</td>\n",
       "      <td>-1.041478</td>\n",
       "      <td>1.457260</td>\n",
       "      <td>-0.013147</td>\n",
       "      <td>-0.011644</td>\n",
       "      <td>-0.835403</td>\n",
       "      <td>0.112330</td>\n",
       "      <td>-1.076149</td>\n",
       "      <td>0.231977</td>\n",
       "      <td>-0.473942</td>\n",
       "      <td>-1.366022e+00</td>\n",
       "      <td>-3.660245e-01</td>\n",
       "      <td>-0.957083</td>\n",
       "      <td>-1.041478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40482</th>\n",
       "      <td>1.785267</td>\n",
       "      <td>0.642193</td>\n",
       "      <td>-0.557606</td>\n",
       "      <td>-1.297134</td>\n",
       "      <td>0.188163</td>\n",
       "      <td>-0.771999</td>\n",
       "      <td>1.605061</td>\n",
       "      <td>0.342908</td>\n",
       "      <td>-1.414210e+00</td>\n",
       "      <td>-8.448240e-12</td>\n",
       "      <td>-0.957830</td>\n",
       "      <td>-1.040792</td>\n",
       "      <td>1.799305</td>\n",
       "      <td>0.687132</td>\n",
       "      <td>-0.026319</td>\n",
       "      <td>-1.076005</td>\n",
       "      <td>-0.374020</td>\n",
       "      <td>-0.795731</td>\n",
       "      <td>0.444550</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>-1.414210e+00</td>\n",
       "      <td>-8.448240e-12</td>\n",
       "      <td>-0.957830</td>\n",
       "      <td>-1.040792</td>\n",
       "      <td>1.595740</td>\n",
       "      <td>-0.694483</td>\n",
       "      <td>0.322875</td>\n",
       "      <td>-0.945894</td>\n",
       "      <td>-0.133939</td>\n",
       "      <td>-0.473478</td>\n",
       "      <td>0.932734</td>\n",
       "      <td>0.856523</td>\n",
       "      <td>-1.414210e+00</td>\n",
       "      <td>-8.448240e-12</td>\n",
       "      <td>-0.957830</td>\n",
       "      <td>-1.040792</td>\n",
       "      <td>1.479648</td>\n",
       "      <td>0.026068</td>\n",
       "      <td>-0.051016</td>\n",
       "      <td>-0.844305</td>\n",
       "      <td>0.080854</td>\n",
       "      <td>-1.148317</td>\n",
       "      <td>0.281185</td>\n",
       "      <td>-0.484599</td>\n",
       "      <td>-1.414210e+00</td>\n",
       "      <td>-8.448240e-12</td>\n",
       "      <td>-0.957830</td>\n",
       "      <td>-1.040792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40483</th>\n",
       "      <td>1.600753</td>\n",
       "      <td>-0.773120</td>\n",
       "      <td>-0.440680</td>\n",
       "      <td>-1.287262</td>\n",
       "      <td>0.033641</td>\n",
       "      <td>-0.779115</td>\n",
       "      <td>1.626575</td>\n",
       "      <td>0.427755</td>\n",
       "      <td>-1.366022e+00</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>-0.958575</td>\n",
       "      <td>-1.040105</td>\n",
       "      <td>1.735127</td>\n",
       "      <td>0.462475</td>\n",
       "      <td>-0.174807</td>\n",
       "      <td>-1.076005</td>\n",
       "      <td>-0.470553</td>\n",
       "      <td>-1.058630</td>\n",
       "      <td>0.422635</td>\n",
       "      <td>0.143652</td>\n",
       "      <td>-1.366022e+00</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>-0.958575</td>\n",
       "      <td>-1.040105</td>\n",
       "      <td>1.594736</td>\n",
       "      <td>-0.582161</td>\n",
       "      <td>0.458195</td>\n",
       "      <td>-0.941452</td>\n",
       "      <td>-0.139745</td>\n",
       "      <td>-0.470950</td>\n",
       "      <td>0.877227</td>\n",
       "      <td>0.981855</td>\n",
       "      <td>-1.366022e+00</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>-0.958575</td>\n",
       "      <td>-1.040105</td>\n",
       "      <td>1.504378</td>\n",
       "      <td>0.032916</td>\n",
       "      <td>-0.250879</td>\n",
       "      <td>-0.852305</td>\n",
       "      <td>0.199734</td>\n",
       "      <td>-1.176217</td>\n",
       "      <td>0.224507</td>\n",
       "      <td>-0.536956</td>\n",
       "      <td>-1.366022e+00</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>-0.958575</td>\n",
       "      <td>-1.040105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40484</th>\n",
       "      <td>1.630836</td>\n",
       "      <td>-1.020234</td>\n",
       "      <td>-0.462247</td>\n",
       "      <td>-1.277390</td>\n",
       "      <td>0.030742</td>\n",
       "      <td>-0.784170</td>\n",
       "      <td>1.131743</td>\n",
       "      <td>0.420406</td>\n",
       "      <td>-1.224742e+00</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>-0.959320</td>\n",
       "      <td>-1.039418</td>\n",
       "      <td>1.660920</td>\n",
       "      <td>0.103026</td>\n",
       "      <td>-0.434094</td>\n",
       "      <td>-1.067120</td>\n",
       "      <td>-0.466923</td>\n",
       "      <td>-1.087275</td>\n",
       "      <td>0.415910</td>\n",
       "      <td>0.075425</td>\n",
       "      <td>-1.224742e+00</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>-0.959320</td>\n",
       "      <td>-1.039418</td>\n",
       "      <td>1.645878</td>\n",
       "      <td>-0.469839</td>\n",
       "      <td>0.374642</td>\n",
       "      <td>-0.901274</td>\n",
       "      <td>-0.109276</td>\n",
       "      <td>-0.452696</td>\n",
       "      <td>0.843019</td>\n",
       "      <td>0.967955</td>\n",
       "      <td>-1.224742e+00</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>-0.959320</td>\n",
       "      <td>-1.039418</td>\n",
       "      <td>1.434347</td>\n",
       "      <td>-0.084990</td>\n",
       "      <td>-0.234105</td>\n",
       "      <td>-0.817741</td>\n",
       "      <td>0.172855</td>\n",
       "      <td>-1.162015</td>\n",
       "      <td>0.228196</td>\n",
       "      <td>-0.571706</td>\n",
       "      <td>-1.224742e+00</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>-0.959320</td>\n",
       "      <td>-1.039418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40485</th>\n",
       "      <td>1.591728</td>\n",
       "      <td>-0.941610</td>\n",
       "      <td>-0.435683</td>\n",
       "      <td>-1.282326</td>\n",
       "      <td>-0.018591</td>\n",
       "      <td>-0.760581</td>\n",
       "      <td>1.217801</td>\n",
       "      <td>0.309933</td>\n",
       "      <td>-9.999976e-01</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>-0.960065</td>\n",
       "      <td>-1.038730</td>\n",
       "      <td>1.605767</td>\n",
       "      <td>-0.144088</td>\n",
       "      <td>-0.691790</td>\n",
       "      <td>-1.076005</td>\n",
       "      <td>-0.448058</td>\n",
       "      <td>-1.112269</td>\n",
       "      <td>0.503091</td>\n",
       "      <td>-0.038097</td>\n",
       "      <td>-9.999976e-01</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>-0.960065</td>\n",
       "      <td>-1.038730</td>\n",
       "      <td>1.642870</td>\n",
       "      <td>-0.593389</td>\n",
       "      <td>0.231832</td>\n",
       "      <td>-0.903544</td>\n",
       "      <td>-0.139745</td>\n",
       "      <td>-0.424894</td>\n",
       "      <td>0.821505</td>\n",
       "      <td>0.863473</td>\n",
       "      <td>-9.999976e-01</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>-0.960065</td>\n",
       "      <td>-1.038730</td>\n",
       "      <td>1.438440</td>\n",
       "      <td>-0.180574</td>\n",
       "      <td>-0.463422</td>\n",
       "      <td>-0.752429</td>\n",
       "      <td>0.228579</td>\n",
       "      <td>-1.199764</td>\n",
       "      <td>0.205198</td>\n",
       "      <td>-0.582594</td>\n",
       "      <td>-9.999976e-01</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>-0.960065</td>\n",
       "      <td>-1.038730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40486 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -1.506584 -0.781585  0.609397  1.684159 -0.388460  0.914476 -0.137610   \n",
       "1     -1.510089 -0.797194  0.583524  1.684159 -0.339509  0.940824 -0.116095   \n",
       "2     -1.514180 -0.801723  0.557911  1.679321 -0.397348  0.916532 -0.116095   \n",
       "3     -1.515415 -0.815917  0.552050  1.673497 -0.220514  0.915128 -0.116095   \n",
       "4     -1.518370 -0.796628  0.560990  1.676459 -0.406037  0.915223 -0.094581   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "40481  1.795294  0.428777 -0.563966 -1.302070  0.051775 -0.768629  1.540518   \n",
       "40482  1.785267  0.642193 -0.557606 -1.297134  0.188163 -0.771999  1.605061   \n",
       "40483  1.600753 -0.773120 -0.440680 -1.287262  0.033641 -0.779115  1.626575   \n",
       "40484  1.630836 -1.020234 -0.462247 -1.277390  0.030742 -0.784170  1.131743   \n",
       "40485  1.591728 -0.941610 -0.435683 -1.282326 -0.018591 -0.760581  1.217801   \n",
       "\n",
       "             7             8             9         10        11        12  \\\n",
       "0     -0.525071  1.747686e-12  1.414210e+00  0.002127  1.413681 -1.689783   \n",
       "1     -0.537748  3.660245e-01  1.366022e+00  0.003141  1.413679 -1.703138   \n",
       "2     -0.548444  7.071051e-01  1.224742e+00  0.004155  1.413677 -1.721294   \n",
       "3     -0.551085  9.999976e-01  9.999976e-01  0.005168  1.413674 -1.736147   \n",
       "4     -0.542766  1.224742e+00  7.071051e-01  0.006182  1.413669 -1.745111   \n",
       "...         ...           ...           ...       ...       ...       ...   \n",
       "40481  0.377217 -1.366022e+00 -3.660245e-01 -0.957083 -1.041478  1.581170   \n",
       "40482  0.342908 -1.414210e+00 -8.448240e-12 -0.957830 -1.040792  1.799305   \n",
       "40483  0.427755 -1.366022e+00  3.660245e-01 -0.958575 -1.040105  1.735127   \n",
       "40484  0.420406 -1.224742e+00  7.071051e-01 -0.959320 -1.039418  1.660920   \n",
       "40485  0.309933 -9.999976e-01  9.999976e-01 -0.960065 -1.038730  1.605767   \n",
       "\n",
       "             13        14        15        16        17        18        19  \\\n",
       "0     -0.004983  0.994299 -0.463754 -1.254349  0.019940 -1.432135 -0.901498   \n",
       "1     -0.056581  0.980826 -0.440851 -1.252129  0.066557 -1.446334 -0.901628   \n",
       "2     -0.082758  0.978333 -0.447071 -1.250806  0.057947 -1.503778 -0.900590   \n",
       "3     -0.104176  0.983347 -0.488828 -1.255655  0.106997 -1.514535 -0.899293   \n",
       "4     -0.117089  0.990270 -0.501366 -1.248377  0.123470 -1.424820 -0.899163   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "40481  0.409410  0.070066 -1.128744 -0.457672 -0.733757  0.627694  0.195259   \n",
       "40482  0.687132 -0.026319 -1.076005 -0.374020 -0.795731  0.444550  0.126600   \n",
       "40483  0.462475 -0.174807 -1.076005 -0.470553 -1.058630  0.422635  0.143652   \n",
       "40484  0.103026 -0.434094 -1.067120 -0.466923 -1.087275  0.415910  0.075425   \n",
       "40485 -0.144088 -0.691790 -1.076005 -0.448058 -1.112269  0.503091 -0.038097   \n",
       "\n",
       "                 20            21        22        23        24        25  \\\n",
       "0      1.747686e-12  1.414210e+00  0.002127  1.413681 -1.356208 -0.588429   \n",
       "1      3.660245e-01  1.366022e+00  0.003141  1.413679 -1.367382 -0.588860   \n",
       "2      7.071051e-01  1.224742e+00  0.004155  1.413677 -1.375527 -0.588900   \n",
       "3      9.999976e-01  9.999976e-01  0.005168  1.413674 -1.381628 -0.588456   \n",
       "4      1.224742e+00  7.071051e-01  0.006182  1.413669 -1.389343 -0.588900   \n",
       "...             ...           ...       ...       ...       ...       ...   \n",
       "40481 -1.366022e+00 -3.660245e-01 -0.957083 -1.041478  1.613790 -0.761879   \n",
       "40482 -1.414210e+00 -8.448240e-12 -0.957830 -1.040792  1.595740 -0.694483   \n",
       "40483 -1.366022e+00  3.660245e-01 -0.958575 -1.040105  1.594736 -0.582161   \n",
       "40484 -1.224742e+00  7.071051e-01 -0.959320 -1.039418  1.645878 -0.469839   \n",
       "40485 -9.999976e-01  9.999976e-01 -0.960065 -1.038730  1.642870 -0.593389   \n",
       "\n",
       "             26        27        28        29        30        31  \\\n",
       "0      0.154454  1.470927 -1.231157  1.157017 -0.740015 -0.865323   \n",
       "1      0.147153  1.470137 -1.172248  1.150839 -0.761529 -0.863685   \n",
       "2      0.148847  1.459278 -1.273414  1.088776 -0.818973 -0.870471   \n",
       "3      0.155937  1.467275 -1.280091  1.089618 -0.847587 -0.868599   \n",
       "4      0.160290  1.494817 -1.296875  1.167127 -0.847587 -0.866727   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "40481  0.284050 -0.947474 -0.089689 -0.463087  0.984799  0.779378   \n",
       "40482  0.322875 -0.945894 -0.133939 -0.473478  0.932734  0.856523   \n",
       "40483  0.458195 -0.941452 -0.139745 -0.470950  0.877227  0.981855   \n",
       "40484  0.374642 -0.901274 -0.109276 -0.452696  0.843019  0.967955   \n",
       "40485  0.231832 -0.903544 -0.139745 -0.424894  0.821505  0.863473   \n",
       "\n",
       "                 32            33        34        35        36        37  \\\n",
       "0      1.747686e-12  1.414210e+00  0.002127  1.413681 -1.507453  0.396912   \n",
       "1      3.660245e-01  1.366022e+00  0.003141  1.413679 -1.530273  0.212962   \n",
       "2      7.071051e-01  1.224742e+00  0.004155  1.413677 -1.551150  0.065689   \n",
       "3      9.999976e-01  9.999976e-01  0.005168  1.413674 -1.573655 -0.036403   \n",
       "4      1.224742e+00  7.071051e-01  0.006182  1.413669 -1.589207 -0.100847   \n",
       "...             ...           ...       ...       ...       ...       ...   \n",
       "40481 -1.366022e+00 -3.660245e-01 -0.957083 -1.041478  1.457260 -0.013147   \n",
       "40482 -1.414210e+00 -8.448240e-12 -0.957830 -1.040792  1.479648  0.026068   \n",
       "40483 -1.366022e+00  3.660245e-01 -0.958575 -1.040105  1.504378  0.032916   \n",
       "40484 -1.224742e+00  7.071051e-01 -0.959320 -1.039418  1.434347 -0.084990   \n",
       "40485 -9.999976e-01  9.999976e-01 -0.960065 -1.038730  1.438440 -0.180574   \n",
       "\n",
       "             38        39        40        41        42        43  \\\n",
       "0      0.801488 -0.042523 -1.445851 -0.430887 -0.021002 -0.837544   \n",
       "1      0.685987 -0.009650 -1.419883 -0.440716 -0.030038 -0.838586   \n",
       "2      0.593695 -0.020410 -1.389266 -0.424052 -0.042516 -0.839976   \n",
       "3      0.538223 -0.068585 -1.483433 -0.405517 -0.051552 -0.839976   \n",
       "4      0.504041 -0.044103 -1.431635 -0.403551 -0.064030 -0.839281   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "40481 -0.011644 -0.835403  0.112330 -1.076149  0.231977 -0.473942   \n",
       "40482 -0.051016 -0.844305  0.080854 -1.148317  0.281185 -0.484599   \n",
       "40483 -0.250879 -0.852305  0.199734 -1.176217  0.224507 -0.536956   \n",
       "40484 -0.234105 -0.817741  0.172855 -1.162015  0.228196 -0.571706   \n",
       "40485 -0.463422 -0.752429  0.228579 -1.199764  0.205198 -0.582594   \n",
       "\n",
       "                 44            45        46        47  \n",
       "0      1.747686e-12  1.414210e+00  0.002127  1.413681  \n",
       "1      3.660245e-01  1.366022e+00  0.003141  1.413679  \n",
       "2      7.071051e-01  1.224742e+00  0.004155  1.413677  \n",
       "3      9.999976e-01  9.999976e-01  0.005168  1.413674  \n",
       "4      1.224742e+00  7.071051e-01  0.006182  1.413669  \n",
       "...             ...           ...       ...       ...  \n",
       "40481 -1.366022e+00 -3.660245e-01 -0.957083 -1.041478  \n",
       "40482 -1.414210e+00 -8.448240e-12 -0.957830 -1.040792  \n",
       "40483 -1.366022e+00  3.660245e-01 -0.958575 -1.040105  \n",
       "40484 -1.224742e+00  7.071051e-01 -0.959320 -1.039418  \n",
       "40485 -9.999976e-01  9.999976e-01 -0.960065 -1.038730  \n",
       "\n",
       "[40486 rows x 48 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40486, 48)\n",
      "4\n",
      "(1675, 168, 48)\n",
      "(1675, 120, 1)\n",
      "(1675, 120, 1)\n",
      "predictions.shape = (1675, 120, 1)\n",
      "pred_day.shape = (1675, 5, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90bd3e5c5ec42ccac17747efe422329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = 4.2986693\n",
      "nse3 = [0.40670437]\n",
      "label_day[predict_day].shape = (5, 1)\n",
      "mean = 4.2988167\n",
      "[0.40670437]\n",
      "[0.35350007]\n"
     ]
    }
   ],
   "source": [
    "val_nse['GRU'], val_pbias['GRU'], pred, label = compa(model=gru_model,\n",
    "                                                      df=test_df,\n",
    "                                                      plot_col=out_features[0], \n",
    "                                                      target_std=target_std, \n",
    "                                                      target_mean=target_mean,\n",
    "                                                      predict_day = 4)\n",
    "\n",
    "print(val_nse['GRU'])\n",
    "print(val_pbias['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40486, 48)\n",
      "4\n",
      "(1675, 168, 48)\n",
      "(1675, 120, 1)\n",
      "(1675, 120, 1)\n",
      "predictions.shape = (1675, 120, 1)\n",
      "pred_day.shape = (1675, 5, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bad8f446364b90af04970463c880ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = 4.2986693\n",
      "nse3 = [0.3791725]\n",
      "label_day[predict_day].shape = (5, 1)\n",
      "mean = 4.2988167\n",
      "[0.3791725]\n",
      "[0.01059659]\n"
     ]
    }
   ],
   "source": [
    "val_nse['LSTM'], val_pbias['LSTM'], pred, label = compa(model=multi_lstm_model,\n",
    "                                                      df=test_df,\n",
    "                                                      plot_col=out_features[0], \n",
    "                                                      target_std=target_std, \n",
    "                                                      target_mean=target_mean,\n",
    "                                                      predict_day = 4)\n",
    "\n",
    "print(val_nse['LSTM'])\n",
    "print(val_pbias['LSTM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40486, 48)\n",
      "4\n",
      "(1675, 168, 48)\n",
      "(1675, 120, 1)\n",
      "(1675, 120, 1)\n",
      "predictions.shape = (1675, 120, 1)\n",
      "pred_day.shape = (1675, 5, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051ea662ffb3424b91fd1bc7772aadfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = 4.2986693\n",
      "nse3 = [0.282094]\n",
      "label_day[predict_day].shape = (5, 1)\n",
      "mean = 4.2988167\n",
      "[0.282094]\n",
      "[5.753906]\n"
     ]
    }
   ],
   "source": [
    "val_nse['CNN'], val_pbias['CNN'], pred, label = compa(model=multi_conv_model,\n",
    "                                                      df=test_df,\n",
    "                                                      plot_col=out_features[0], \n",
    "                                                      target_std=target_std, \n",
    "                                                      target_mean=target_mean,\n",
    "                                                      predict_day = 4)\n",
    "\n",
    "print(val_nse['CNN'])\n",
    "print(val_pbias['CNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5061, 48)\n",
      "4\n",
      "(199, 168, 48)\n",
      "(199, 120, 1)\n",
      "(199, 120, 1)\n",
      "predictions.shape = (199, 120, 1)\n",
      "pred_day.shape = (199, 5, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3bf1f763a0431bb568f3bf37dd8401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = 4.5696707\n",
      "nse3 = [0.64476454]\n",
      "label_day[predict_day].shape = (5, 1)\n",
      "mean = 4.584215\n",
      "\n",
      "\n",
      "[0.64476454]\n",
      "[2.1833117]\n"
     ]
    }
   ],
   "source": [
    "val_nse['GRU'], val_pbias['GRU'], pred, label = compa(\n",
    "    model=gru_model,df=train_df,\n",
    "    plot_col=out_features[0], target_std=target_std, target_mean=target_mean, predict_day = 4)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(val_nse['GRU'])\n",
    "print(val_pbias['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52608, 48)\n",
      "4\n",
      "(2180, 168, 48)\n",
      "(2180, 120, 1)\n",
      "(2180, 120, 1)\n",
      "predictions.shape = (2180, 120, 1)\n",
      "pred_day.shape = (2180, 5, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8eea7854c49467f97c0d953c3cffd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = 4.27052\n",
      "nse3 = [0.43067127]\n",
      "label_day[predict_day].shape = (5, 1)\n",
      "mean = 4.2712\n",
      "\n",
      "\n",
      "[0.43067127]\n",
      "[0.16672361]\n"
     ]
    }
   ],
   "source": [
    "val_nse['GRU'], val_pbias['GRU'], pred, label = compa(\n",
    "    model=gru_model,df=real_df_all,\n",
    "    plot_col=out_features[0], target_std=target_std, target_mean=target_mean, predict_day = 4)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(val_nse['GRU'])\n",
    "print(val_pbias['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
