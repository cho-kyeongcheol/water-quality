{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from core.gain import *\n",
    "from core.rnn_predic import *\n",
    "from core.models import *\n",
    "from core.util import *\n",
    "#from core.window import WindowGenerator, MissData, make_dataset_water, WaterDataGenerator\n",
    "from core.window import WindowGenerator, make_dataset_gain, make_dataset_water\n",
    "from core.file_open import make_dataframe\n",
    "from core.miss_data import MissData\n",
    "import json\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "parameters_dir = 'input'\n",
    "\n",
    "parameters_file = 'input.json'\n",
    "parameters_path = '{dir}/{file}'.format(dir=parameters_dir, file=parameters_file)\n",
    "\n",
    "with open(parameters_path, encoding='utf8') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "gain_parameters = parameters['gain']\n",
    "rnn_parameters = parameters['rnn']\n",
    "file_parameters = parameters['file']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input json name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_parameters['watershed'] = 'geum_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_path = parameters_dir+'/'+ file_parameters['watershed'] + '.json'\n",
    "with open(parameters_path, encoding='utf8') as json_file:\n",
    "    parameters = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parameters = parameters['data']\n",
    "\n",
    "interpolation_option = data_parameters['interpolation']\n",
    "colum_idx = data_parameters['columns']\n",
    "watershed = data_parameters['watershed']\n",
    "file_names = data_parameters['files']\n",
    "folder = data_parameters['directorys']\n",
    "for i in range(len(folder)):\n",
    "    folder[i] = watershed+folder[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "__GAIN_TRAINING__ = gain_parameters['train']\n",
    "gain_epochs = gain_parameters['max_epochs']\n",
    "gain_in_setps = gain_parameters['input_width']\n",
    "gain_out_setps = gain_parameters['label_width']\n",
    "gain_batch_size = gain_parameters['batch_size']\n",
    "gain_fill_no = gain_parameters['fill_width']\n",
    "gain_shift = gain_parameters['shift_width']\n",
    "gain_miss_rate = gain_parameters['miss_rate']\n",
    "\n",
    "__RNN_TRAINING__ = rnn_parameters['train']\n",
    "rnn_epochs = rnn_parameters['max_epochs']\n",
    "rnn_in_setps = rnn_parameters['input_width']\n",
    "rnn_out_steps = rnn_parameters['label_width']\n",
    "rnn_batch_size = rnn_parameters['batch_size']\n",
    "rnn_predict_day = rnn_parameters['predict_day']\n",
    "rnn_target_column = rnn_parameters['target_column']\n",
    "\n",
    "if rnn_predict_day < 3 or rnn_predict_day >5:\n",
    "    print('predict_day err')\n",
    "    exit(88)\n",
    "rnn_predict_day -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gain_epochs , rnn_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_num = range(len(folder))\n",
    "\n",
    "\n",
    "# run_num = [0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "real_df_all = pd.DataFrame([])\n",
    "target_all = target_mean = target_std = 0\n",
    "\n",
    "gain_val_performance = {}\n",
    "gain_performance = {}\n",
    "\n",
    "length = len(run_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain Training Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "__GAIN_TRAINING__ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpol flag :  [False, False]\n",
      "folder :  data/geum/자동/\n",
      "colum_idx :  :,[26,27,28,29,30,31,32,33]\n",
      "file_names[idx] :  [['현도_2015.xlsx', '현도_2016.xlsx', '현도_2017.xlsx', '현도_2018.xlsx', '현도_2019.xlsx', '현도_2020.xlsx'], ['대청호_2015.xlsx', '대청호_2016.xlsx', '대청호_2017.xlsx', '대청호_2018.xlsx', '대청호_2019.xlsx', '대청호_2020.xlsx'], ['장계_2015.xlsx', '장계_2016.xlsx', '장계_2017.xlsx', '장계_2018.xlsx', '장계_2019.xlsx', '장계_2020.xlsx']]\n",
      "data/geum/자동/현도_2015.xlsx\n",
      "data/geum/자동/현도_2016.xlsx\n",
      "data/geum/자동/현도_2017.xlsx\n",
      "data/geum/자동/현도_2018.xlsx\n",
      "data/geum/자동/현도_2019.xlsx\n",
      "data/geum/자동/현도_2020.xlsx\n",
      "time range in files :  2015-01-01 00:00  ~  2020-10-31 23:00\n",
      "data/geum/자동/대청호_2015.xlsx\n",
      "data/geum/자동/대청호_2016.xlsx\n",
      "data/geum/자동/대청호_2017.xlsx\n",
      "data/geum/자동/대청호_2018.xlsx\n",
      "data/geum/자동/대청호_2019.xlsx\n",
      "data/geum/자동/대청호_2020.xlsx\n",
      "time range in files :  2015-01-01 00:00  ~  2020-10-31 23:00\n",
      "data/geum/자동/장계_2015.xlsx\n",
      "data/geum/자동/장계_2016.xlsx\n",
      "data/geum/자동/장계_2017.xlsx\n",
      "data/geum/자동/장계_2018.xlsx\n",
      "data/geum/자동/장계_2019.xlsx\n",
      "data/geum/자동/장계_2020.xlsx\n",
      "time range in files :  2015-01-01 00:00  ~  2020-10-31 23:00\n",
      "MissData :  save/  miss :  (9088, 12)\n",
      "1/1 [==============================] - 1s 667ms/step - loss: 0.2369\n",
      "MissData :  save/  miss :  (9088, 12)\n"
     ]
    }
   ],
   "source": [
    "month=12\n",
    "day=31\n",
    "for i in range(length):\n",
    "\n",
    "    idx = run_num[i]\n",
    "\n",
    "    print('interpol flag : ', interpolation_option[idx])\n",
    "    print('folder : ', data_path + folder[idx])\n",
    "    print('colum_idx : ', colum_idx[idx])\n",
    "    print('file_names[idx] : ', file_names[idx])\n",
    "\n",
    "    #start = time.time()\n",
    "\n",
    "    #if watershed == '한강_12days_test':\n",
    "    #    df, times = make_dataframe_temp_12days(folder[idx], file_names[idx], colum_idx[idx], interpolate=interpolation_option[idx])\n",
    "    #else:\n",
    "    df, times , month, day= make_dataframe(data_path+folder[idx], file_names[idx], colum_idx[idx], \n",
    "                               interpolation=interpolation_option[idx],first_file_no=i, month=month, day=day)\n",
    "\n",
    "    df_all, train_mean, train_std, df = normalize(df)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    if i == 0:\n",
    "        dfff = df\n",
    "        target_all = df_all\n",
    "        target_std = train_std\n",
    "        target_mean = train_mean\n",
    "        start_year = str(times.iloc[0].year)\n",
    "        end_year = str(times.iloc[-1].year)\n",
    "\n",
    "    if interpolation_option[idx][0] == False:\n",
    "\n",
    "        loadfiles = ['idx.npy', 'miss.npy', 'discriminator.h5', 'generator.h5']\n",
    "\n",
    "        gain_calc_falg = True\n",
    "\n",
    "        if __GAIN_TRAINING__ == True:\n",
    "            gain_calc_falg = MissData.save(pd.concat(df, axis=0).to_numpy(), max_tseq=24, save_dir='save/')\n",
    "            #print(folder[idx], ': training ', 'Miss date save : ', gain_calc_falg)\n",
    "        else:\n",
    "            for file in loadfiles:\n",
    "                if os.path.isfile('save/' + folder[idx]+file):\n",
    "                    shutil.copyfile('save/' + folder[idx]+file, 'save/'+file)\n",
    "                    #print('load file name : save/' + folder[idx]+file)\n",
    "                else:\n",
    "                    if file == 'miss.npy':\n",
    "                        gain_calc_falg = MissData.save(pd.concat(df, axis=0).to_numpy(), max_tseq=24, save_dir='save/')\n",
    "                        #print(folder[idx], ': is not miss.npy ', 'Miss date save : ', gain_calc_falg)\n",
    "\n",
    "        if gain_calc_falg == True:\n",
    "            #print('GainWindowGenerator in main')\n",
    "            WindowGenerator.make_dataset = make_dataset_gain\n",
    "            wide_window = WindowGenerator(input_width=gain_in_setps, label_width=gain_out_setps, shift=gain_shift,\n",
    "                                          fill_no=gain_fill_no, miss_rate=gain_miss_rate, batch_size=gain_batch_size,\n",
    "                                          train_df = df_all, val_df = df_all, test_df = df_all, df = df)\n",
    "\n",
    "            #gain = model_GAIN(shape=wide_window.dg.shape[1:], gen_sigmoid=False, epochs=gain_epochs, training_flag=__GAIN_TRAINING__, window=wide_window, model_save_path='save/')\n",
    "            gain = model_GAIN(shape=(gain_in_setps, df_all.shape[1]), gen_sigmoid=False, epochs=gain_epochs,\n",
    "                              training_flag=__GAIN_TRAINING__, window=wide_window, model_save_path='save/')\n",
    "\n",
    "            gain_val_performance[str(i)] = gain.evaluate(wide_window.val)\n",
    "            gain_performance[str(i)] = gain.evaluate(wide_window.test, verbose=0)\n",
    "\n",
    "            #print('file proc in main')\n",
    "            if __GAIN_TRAINING__ == True:\n",
    "                #dir = 'save/'+folder[i]\n",
    "                if not os.path.exists('save/' + folder[idx]):\n",
    "                    os.makedirs('save/'+folder[idx])\n",
    "                for file in loadfiles:\n",
    "                    shutil.copyfile('save/' + file, 'save/' + folder[idx] + file)\n",
    "\n",
    "            #print('create_dataset_with_gain in main')\n",
    "            #ori, gan = create_dataset_with_gain(gain=gain, window=wide_window, df=df)\n",
    "            ori, gan = create_dataset_with_gain(gain=gain, shape=(gain_in_setps, df_all.shape[1]), df=df)\n",
    "\n",
    "        else:\n",
    "            gan = create_dataset_interpol(window=gain_in_setps, df=df)\n",
    "    else:\n",
    "        gan = create_dataset_interpol(window=gain_in_setps, df=df)\n",
    "\n",
    "    if i == 0 :\n",
    "#        if i < length -1:\n",
    "#            gan = gan[:,:-4]  #맨마지막전까지 사인코사인삭제\n",
    "#            print(gan.shape)\n",
    "        real_df_all = pd.DataFrame(gan)\n",
    "    else:\n",
    "#        if i < length -1:\n",
    "#            gan = gan[:,:-4]  #맨마지막전까지 사인코사인삭제\n",
    "#            print(gan.shape)\n",
    "        real_df_all = pd.concat([real_df_all, pd.DataFrame(gan)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51144, 36)\n"
     ]
    }
   ],
   "source": [
    "print(real_df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_slice(df, train_ratio, val_ratio, test_ratio):\n",
    "    total_no = df.shape[0]\n",
    "\n",
    "    train_no = int(total_no * train_ratio)\n",
    "    #\n",
    "    val_no = int(total_no * (train_ratio + val_ratio))\n",
    "\n",
    "    # val_no = int(total_no*val_ratio)\n",
    "    # train_no =int(total_no * (train_ratio+val_ratio))\n",
    "    #\n",
    "    # val_slice = slice(0, val_no)\n",
    "    # train_slice = slice(val_no, train_no)\n",
    "    # test_slice = slice(train_no, None)\n",
    "\n",
    "    train_slice = slice(0, train_no)    #0.8\n",
    "    val_slice = slice(train_no, val_no)   #0.1\n",
    "    test_slice = slice(val_no, None)\n",
    "    # val_slice = slice(train_no, val_no)   #0.1\n",
    "    # test_slice = slice(val_no, None)\n",
    "#     train_slice = slice(0, train_no)  # 0.8\n",
    "#     val_slice = slice(train_no, None)  # 0.1\n",
    "#     test_slice = slice(val_no, None)\n",
    "\n",
    "    train = pd.DataFrame(df[train_slice])\n",
    "    val = pd.DataFrame(df[val_slice])\n",
    "    test = pd.DataFrame(df[test_slice])\n",
    "\n",
    "    test_slice2 = slice(total_no - 288, None)\n",
    "    test2 = pd.DataFrame(df[test_slice2])\n",
    "\n",
    "    #print('total_no :1111111111111111111111111 ')\n",
    "    #print('total_no : ', total_no)\n",
    "    #print('train : ', train.shape)\n",
    "    #print('val : ', val.shape)\n",
    "    #print('test : ', test.shape)\n",
    "\n",
    "\n",
    "    return train, val, test, test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmpr_value</th>\n",
       "      <th>ph_value</th>\n",
       "      <th>do_value</th>\n",
       "      <th>ec_value</th>\n",
       "      <th>toc_value</th>\n",
       "      <th>총질소_값</th>\n",
       "      <th>총인_값</th>\n",
       "      <th>클로로필-a_값</th>\n",
       "      <th>Day sin</th>\n",
       "      <th>Day cos</th>\n",
       "      <th>Year sin</th>\n",
       "      <th>Year cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.991165</td>\n",
       "      <td>-0.888834</td>\n",
       "      <td>-0.438505</td>\n",
       "      <td>-0.436215</td>\n",
       "      <td>-0.861440</td>\n",
       "      <td>0.599405</td>\n",
       "      <td>-0.197393</td>\n",
       "      <td>-0.819782</td>\n",
       "      <td>1.747378e-12</td>\n",
       "      <td>1.414209</td>\n",
       "      <td>-0.016785</td>\n",
       "      <td>1.457047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.996774</td>\n",
       "      <td>-0.889673</td>\n",
       "      <td>-0.436925</td>\n",
       "      <td>-0.452594</td>\n",
       "      <td>-0.823175</td>\n",
       "      <td>0.969532</td>\n",
       "      <td>-0.148334</td>\n",
       "      <td>-0.821120</td>\n",
       "      <td>3.660242e-01</td>\n",
       "      <td>1.366021</td>\n",
       "      <td>-0.015777</td>\n",
       "      <td>1.457045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.999685</td>\n",
       "      <td>-0.889926</td>\n",
       "      <td>-0.442673</td>\n",
       "      <td>-0.470742</td>\n",
       "      <td>-0.832008</td>\n",
       "      <td>0.705956</td>\n",
       "      <td>-0.135698</td>\n",
       "      <td>-0.819885</td>\n",
       "      <td>7.071045e-01</td>\n",
       "      <td>1.224741</td>\n",
       "      <td>-0.014769</td>\n",
       "      <td>1.457042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.002813</td>\n",
       "      <td>-0.888594</td>\n",
       "      <td>-0.440886</td>\n",
       "      <td>-0.477825</td>\n",
       "      <td>-0.852607</td>\n",
       "      <td>0.747694</td>\n",
       "      <td>-0.210029</td>\n",
       "      <td>-0.821840</td>\n",
       "      <td>9.999967e-01</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>-0.013761</td>\n",
       "      <td>1.457039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.009741</td>\n",
       "      <td>-0.889513</td>\n",
       "      <td>-0.443455</td>\n",
       "      <td>-0.479595</td>\n",
       "      <td>-0.859974</td>\n",
       "      <td>0.713922</td>\n",
       "      <td>-0.210029</td>\n",
       "      <td>-0.822560</td>\n",
       "      <td>1.224741e+00</td>\n",
       "      <td>0.707104</td>\n",
       "      <td>-0.012753</td>\n",
       "      <td>1.457035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51139</th>\n",
       "      <td>0.404498</td>\n",
       "      <td>-1.127373</td>\n",
       "      <td>-1.253074</td>\n",
       "      <td>-3.347996</td>\n",
       "      <td>0.132293</td>\n",
       "      <td>-1.277243</td>\n",
       "      <td>0.148989</td>\n",
       "      <td>-0.491433</td>\n",
       "      <td>-1.366021e+00</td>\n",
       "      <td>0.366024</td>\n",
       "      <td>-1.232984</td>\n",
       "      <td>0.751588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51140</th>\n",
       "      <td>0.397289</td>\n",
       "      <td>-1.205052</td>\n",
       "      <td>-1.267018</td>\n",
       "      <td>-3.347996</td>\n",
       "      <td>0.152892</td>\n",
       "      <td>-1.287170</td>\n",
       "      <td>0.012964</td>\n",
       "      <td>-0.475953</td>\n",
       "      <td>-1.224741e+00</td>\n",
       "      <td>0.707104</td>\n",
       "      <td>-1.232475</td>\n",
       "      <td>0.752469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51141</th>\n",
       "      <td>0.401408</td>\n",
       "      <td>-1.216144</td>\n",
       "      <td>-1.347446</td>\n",
       "      <td>-3.363932</td>\n",
       "      <td>0.167626</td>\n",
       "      <td>-1.257985</td>\n",
       "      <td>0.012964</td>\n",
       "      <td>-0.470685</td>\n",
       "      <td>-9.999967e-01</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>-1.231966</td>\n",
       "      <td>0.753350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51142</th>\n",
       "      <td>0.410677</td>\n",
       "      <td>-1.216144</td>\n",
       "      <td>-1.382794</td>\n",
       "      <td>-3.398901</td>\n",
       "      <td>0.157309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012964</td>\n",
       "      <td>-0.456422</td>\n",
       "      <td>-7.071045e-01</td>\n",
       "      <td>1.224741</td>\n",
       "      <td>-1.231456</td>\n",
       "      <td>0.754230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51143</th>\n",
       "      <td>0.404498</td>\n",
       "      <td>-1.216144</td>\n",
       "      <td>-1.404198</td>\n",
       "      <td>-3.374998</td>\n",
       "      <td>0.174975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012964</td>\n",
       "      <td>-0.439807</td>\n",
       "      <td>-3.660242e-01</td>\n",
       "      <td>1.366021</td>\n",
       "      <td>-1.230946</td>\n",
       "      <td>0.755109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51144 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tmpr_value  ph_value  do_value  ec_value  toc_value     총질소_값  \\\n",
       "0       -0.991165 -0.888834 -0.438505 -0.436215  -0.861440  0.599405   \n",
       "1       -0.996774 -0.889673 -0.436925 -0.452594  -0.823175  0.969532   \n",
       "2       -0.999685 -0.889926 -0.442673 -0.470742  -0.832008  0.705956   \n",
       "3       -1.002813 -0.888594 -0.440886 -0.477825  -0.852607  0.747694   \n",
       "4       -1.009741 -0.889513 -0.443455 -0.479595  -0.859974  0.713922   \n",
       "...           ...       ...       ...       ...        ...       ...   \n",
       "51139    0.404498 -1.127373 -1.253074 -3.347996   0.132293 -1.277243   \n",
       "51140    0.397289 -1.205052 -1.267018 -3.347996   0.152892 -1.287170   \n",
       "51141    0.401408 -1.216144 -1.347446 -3.363932   0.167626 -1.257985   \n",
       "51142    0.410677 -1.216144 -1.382794 -3.398901   0.157309       NaN   \n",
       "51143    0.404498 -1.216144 -1.404198 -3.374998   0.174975       NaN   \n",
       "\n",
       "           총인_값  클로로필-a_값       Day sin   Day cos  Year sin  Year cos  \n",
       "0     -0.197393 -0.819782  1.747378e-12  1.414209 -0.016785  1.457047  \n",
       "1     -0.148334 -0.821120  3.660242e-01  1.366021 -0.015777  1.457045  \n",
       "2     -0.135698 -0.819885  7.071045e-01  1.224741 -0.014769  1.457042  \n",
       "3     -0.210029 -0.821840  9.999967e-01  0.999997 -0.013761  1.457039  \n",
       "4     -0.210029 -0.822560  1.224741e+00  0.707104 -0.012753  1.457035  \n",
       "...         ...       ...           ...       ...       ...       ...  \n",
       "51139  0.148989 -0.491433 -1.366021e+00  0.366024 -1.232984  0.751588  \n",
       "51140  0.012964 -0.475953 -1.224741e+00  0.707104 -1.232475  0.752469  \n",
       "51141  0.012964 -0.470685 -9.999967e-01  0.999997 -1.231966  0.753350  \n",
       "51142  0.012964 -0.456422 -7.071045e-01  1.224741 -1.231456  0.754230  \n",
       "51143  0.012964 -0.439807 -3.660242e-01  1.366021 -1.230946  0.755109  \n",
       "\n",
       "[51144 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# slice DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_df=real_df_all.iloc[:-2000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, val_df, test_df2 = dataset_slice(real_df_all, 0.7, 0.1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10229, 36), (5115, 36), (35800, 36))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape, test_df.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------prediction\n",
      "-------------------prediction\n",
      "-------------------prediction\n",
      "real_df_all.type :  <class 'pandas.core.frame.DataFrame'>\n",
      "train_df.type :  <class 'pandas.core.frame.DataFrame'>\n",
      "train_df.shape :  (35800, 36) val_df.shape :  (10229, 36) test_df.shape: (5115, 36)\n"
     ]
    }
   ],
   "source": [
    "print('-------------------prediction')\n",
    "print('-------------------prediction')\n",
    "print('-------------------prediction')\n",
    "\n",
    "print('real_df_all.type : ', type(real_df_all))\n",
    "print('train_df.type : ', type(train_df))\n",
    "print('train_df.shape : ', train_df.shape, 'val_df.shape : ', val_df.shape, 'test_df.shape:' ,test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_columns_indices:\n",
      "{'tmpr_value': 0, 'ph_value': 1, 'do_value': 2, 'ec_value': 3, 'toc_value': 4, '총질소_값': 5, '총인_값': 6, '클로로필-a_값': 7, 'Day sin': 8, 'Day cos': 9, 'Year sin': 10, 'Year cos': 11}\n",
      "target columns :  do\n",
      "target_col_idx :  2\n",
      "out_num_features :  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_columns_indices = {name: i for i, name in enumerate(dfff[0])}\n",
    "\n",
    "print(\"label_columns_indices:\")\n",
    "print(label_columns_indices)\n",
    "\n",
    "\n",
    "target_dic = {\"do\":\"do_value\", \"toc\":\"toc_value\", \"tn\":\"총질소_값\", \"tp\":\"총인_값\", \"chl-a\":\"클로로필-a_값\"}\n",
    "\n",
    "rnn_target_column = \"do\"\n",
    "\n",
    "print('target columns : ', rnn_target_column)\n",
    "num_features = dfff[0].shape[1]\n",
    "\n",
    "\n",
    "\n",
    "target_col_idx = label_columns_indices[target_dic[rnn_target_column]]\n",
    "out_features = [target_col_idx]\n",
    "out_num_features = len(out_features)\n",
    "\n",
    "print(\"target_col_idx : \", target_col_idx)\n",
    "print('out_num_features : ', out_num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model path :  save/geum/models/do/\n"
     ]
    }
   ],
   "source": [
    "val_nse = {}\n",
    "val_pbias = {}\n",
    "\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset_water\n",
    "multi_window = WindowGenerator(\n",
    "    input_width=rnn_in_setps,label_width=rnn_out_steps, shift=rnn_out_steps,out_features=out_features,\n",
    "    out_num_features=out_num_features,label_columns=dfff[0].columns, batch_size=rnn_batch_size,\n",
    "    train_df=train_df, val_df=val_df, test_df=test_df, test_df2=test_df2)\n",
    "\n",
    "if __RNN_TRAINING__:\n",
    "    if not os.path.exists('save/' + watershed):\n",
    "        os.makedirs('save/' + watershed)\n",
    "\n",
    "\n",
    "idx = [2, 4, 5, 6, 7]\n",
    "pa = [\"do/\", \"toc/\", \"nitrogen/\", \"phosphorus/\", \"chlorophyll-a/\"]\n",
    "\n",
    "indices = {name: i for i, name in enumerate(idx)}\n",
    "\n",
    "model_path = \"save/\" + watershed + \"models/\" + pa[indices[target_col_idx]]\n",
    "print(\"save model path : \", model_path)\n",
    "\n",
    "val_nse = {}\n",
    "val_pbias = {}\n",
    "\n",
    " # +\"gru.ckpt\" -- path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2448e50f814e41f295e458d3b6f3c5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(real_df_all.iloc[:,out_features[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cdc30b628547d8bc61d0450db21a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(val_df.iloc[:,out_features[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "__RNN_TRAINING__ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/30\n",
      "10/10 [==============================] - 9s 894ms/step - loss: 1.5027 - mean_absolute_error: 0.9449 - nse: -0.1166 - val_loss: 0.5291 - val_mean_absolute_error: 0.6023 - val_nse: 0.1683\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.52914, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 2/30\n",
      "10/10 [==============================] - 8s 847ms/step - loss: 0.5503 - mean_absolute_error: 0.6056 - nse: 0.5671 - val_loss: 0.4184 - val_mean_absolute_error: 0.5267 - val_nse: 0.3423\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.52914 to 0.41845, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - 7s 807ms/step - loss: 0.3320 - mean_absolute_error: 0.4781 - nse: 0.7311 - val_loss: 0.2336 - val_mean_absolute_error: 0.3979 - val_nse: 0.6326\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.41845 to 0.23364, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 4/30\n",
      "10/10 [==============================] - 8s 830ms/step - loss: 0.1927 - mean_absolute_error: 0.3165 - nse: 0.8504 - val_loss: 0.2062 - val_mean_absolute_error: 0.3734 - val_nse: 0.6748\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.23364 to 0.20616, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 5/30\n",
      "10/10 [==============================] - 7s 819ms/step - loss: 0.1002 - mean_absolute_error: 0.2364 - nse: 0.9215 - val_loss: 0.1982 - val_mean_absolute_error: 0.3569 - val_nse: 0.6861\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.20616 to 0.19816, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 6/30\n",
      "10/10 [==============================] - 7s 797ms/step - loss: 0.0782 - mean_absolute_error: 0.2115 - nse: 0.9382 - val_loss: 0.2214 - val_mean_absolute_error: 0.3938 - val_nse: 0.6489\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.19816\n",
      "Epoch 7/30\n",
      "10/10 [==============================] - 7s 809ms/step - loss: 0.0666 - mean_absolute_error: 0.1912 - nse: 0.9518 - val_loss: 0.2081 - val_mean_absolute_error: 0.3708 - val_nse: 0.6715\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.19816\n",
      "Epoch 8/30\n",
      "10/10 [==============================] - 7s 813ms/step - loss: 0.0516 - mean_absolute_error: 0.1736 - nse: 0.9586 - val_loss: 0.1788 - val_mean_absolute_error: 0.3452 - val_nse: 0.7179\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.19816 to 0.17879, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 9/30\n",
      "10/10 [==============================] - 8s 831ms/step - loss: 0.0467 - mean_absolute_error: 0.1623 - nse: 0.9638 - val_loss: 0.1767 - val_mean_absolute_error: 0.3437 - val_nse: 0.7198\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.17879 to 0.17675, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 10/30\n",
      "10/10 [==============================] - 8s 836ms/step - loss: 0.0388 - mean_absolute_error: 0.1485 - nse: 0.9721 - val_loss: 0.1613 - val_mean_absolute_error: 0.3234 - val_nse: 0.7450\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.17675 to 0.16131, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 11/30\n",
      "10/10 [==============================] - 7s 827ms/step - loss: 0.0382 - mean_absolute_error: 0.1482 - nse: 0.9697 - val_loss: 0.1478 - val_mean_absolute_error: 0.3049 - val_nse: 0.7661\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.16131 to 0.14778, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 12/30\n",
      "10/10 [==============================] - 8s 836ms/step - loss: 0.0415 - mean_absolute_error: 0.1503 - nse: 0.9674 - val_loss: 0.1498 - val_mean_absolute_error: 0.3100 - val_nse: 0.7626\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.14778\n",
      "Epoch 13/30\n",
      "10/10 [==============================] - 7s 817ms/step - loss: 0.0356 - mean_absolute_error: 0.1429 - nse: 0.9717 - val_loss: 0.1457 - val_mean_absolute_error: 0.3052 - val_nse: 0.7697\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.14778 to 0.14574, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 14/30\n",
      "10/10 [==============================] - 7s 823ms/step - loss: 0.0369 - mean_absolute_error: 0.1402 - nse: 0.9706 - val_loss: 0.1383 - val_mean_absolute_error: 0.2919 - val_nse: 0.7814\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.14574 to 0.13830, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 15/30\n",
      "10/10 [==============================] - 8s 835ms/step - loss: 0.0363 - mean_absolute_error: 0.1413 - nse: 0.9705 - val_loss: 0.1373 - val_mean_absolute_error: 0.2938 - val_nse: 0.7824\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.13830 to 0.13726, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 16/30\n",
      "10/10 [==============================] - 7s 803ms/step - loss: 0.0316 - mean_absolute_error: 0.1317 - nse: 0.9752 - val_loss: 0.1371 - val_mean_absolute_error: 0.2904 - val_nse: 0.7833\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.13726 to 0.13709, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 17/30\n",
      "10/10 [==============================] - 7s 825ms/step - loss: 0.0326 - mean_absolute_error: 0.1343 - nse: 0.9762 - val_loss: 0.1297 - val_mean_absolute_error: 0.2824 - val_nse: 0.7949\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.13709 to 0.12974, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 18/30\n",
      "10/10 [==============================] - 8s 831ms/step - loss: 0.0334 - mean_absolute_error: 0.1348 - nse: 0.9737 - val_loss: 0.1281 - val_mean_absolute_error: 0.2756 - val_nse: 0.7968\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.12974 to 0.12814, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 19/30\n",
      "10/10 [==============================] - 7s 811ms/step - loss: 0.0320 - mean_absolute_error: 0.1312 - nse: 0.9755 - val_loss: 0.1309 - val_mean_absolute_error: 0.2840 - val_nse: 0.7936\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.12814\n",
      "Epoch 20/30\n",
      "10/10 [==============================] - 8s 833ms/step - loss: 0.0288 - mean_absolute_error: 0.1279 - nse: 0.9790 - val_loss: 0.1317 - val_mean_absolute_error: 0.2846 - val_nse: 0.7913\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.12814\n",
      "Epoch 21/30\n",
      "10/10 [==============================] - 8s 841ms/step - loss: 0.0300 - mean_absolute_error: 0.1292 - nse: 0.9749 - val_loss: 0.1363 - val_mean_absolute_error: 0.2886 - val_nse: 0.7843\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12814\n",
      "Epoch 22/30\n",
      "10/10 [==============================] - 7s 821ms/step - loss: 0.0306 - mean_absolute_error: 0.1297 - nse: 0.9760 - val_loss: 0.1369 - val_mean_absolute_error: 0.2918 - val_nse: 0.7832\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12814\n",
      "Epoch 23/30\n",
      "10/10 [==============================] - 7s 826ms/step - loss: 0.0293 - mean_absolute_error: 0.1273 - nse: 0.9756 - val_loss: 0.1351 - val_mean_absolute_error: 0.2913 - val_nse: 0.7860\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12814\n",
      "Epoch 24/30\n",
      "10/10 [==============================] - 7s 819ms/step - loss: 0.0270 - mean_absolute_error: 0.1221 - nse: 0.9787 - val_loss: 0.1396 - val_mean_absolute_error: 0.2975 - val_nse: 0.7794\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12814\n",
      "Epoch 25/30\n",
      "10/10 [==============================] - 7s 816ms/step - loss: 0.0284 - mean_absolute_error: 0.1256 - nse: 0.9768 - val_loss: 0.1351 - val_mean_absolute_error: 0.2915 - val_nse: 0.7864\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12814\n",
      "Epoch 26/30\n",
      "10/10 [==============================] - 7s 799ms/step - loss: 0.0279 - mean_absolute_error: 0.1244 - nse: 0.9788 - val_loss: 0.1263 - val_mean_absolute_error: 0.2780 - val_nse: 0.8002\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.12814 to 0.12635, saving model to save/geum/models/do/elman.ckpt\n",
      "Epoch 27/30\n",
      "10/10 [==============================] - 7s 823ms/step - loss: 0.0258 - mean_absolute_error: 0.1199 - nse: 0.9802 - val_loss: 0.1379 - val_mean_absolute_error: 0.2944 - val_nse: 0.7819\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12635\n",
      "Epoch 28/30\n",
      "10/10 [==============================] - 8s 850ms/step - loss: 0.0285 - mean_absolute_error: 0.1236 - nse: 0.9782 - val_loss: 0.1475 - val_mean_absolute_error: 0.3105 - val_nse: 0.7665\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12635\n",
      "Epoch 29/30\n",
      "10/10 [==============================] - 7s 801ms/step - loss: 0.0283 - mean_absolute_error: 0.1247 - nse: 0.9789 - val_loss: 0.1300 - val_mean_absolute_error: 0.2810 - val_nse: 0.7943\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12635\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 7s 814ms/step - loss: 0.0282 - mean_absolute_error: 0.1210 - nse: 0.9795 - val_loss: 0.1326 - val_mean_absolute_error: 0.2890 - val_nse: 0.7901\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.12635\n",
      "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/30\n",
      "10/10 [==============================] - 11s 969ms/step - loss: 1.3115 - mean_absolute_error: 0.8790 - nse: -0.0063 - val_loss: 0.6345 - val_mean_absolute_error: 0.7075 - val_nse: -0.0108\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63447, saving model to save/geum/models/do/gru.ckpt\n",
      "Epoch 2/30\n",
      "10/10 [==============================] - 9s 956ms/step - loss: 0.3674 - mean_absolute_error: 0.4828 - nse: 0.7293 - val_loss: 0.2349 - val_mean_absolute_error: 0.4109 - val_nse: 0.6283\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63447 to 0.23494, saving model to save/geum/models/do/gru.ckpt\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - 9s 943ms/step - loss: 0.1531 - mean_absolute_error: 0.2939 - nse: 0.8762 - val_loss: 0.2186 - val_mean_absolute_error: 0.3875 - val_nse: 0.6539\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.23494 to 0.21860, saving model to save/geum/models/do/gru.ckpt\n",
      "Epoch 4/30\n",
      "10/10 [==============================] - 9s 946ms/step - loss: 0.1100 - mean_absolute_error: 0.2590 - nse: 0.9125 - val_loss: 0.2874 - val_mean_absolute_error: 0.4467 - val_nse: 0.5426\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.21860\n",
      "Epoch 5/30\n",
      "10/10 [==============================] - 9s 958ms/step - loss: 0.0838 - mean_absolute_error: 0.2198 - nse: 0.9372 - val_loss: 0.1870 - val_mean_absolute_error: 0.3249 - val_nse: 0.7033\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.21860 to 0.18704, saving model to save/geum/models/do/gru.ckpt\n",
      "Epoch 6/30\n",
      "10/10 [==============================] - 9s 940ms/step - loss: 0.0674 - mean_absolute_error: 0.1980 - nse: 0.9509 - val_loss: 0.1344 - val_mean_absolute_error: 0.2628 - val_nse: 0.7871\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.18704 to 0.13439, saving model to save/geum/models/do/gru.ckpt\n",
      "Epoch 7/30\n",
      "10/10 [==============================] - 9s 965ms/step - loss: 0.0546 - mean_absolute_error: 0.1793 - nse: 0.9587 - val_loss: 0.1264 - val_mean_absolute_error: 0.2553 - val_nse: 0.7984\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.13439 to 0.12641, saving model to save/geum/models/do/gru.ckpt\n",
      "Epoch 8/30\n",
      "10/10 [==============================] - 9s 971ms/step - loss: 0.0505 - mean_absolute_error: 0.1720 - nse: 0.9604 - val_loss: 0.1325 - val_mean_absolute_error: 0.2712 - val_nse: 0.7895\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12641\n",
      "Epoch 9/30\n",
      "10/10 [==============================] - 9s 926ms/step - loss: 0.0467 - mean_absolute_error: 0.1645 - nse: 0.9626 - val_loss: 0.1352 - val_mean_absolute_error: 0.2732 - val_nse: 0.7857\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12641\n",
      "Epoch 10/30\n",
      "10/10 [==============================] - 9s 942ms/step - loss: 0.0502 - mean_absolute_error: 0.1704 - nse: 0.9628 - val_loss: 0.1256 - val_mean_absolute_error: 0.2548 - val_nse: 0.7996\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12641 to 0.12556, saving model to save/geum/models/do/gru.ckpt\n",
      "Epoch 11/30\n",
      "10/10 [==============================] - 9s 960ms/step - loss: 0.0483 - mean_absolute_error: 0.1644 - nse: 0.9598 - val_loss: 0.1371 - val_mean_absolute_error: 0.2739 - val_nse: 0.7824\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.12556\n",
      "Epoch 12/30\n",
      "10/10 [==============================] - 9s 930ms/step - loss: 0.0427 - mean_absolute_error: 0.1571 - nse: 0.9669 - val_loss: 0.1255 - val_mean_absolute_error: 0.2649 - val_nse: 0.8013\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.12556 to 0.12546, saving model to save/geum/models/do/gru.ckpt\n",
      "Epoch 13/30\n",
      "10/10 [==============================] - 9s 934ms/step - loss: 0.0453 - mean_absolute_error: 0.1619 - nse: 0.9641 - val_loss: 0.1303 - val_mean_absolute_error: 0.2668 - val_nse: 0.7930\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.12546\n",
      "Epoch 14/30\n",
      "10/10 [==============================] - 9s 953ms/step - loss: 0.0436 - mean_absolute_error: 0.1596 - nse: 0.9667 - val_loss: 0.1282 - val_mean_absolute_error: 0.2669 - val_nse: 0.7964\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.12546\n",
      "Epoch 15/30\n",
      "10/10 [==============================] - 9s 943ms/step - loss: 0.0435 - mean_absolute_error: 0.1583 - nse: 0.9656 - val_loss: 0.1156 - val_mean_absolute_error: 0.2529 - val_nse: 0.8173\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.12546 to 0.11564, saving model to save/geum/models/do/gru.ckpt\n",
      "Epoch 16/30\n",
      "10/10 [==============================] - 9s 961ms/step - loss: 0.0440 - mean_absolute_error: 0.1596 - nse: 0.9661 - val_loss: 0.1479 - val_mean_absolute_error: 0.2957 - val_nse: 0.7661\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11564\n",
      "Epoch 17/30\n",
      "10/10 [==============================] - 9s 935ms/step - loss: 0.0423 - mean_absolute_error: 0.1563 - nse: 0.9672 - val_loss: 0.1270 - val_mean_absolute_error: 0.2703 - val_nse: 0.7989\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11564\n",
      "Epoch 18/30\n",
      "10/10 [==============================] - 9s 913ms/step - loss: 0.0434 - mean_absolute_error: 0.1578 - nse: 0.9644 - val_loss: 0.1236 - val_mean_absolute_error: 0.2623 - val_nse: 0.8049\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11564\n",
      "Epoch 19/30\n",
      "10/10 [==============================] - 9s 940ms/step - loss: 0.0389 - mean_absolute_error: 0.1488 - nse: 0.9685 - val_loss: 0.1292 - val_mean_absolute_error: 0.2748 - val_nse: 0.7947\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.11564\n",
      "Epoch 20/30\n",
      "10/10 [==============================] - 9s 938ms/step - loss: 0.0392 - mean_absolute_error: 0.1499 - nse: 0.9681 - val_loss: 0.1257 - val_mean_absolute_error: 0.2738 - val_nse: 0.8000\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.11564\n",
      "Epoch 21/30\n",
      "10/10 [==============================] - 9s 956ms/step - loss: 0.0404 - mean_absolute_error: 0.1515 - nse: 0.9696 - val_loss: 0.1213 - val_mean_absolute_error: 0.2656 - val_nse: 0.8079\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.11564\n",
      "Epoch 22/30\n",
      "10/10 [==============================] - 9s 945ms/step - loss: 0.0353 - mean_absolute_error: 0.1434 - nse: 0.9714 - val_loss: 0.1256 - val_mean_absolute_error: 0.2665 - val_nse: 0.8006\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.11564\n",
      "Epoch 23/30\n",
      "10/10 [==============================] - 9s 950ms/step - loss: 0.0431 - mean_absolute_error: 0.1556 - nse: 0.9668 - val_loss: 0.1346 - val_mean_absolute_error: 0.2809 - val_nse: 0.7862\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.11564\n",
      "Epoch 24/30\n",
      "10/10 [==============================] - 9s 930ms/step - loss: 0.0388 - mean_absolute_error: 0.1495 - nse: 0.9664 - val_loss: 0.1401 - val_mean_absolute_error: 0.2983 - val_nse: 0.7781\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.11564\n",
      "Epoch 25/30\n",
      "10/10 [==============================] - 9s 954ms/step - loss: 0.0428 - mean_absolute_error: 0.1571 - nse: 0.9643 - val_loss: 0.1626 - val_mean_absolute_error: 0.3037 - val_nse: 0.7417\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.11564\n",
      "Epoch 26/30\n",
      "10/10 [==============================] - 9s 940ms/step - loss: 0.0393 - mean_absolute_error: 0.1498 - nse: 0.9692 - val_loss: 0.1443 - val_mean_absolute_error: 0.2971 - val_nse: 0.7710\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.11564\n",
      "Epoch 27/30\n",
      "10/10 [==============================] - 9s 937ms/step - loss: 0.0394 - mean_absolute_error: 0.1495 - nse: 0.9712 - val_loss: 0.1436 - val_mean_absolute_error: 0.3009 - val_nse: 0.7731\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.11564\n",
      "Epoch 28/30\n",
      "10/10 [==============================] - 9s 969ms/step - loss: 0.0382 - mean_absolute_error: 0.1482 - nse: 0.9690 - val_loss: 0.1501 - val_mean_absolute_error: 0.2919 - val_nse: 0.7616\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.11564\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 9s 944ms/step - loss: 0.0363 - mean_absolute_error: 0.1439 - nse: 0.9729 - val_loss: 0.1352 - val_mean_absolute_error: 0.2774 - val_nse: 0.7852\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.11564\n",
      "Epoch 30/30\n",
      "10/10 [==============================] - 9s 948ms/step - loss: 0.0383 - mean_absolute_error: 0.1468 - nse: 0.9720 - val_loss: 0.1670 - val_mean_absolute_error: 0.3263 - val_nse: 0.7366\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.11564\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/30\n",
      "10/10 [==============================] - 10s 919ms/step - loss: 1.5066 - mean_absolute_error: 0.9564 - nse: -0.1700 - val_loss: 0.5572 - val_mean_absolute_error: 0.5709 - val_nse: 0.1285\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55722, saving model to save/geum/models/do/multi_lstm.ckpt\n",
      "Epoch 2/30\n",
      "10/10 [==============================] - 7s 826ms/step - loss: 0.5639 - mean_absolute_error: 0.5866 - nse: 0.5481 - val_loss: 0.3986 - val_mean_absolute_error: 0.5450 - val_nse: 0.3697\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55722 to 0.39860, saving model to save/geum/models/do/multi_lstm.ckpt\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - 8s 863ms/step - loss: 0.3566 - mean_absolute_error: 0.4504 - nse: 0.7328 - val_loss: 0.2554 - val_mean_absolute_error: 0.4247 - val_nse: 0.5950\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.39860 to 0.25539, saving model to save/geum/models/do/multi_lstm.ckpt\n",
      "Epoch 4/30\n",
      "10/10 [==============================] - 7s 804ms/step - loss: 0.1647 - mean_absolute_error: 0.2998 - nse: 0.8688 - val_loss: 0.2571 - val_mean_absolute_error: 0.4302 - val_nse: 0.5930\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25539\n",
      "Epoch 5/30\n",
      "10/10 [==============================] - 7s 826ms/step - loss: 0.0977 - mean_absolute_error: 0.2312 - nse: 0.9253 - val_loss: 0.2927 - val_mean_absolute_error: 0.4493 - val_nse: 0.5357\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.25539\n",
      "Epoch 6/30\n",
      "10/10 [==============================] - 7s 831ms/step - loss: 0.0536 - mean_absolute_error: 0.1734 - nse: 0.9553 - val_loss: 0.2256 - val_mean_absolute_error: 0.3995 - val_nse: 0.6417\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.25539 to 0.22559, saving model to save/geum/models/do/multi_lstm.ckpt\n",
      "Epoch 7/30\n",
      "10/10 [==============================] - 7s 817ms/step - loss: 0.0526 - mean_absolute_error: 0.1718 - nse: 0.9544 - val_loss: 0.2625 - val_mean_absolute_error: 0.4247 - val_nse: 0.5853\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.22559\n",
      "Epoch 8/30\n",
      "10/10 [==============================] - 7s 817ms/step - loss: 0.0378 - mean_absolute_error: 0.1468 - nse: 0.9701 - val_loss: 0.2211 - val_mean_absolute_error: 0.3922 - val_nse: 0.6504\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.22559 to 0.22110, saving model to save/geum/models/do/multi_lstm.ckpt\n",
      "Epoch 9/30\n",
      "10/10 [==============================] - 7s 823ms/step - loss: 0.0351 - mean_absolute_error: 0.1413 - nse: 0.9735 - val_loss: 0.1948 - val_mean_absolute_error: 0.3661 - val_nse: 0.6896\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.22110 to 0.19478, saving model to save/geum/models/do/multi_lstm.ckpt\n",
      "Epoch 10/30\n",
      "10/10 [==============================] - 8s 856ms/step - loss: 0.0316 - mean_absolute_error: 0.1338 - nse: 0.9763 - val_loss: 0.2200 - val_mean_absolute_error: 0.3926 - val_nse: 0.6518\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.19478\n",
      "Epoch 11/30\n",
      "10/10 [==============================] - 7s 824ms/step - loss: 0.0290 - mean_absolute_error: 0.1279 - nse: 0.9759 - val_loss: 0.2094 - val_mean_absolute_error: 0.3809 - val_nse: 0.6690\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.19478\n",
      "Epoch 12/30\n",
      "10/10 [==============================] - 8s 873ms/step - loss: 0.0289 - mean_absolute_error: 0.1284 - nse: 0.9764 - val_loss: 0.2063 - val_mean_absolute_error: 0.3740 - val_nse: 0.6708\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.19478\n",
      "Epoch 13/30\n",
      "10/10 [==============================] - 8s 863ms/step - loss: 0.0276 - mean_absolute_error: 0.1242 - nse: 0.9793 - val_loss: 0.2030 - val_mean_absolute_error: 0.3721 - val_nse: 0.6789\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.19478\n",
      "Epoch 14/30\n",
      "10/10 [==============================] - 8s 853ms/step - loss: 0.0277 - mean_absolute_error: 0.1242 - nse: 0.9784 - val_loss: 0.2076 - val_mean_absolute_error: 0.3822 - val_nse: 0.6715\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.19478\n",
      "Epoch 15/30\n",
      "10/10 [==============================] - 8s 850ms/step - loss: 0.0251 - mean_absolute_error: 0.1201 - nse: 0.9806 - val_loss: 0.2100 - val_mean_absolute_error: 0.3824 - val_nse: 0.6647\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.19478\n",
      "Epoch 16/30\n",
      "10/10 [==============================] - 8s 837ms/step - loss: 0.0284 - mean_absolute_error: 0.1245 - nse: 0.9783 - val_loss: 0.2034 - val_mean_absolute_error: 0.3787 - val_nse: 0.6782\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.19478\n",
      "Epoch 17/30\n",
      "10/10 [==============================] - 8s 844ms/step - loss: 0.0241 - mean_absolute_error: 0.1175 - nse: 0.9816 - val_loss: 0.2029 - val_mean_absolute_error: 0.3761 - val_nse: 0.6788\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.19478\n",
      "Epoch 18/30\n",
      "10/10 [==============================] - 8s 848ms/step - loss: 0.0239 - mean_absolute_error: 0.1166 - nse: 0.9804 - val_loss: 0.2049 - val_mean_absolute_error: 0.3818 - val_nse: 0.6731\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.19478\n",
      "Epoch 19/30\n",
      "10/10 [==============================] - 8s 838ms/step - loss: 0.0242 - mean_absolute_error: 0.1158 - nse: 0.9813 - val_loss: 0.1890 - val_mean_absolute_error: 0.3632 - val_nse: 0.7017\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.19478 to 0.18897, saving model to save/geum/models/do/multi_lstm.ckpt\n",
      "Epoch 20/30\n",
      "10/10 [==============================] - 7s 821ms/step - loss: 0.0229 - mean_absolute_error: 0.1134 - nse: 0.9821 - val_loss: 0.1875 - val_mean_absolute_error: 0.3614 - val_nse: 0.7031\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.18897 to 0.18746, saving model to save/geum/models/do/multi_lstm.ckpt\n",
      "Epoch 21/30\n",
      "10/10 [==============================] - 8s 864ms/step - loss: 0.0242 - mean_absolute_error: 0.1166 - nse: 0.9819 - val_loss: 0.1898 - val_mean_absolute_error: 0.3615 - val_nse: 0.6976\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.18746\n",
      "Epoch 22/30\n",
      "10/10 [==============================] - 8s 837ms/step - loss: 0.0238 - mean_absolute_error: 0.1159 - nse: 0.9804 - val_loss: 0.1893 - val_mean_absolute_error: 0.3605 - val_nse: 0.7000\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.18746\n",
      "Epoch 23/30\n",
      "10/10 [==============================] - 8s 833ms/step - loss: 0.0247 - mean_absolute_error: 0.1162 - nse: 0.9812 - val_loss: 0.1975 - val_mean_absolute_error: 0.3665 - val_nse: 0.6876\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.18746\n",
      "Epoch 24/30\n",
      "10/10 [==============================] - 7s 825ms/step - loss: 0.0231 - mean_absolute_error: 0.1146 - nse: 0.9822 - val_loss: 0.1961 - val_mean_absolute_error: 0.3652 - val_nse: 0.6876\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.18746\n",
      "Epoch 25/30\n",
      "10/10 [==============================] - 7s 808ms/step - loss: 0.0207 - mean_absolute_error: 0.1093 - nse: 0.9844 - val_loss: 0.1884 - val_mean_absolute_error: 0.3602 - val_nse: 0.7014\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.18746\n",
      "Epoch 26/30\n",
      "10/10 [==============================] - 7s 814ms/step - loss: 0.0206 - mean_absolute_error: 0.1087 - nse: 0.9842 - val_loss: 0.1926 - val_mean_absolute_error: 0.3630 - val_nse: 0.6950\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.18746\n",
      "Epoch 27/30\n",
      "10/10 [==============================] - 8s 833ms/step - loss: 0.0210 - mean_absolute_error: 0.1085 - nse: 0.9832 - val_loss: 0.1890 - val_mean_absolute_error: 0.3608 - val_nse: 0.6995\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.18746\n",
      "Epoch 28/30\n",
      "10/10 [==============================] - 8s 839ms/step - loss: 0.0218 - mean_absolute_error: 0.1097 - nse: 0.9831 - val_loss: 0.1917 - val_mean_absolute_error: 0.3653 - val_nse: 0.6963\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.18746\n",
      "Epoch 29/30\n",
      "10/10 [==============================] - 8s 848ms/step - loss: 0.0210 - mean_absolute_error: 0.1087 - nse: 0.9839 - val_loss: 0.1885 - val_mean_absolute_error: 0.3586 - val_nse: 0.7016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: val_loss did not improve from 0.18746\n",
      "Epoch 30/30\n",
      "10/10 [==============================] - 7s 829ms/step - loss: 0.0210 - mean_absolute_error: 0.1087 - nse: 0.9835 - val_loss: 0.1883 - val_mean_absolute_error: 0.3567 - val_nse: 0.7006\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.18746\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/30\n",
      "10/10 [==============================] - 8s 816ms/step - loss: 1.0648 - mean_absolute_error: 0.7810 - nse: 0.1656 - val_loss: 0.3473 - val_mean_absolute_error: 0.4886 - val_nse: 0.4488\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34733, saving model to save/geum/models/do/multi_conv.ckpt\n",
      "Epoch 2/30\n",
      "10/10 [==============================] - 7s 807ms/step - loss: 0.1793 - mean_absolute_error: 0.3327 - nse: 0.8594 - val_loss: 0.3368 - val_mean_absolute_error: 0.4680 - val_nse: 0.4650\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34733 to 0.33676, saving model to save/geum/models/do/multi_conv.ckpt\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - 7s 811ms/step - loss: 0.1084 - mean_absolute_error: 0.2468 - nse: 0.9155 - val_loss: 0.2013 - val_mean_absolute_error: 0.3749 - val_nse: 0.6814\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33676 to 0.20129, saving model to save/geum/models/do/multi_conv.ckpt\n",
      "Epoch 4/30\n",
      "10/10 [==============================] - 7s 780ms/step - loss: 0.0766 - mean_absolute_error: 0.2087 - nse: 0.9417 - val_loss: 0.2140 - val_mean_absolute_error: 0.3907 - val_nse: 0.6606\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.20129\n",
      "Epoch 5/30\n",
      "10/10 [==============================] - 7s 821ms/step - loss: 0.0582 - mean_absolute_error: 0.1786 - nse: 0.9566 - val_loss: 0.2186 - val_mean_absolute_error: 0.3929 - val_nse: 0.6534\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.20129\n",
      "Epoch 6/30\n",
      "10/10 [==============================] - 7s 797ms/step - loss: 0.0462 - mean_absolute_error: 0.1598 - nse: 0.9652 - val_loss: 0.2041 - val_mean_absolute_error: 0.3805 - val_nse: 0.6750\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.20129\n",
      "Epoch 7/30\n",
      "10/10 [==============================] - 8s 838ms/step - loss: 0.0408 - mean_absolute_error: 0.1523 - nse: 0.9663 - val_loss: 0.2025 - val_mean_absolute_error: 0.3779 - val_nse: 0.6792\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.20129\n",
      "Epoch 8/30\n",
      "10/10 [==============================] - 7s 816ms/step - loss: 0.0366 - mean_absolute_error: 0.1466 - nse: 0.9725 - val_loss: 0.2046 - val_mean_absolute_error: 0.3814 - val_nse: 0.6759\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.20129\n",
      "Epoch 9/30\n",
      "10/10 [==============================] - 7s 793ms/step - loss: 0.0373 - mean_absolute_error: 0.1410 - nse: 0.9721 - val_loss: 0.1802 - val_mean_absolute_error: 0.3573 - val_nse: 0.7136\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.20129 to 0.18023, saving model to save/geum/models/do/multi_conv.ckpt\n",
      "Epoch 10/30\n",
      "10/10 [==============================] - 7s 793ms/step - loss: 0.0304 - mean_absolute_error: 0.1322 - nse: 0.9755 - val_loss: 0.1799 - val_mean_absolute_error: 0.3525 - val_nse: 0.7146\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.18023 to 0.17985, saving model to save/geum/models/do/multi_conv.ckpt\n",
      "Epoch 11/30\n",
      "10/10 [==============================] - 8s 851ms/step - loss: 0.0330 - mean_absolute_error: 0.1357 - nse: 0.9747 - val_loss: 0.1517 - val_mean_absolute_error: 0.3173 - val_nse: 0.7594\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.17985 to 0.15171, saving model to save/geum/models/do/multi_conv.ckpt\n",
      "Epoch 12/30\n",
      "10/10 [==============================] - 7s 804ms/step - loss: 0.0330 - mean_absolute_error: 0.1344 - nse: 0.9718 - val_loss: 0.1633 - val_mean_absolute_error: 0.3316 - val_nse: 0.7404\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.15171\n",
      "Epoch 13/30\n",
      "10/10 [==============================] - 7s 824ms/step - loss: 0.0327 - mean_absolute_error: 0.1358 - nse: 0.9752 - val_loss: 0.1711 - val_mean_absolute_error: 0.3441 - val_nse: 0.7286\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.15171\n",
      "Epoch 14/30\n",
      "10/10 [==============================] - 7s 808ms/step - loss: 0.0303 - mean_absolute_error: 0.1310 - nse: 0.9765 - val_loss: 0.1452 - val_mean_absolute_error: 0.3113 - val_nse: 0.7696\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.15171 to 0.14519, saving model to save/geum/models/do/multi_conv.ckpt\n",
      "Epoch 15/30\n",
      "10/10 [==============================] - 7s 797ms/step - loss: 0.0305 - mean_absolute_error: 0.1293 - nse: 0.9766 - val_loss: 0.1590 - val_mean_absolute_error: 0.3241 - val_nse: 0.7476\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.14519\n",
      "Epoch 16/30\n",
      "10/10 [==============================] - 7s 799ms/step - loss: 0.0292 - mean_absolute_error: 0.1268 - nse: 0.9779 - val_loss: 0.1432 - val_mean_absolute_error: 0.3015 - val_nse: 0.7725\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.14519 to 0.14321, saving model to save/geum/models/do/multi_conv.ckpt\n",
      "Epoch 17/30\n",
      "10/10 [==============================] - 7s 790ms/step - loss: 0.0285 - mean_absolute_error: 0.1239 - nse: 0.9774 - val_loss: 0.1561 - val_mean_absolute_error: 0.3194 - val_nse: 0.7529\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.14321\n",
      "Epoch 18/30\n",
      "10/10 [==============================] - 7s 821ms/step - loss: 0.0277 - mean_absolute_error: 0.1220 - nse: 0.9776 - val_loss: 0.1561 - val_mean_absolute_error: 0.3189 - val_nse: 0.7519\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.14321\n",
      "Epoch 19/30\n",
      "10/10 [==============================] - 7s 832ms/step - loss: 0.0281 - mean_absolute_error: 0.1228 - nse: 0.9783 - val_loss: 0.1454 - val_mean_absolute_error: 0.3076 - val_nse: 0.7699\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.14321\n",
      "Epoch 20/30\n",
      "10/10 [==============================] - 7s 828ms/step - loss: 0.0276 - mean_absolute_error: 0.1239 - nse: 0.9782 - val_loss: 0.1278 - val_mean_absolute_error: 0.2787 - val_nse: 0.7977\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.14321 to 0.12778, saving model to save/geum/models/do/multi_conv.ckpt\n",
      "Epoch 21/30\n",
      "10/10 [==============================] - 7s 808ms/step - loss: 0.0267 - mean_absolute_error: 0.1214 - nse: 0.9796 - val_loss: 0.1387 - val_mean_absolute_error: 0.2943 - val_nse: 0.7794\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12778\n",
      "Epoch 22/30\n",
      "10/10 [==============================] - 7s 803ms/step - loss: 0.0277 - mean_absolute_error: 0.1221 - nse: 0.9766 - val_loss: 0.1432 - val_mean_absolute_error: 0.2917 - val_nse: 0.7723\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12778\n",
      "Epoch 23/30\n",
      "10/10 [==============================] - 7s 792ms/step - loss: 0.0284 - mean_absolute_error: 0.1237 - nse: 0.9771 - val_loss: 0.1568 - val_mean_absolute_error: 0.3174 - val_nse: 0.7513\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12778\n",
      "Epoch 24/30\n",
      "10/10 [==============================] - 7s 790ms/step - loss: 0.0244 - mean_absolute_error: 0.1168 - nse: 0.9810 - val_loss: 0.1436 - val_mean_absolute_error: 0.3029 - val_nse: 0.7720\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12778\n",
      "Epoch 25/30\n",
      "10/10 [==============================] - 7s 784ms/step - loss: 0.0258 - mean_absolute_error: 0.1178 - nse: 0.9794 - val_loss: 0.1451 - val_mean_absolute_error: 0.3051 - val_nse: 0.7693\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12778\n",
      "Epoch 26/30\n",
      "10/10 [==============================] - 7s 804ms/step - loss: 0.0286 - mean_absolute_error: 0.1263 - nse: 0.9777 - val_loss: 0.1474 - val_mean_absolute_error: 0.3064 - val_nse: 0.7661\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12778\n",
      "Epoch 27/30\n",
      "10/10 [==============================] - 7s 795ms/step - loss: 0.0266 - mean_absolute_error: 0.1216 - nse: 0.9783 - val_loss: 0.1370 - val_mean_absolute_error: 0.2944 - val_nse: 0.7828\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.12778\n",
      "Epoch 28/30\n",
      "10/10 [==============================] - 7s 782ms/step - loss: 0.0253 - mean_absolute_error: 0.1166 - nse: 0.9793 - val_loss: 0.1494 - val_mean_absolute_error: 0.3067 - val_nse: 0.7625\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.12778\n",
      "Epoch 29/30\n",
      "10/10 [==============================] - 7s 802ms/step - loss: 0.0241 - mean_absolute_error: 0.1147 - nse: 0.9814 - val_loss: 0.1478 - val_mean_absolute_error: 0.3051 - val_nse: 0.7661\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.12778\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 7s 788ms/step - loss: 0.0243 - mean_absolute_error: 0.1167 - nse: 0.9812 - val_loss: 0.1335 - val_mean_absolute_error: 0.2872 - val_nse: 0.7878\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.12778\n"
     ]
    }
   ],
   "source": [
    "rnn_epochs = 30\n",
    "# multi_linear_model = model_multi_linear(\n",
    "#     window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "#     #training_flag=__RNN_TRAINING__, checkpoint_path=\"save/\"+watershed+\"models/multi_linear.ckpt\")\n",
    "#     training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"multi_linear.ckpt\")\n",
    "print('ELMAN Running....')\n",
    "elman_model = model_elman(\n",
    "    window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "    training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"elman.ckpt\")\n",
    "print('GRU Running....')\n",
    "gru_model = model_gru(\n",
    "    window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "    training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"gru.ckpt\")\n",
    "print('LSTM Running....')\n",
    "multi_lstm_model = model_multi_lstm(\n",
    "    window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "    training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"multi_lstm.ckpt\")\n",
    "print('CNN Running....')\n",
    "multi_conv_model = model_multi_conv(\n",
    "    window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "    training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"multi_conv.ckpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## core / window.py / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_to_day_mean(array):\n",
    "    time = 24\n",
    "    array = array.reshape((array.shape[0], array.shape[1] // time, time, array.shape[2]))\n",
    "    array = array.mean(2)\n",
    "    return array\n",
    "\n",
    "def compa(model=None,df = None, plot_col=0, input_width=7*24, label_width=5*24, target_std=None, target_mean=None, predict_day=4):\n",
    "    \n",
    "    print(df.shape)\n",
    "    print(plot_col)\n",
    "    \n",
    "    width = input_width + label_width\n",
    "    \n",
    "    length = df.shape[0]\n",
    "    length -= width\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "   \n",
    "    for i in range(0,length,24):\n",
    "#         print('i = ', i)\n",
    "        dataset = df.iloc[i:i+width].to_numpy()\n",
    "        input = dataset[:input_width]\n",
    "        label = dataset[input_width:, plot_col:plot_col+1]\n",
    "        \n",
    "        input = input.reshape((-1,)+input.shape)\n",
    "        label = label.reshape((-1,)+label.shape)\n",
    "        \n",
    "        inputs.append(input)\n",
    "        labels.append(label)\n",
    "        \n",
    "    inputs = np.concatenate(inputs, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "#     print('labels.mean(axis=1)')\n",
    "#     print(labels.mean(axis=1).shape)\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "\n",
    "    predictions = model(inputs)\n",
    "    print(predictions.shape)\n",
    "    \n",
    "    predictions = predictions.numpy() * target_std[plot_col] + target_mean[plot_col]\n",
    "    labels = labels * target_std[plot_col] + target_mean[plot_col]\n",
    "\n",
    "    print('predictions.shape =', predictions.shape)\n",
    "#     pred_day = predictions\n",
    "    pred_day = hour_to_day_mean(predictions)\n",
    "    print('pred_day.shape =', pred_day.shape)\n",
    "#     label_day= labels\n",
    "    label_day = hour_to_day_mean(labels)\n",
    "    \n",
    "    inputs_target = inputs[:,:,plot_col:plot_col+1]\n",
    "    inputs_target = inputs_target * target_std[plot_col] + target_mean[plot_col]\n",
    "#     inputs_day = inputs_target\n",
    "    inputs_day = hour_to_day_mean(inputs_target)\n",
    "    \n",
    "    #plt.figure(figsize=(10, 800))\n",
    "    \n",
    "#     input_index = np.array(range(0,length,24))\n",
    "#     label_index = input_index + 24*7\n",
    "    \n",
    "#     print(input_index)\n",
    "#     print(label_index)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(label_day[:, predict_day, :], label='label')\n",
    "    plt.plot(pred_day[:, predict_day, :], label='pred')\n",
    "#     plt.plot(input_index, inputs_day[:, 0, :], label='input')\n",
    "#     plt.plot(label_index, label_day[:, 0, :], label='label')\n",
    "#     plt.plot(label_index, pred_day[:, 0, :], label='pred')\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "        \n",
    "#     o1 = np.mean(labels)\n",
    "# #     print('labels =', labels)\n",
    "#     print('o1 =', o1)\n",
    "#     nse1 = ((labels - predictions)**2).sum()\n",
    "#     nse2 = ((labels - o1)**2).sum()\n",
    "#     nse3 = 1 - (nse1/nse2)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print('pred_day.shape =',pred_day.shape)\n",
    "#     print('label_day.shape =',label_day.shape)\n",
    "    #print('label_index =', label_index)\n",
    "    \n",
    "    o1 = np.mean(label_day[:,predict_day,0])\n",
    "#     o1 = np.mean(label_day)\n",
    "#     print('labels =', labels)\n",
    "#     o1 = target_mean[plot_col]\n",
    "    print('o1 =', o1)\n",
    "    nse1 = ((pred_day-label_day)**2).sum(axis=0)\n",
    "    nse2 = ((label_day - o1)**2).sum(axis=0)\n",
    "    nse3 = 1 - (nse1[predict_day]/nse2[predict_day])\n",
    "    print('nse3 =', nse3)\n",
    "    #mean = np.mean(label_day[predict_day])\n",
    "    print('label_day[predict_day].shape =', label_day[predict_day].shape)\n",
    "#     print(type(pred_day))\n",
    "#     print(pred_day[:20, 0, 0])\n",
    "#     print(label_day[:20, 0, 0])\n",
    "    o = label_day[:, 0, 0]\n",
    "    s = pred_day[:, 0, 0]\n",
    "    mean = np.mean(o)\n",
    "    print('mean =', mean)\n",
    "    nse = 1. - np.sum((o-s)**2)/np.sum((o-mean)**2)\n",
    "#     print('nse =', nse)\n",
    "    \n",
    "    \n",
    "#     pbias1 = (label_day - pred_day).sum(axis=0)\n",
    "#     pbias2 = (label_day).sum(axis=0)\n",
    "#     pbias3 = (pbias1[predict_day]/pbias2[predict_day])*100\n",
    "    \n",
    "    \n",
    "#     o1 = np.mean(label_day)\n",
    "# #     print('labels =', labels)\n",
    "#     print('o1 =', o1)\n",
    "#     nse1 = ((label_day - pred_day)**2).sum()\n",
    "#     nse2 = ((label_day - o1)**2).sum()\n",
    "#     nse3 = 1 - (nse1/nse2)\n",
    "#     print('nse3 =', nse3)\n",
    "    #mean = np.mean(label_day[predict_day])\n",
    "#     print(type(pred_day))\n",
    "#     print(pred_day[:20, 0, 0])\n",
    "#     print(label_day[:20, 0, 0])\n",
    "#     o = label_day[:, 0, 0]\n",
    "#     s = pred_day[:, 0, 0]\n",
    "#     mean = np.mean(o)\n",
    "#     print('mean =', mean)\n",
    "#     nse = 1. - np.sum((o-s)**2)/np.sum((o-mean)**2)\n",
    "#     print('nse =', nse)\n",
    "    \n",
    "    \n",
    "    pbias1 = (label_day - pred_day).sum(axis=0)\n",
    "    pbias2 = (label_day).sum(axis=0)\n",
    "    pbias3 = (pbias1[predict_day]/pbias2[predict_day])*100\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return float(nse3), float(np.abs(pbias3)), pred_day, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35800</th>\n",
       "      <td>-1.135188</td>\n",
       "      <td>-0.683542</td>\n",
       "      <td>-0.035966</td>\n",
       "      <td>-0.532714</td>\n",
       "      <td>-0.194921</td>\n",
       "      <td>-0.590213</td>\n",
       "      <td>-0.451605</td>\n",
       "      <td>-0.592412</td>\n",
       "      <td>-1.224741</td>\n",
       "      <td>-7.071045e-01</td>\n",
       "      <td>0.691402</td>\n",
       "      <td>1.262056</td>\n",
       "      <td>-1.115392</td>\n",
       "      <td>-0.602630</td>\n",
       "      <td>0.500946</td>\n",
       "      <td>-1.072765</td>\n",
       "      <td>-0.091048</td>\n",
       "      <td>0.022988</td>\n",
       "      <td>-0.507353</td>\n",
       "      <td>-0.602982</td>\n",
       "      <td>-1.224741</td>\n",
       "      <td>-7.071045e-01</td>\n",
       "      <td>0.691402</td>\n",
       "      <td>1.262056</td>\n",
       "      <td>-1.517278</td>\n",
       "      <td>-0.550392</td>\n",
       "      <td>1.562846</td>\n",
       "      <td>0.711145</td>\n",
       "      <td>0.398755</td>\n",
       "      <td>1.154230</td>\n",
       "      <td>-0.205267</td>\n",
       "      <td>0.324512</td>\n",
       "      <td>-1.224741</td>\n",
       "      <td>-7.071045e-01</td>\n",
       "      <td>0.691402</td>\n",
       "      <td>1.262056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35801</th>\n",
       "      <td>-1.143427</td>\n",
       "      <td>-0.683542</td>\n",
       "      <td>-0.035966</td>\n",
       "      <td>-0.532714</td>\n",
       "      <td>-0.114747</td>\n",
       "      <td>-0.596201</td>\n",
       "      <td>-0.748928</td>\n",
       "      <td>-0.579040</td>\n",
       "      <td>-1.366021</td>\n",
       "      <td>-3.660242e-01</td>\n",
       "      <td>0.692272</td>\n",
       "      <td>1.261540</td>\n",
       "      <td>-1.180503</td>\n",
       "      <td>-0.816693</td>\n",
       "      <td>0.392822</td>\n",
       "      <td>-0.957662</td>\n",
       "      <td>-0.008070</td>\n",
       "      <td>-0.045589</td>\n",
       "      <td>-0.544518</td>\n",
       "      <td>-0.609542</td>\n",
       "      <td>-1.366021</td>\n",
       "      <td>-3.660242e-01</td>\n",
       "      <td>0.692272</td>\n",
       "      <td>1.261540</td>\n",
       "      <td>-1.489471</td>\n",
       "      <td>-0.550392</td>\n",
       "      <td>1.592033</td>\n",
       "      <td>0.781970</td>\n",
       "      <td>0.487087</td>\n",
       "      <td>1.213331</td>\n",
       "      <td>-0.161937</td>\n",
       "      <td>0.564968</td>\n",
       "      <td>-1.366021</td>\n",
       "      <td>-3.660242e-01</td>\n",
       "      <td>0.692272</td>\n",
       "      <td>1.261540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35802</th>\n",
       "      <td>-1.148577</td>\n",
       "      <td>-0.683542</td>\n",
       "      <td>-0.055424</td>\n",
       "      <td>-0.532714</td>\n",
       "      <td>-0.155677</td>\n",
       "      <td>-0.535362</td>\n",
       "      <td>-0.767511</td>\n",
       "      <td>-0.553431</td>\n",
       "      <td>-1.414209</td>\n",
       "      <td>-2.493397e-13</td>\n",
       "      <td>0.693141</td>\n",
       "      <td>1.261024</td>\n",
       "      <td>-1.183593</td>\n",
       "      <td>-0.694634</td>\n",
       "      <td>0.384331</td>\n",
       "      <td>-0.957662</td>\n",
       "      <td>-0.003291</td>\n",
       "      <td>-0.267059</td>\n",
       "      <td>-0.544518</td>\n",
       "      <td>-0.620292</td>\n",
       "      <td>-1.414209</td>\n",
       "      <td>-2.493397e-13</td>\n",
       "      <td>0.693141</td>\n",
       "      <td>1.261024</td>\n",
       "      <td>-1.489471</td>\n",
       "      <td>-0.550392</td>\n",
       "      <td>1.598521</td>\n",
       "      <td>0.764264</td>\n",
       "      <td>0.384039</td>\n",
       "      <td>1.142389</td>\n",
       "      <td>-0.081008</td>\n",
       "      <td>0.548030</td>\n",
       "      <td>-1.414209</td>\n",
       "      <td>-2.493397e-13</td>\n",
       "      <td>0.693141</td>\n",
       "      <td>1.261024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35803</th>\n",
       "      <td>-1.155786</td>\n",
       "      <td>-0.683542</td>\n",
       "      <td>-0.045695</td>\n",
       "      <td>-0.532714</td>\n",
       "      <td>-0.150010</td>\n",
       "      <td>-0.499034</td>\n",
       "      <td>-0.563101</td>\n",
       "      <td>-0.543949</td>\n",
       "      <td>-1.366021</td>\n",
       "      <td>3.660242e-01</td>\n",
       "      <td>0.694010</td>\n",
       "      <td>1.260507</td>\n",
       "      <td>-1.189772</td>\n",
       "      <td>-0.683542</td>\n",
       "      <td>0.380439</td>\n",
       "      <td>-0.957662</td>\n",
       "      <td>-0.058585</td>\n",
       "      <td>-0.324834</td>\n",
       "      <td>-0.507353</td>\n",
       "      <td>-0.618022</td>\n",
       "      <td>-1.366021</td>\n",
       "      <td>3.660242e-01</td>\n",
       "      <td>0.694010</td>\n",
       "      <td>1.260507</td>\n",
       "      <td>-1.489471</td>\n",
       "      <td>-0.550392</td>\n",
       "      <td>1.579062</td>\n",
       "      <td>0.755411</td>\n",
       "      <td>0.398755</td>\n",
       "      <td>1.162190</td>\n",
       "      <td>-0.096142</td>\n",
       "      <td>0.608083</td>\n",
       "      <td>-1.366021</td>\n",
       "      <td>3.660242e-01</td>\n",
       "      <td>0.694010</td>\n",
       "      <td>1.260507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35804</th>\n",
       "      <td>-1.165055</td>\n",
       "      <td>-0.683542</td>\n",
       "      <td>-0.061911</td>\n",
       "      <td>-0.532714</td>\n",
       "      <td>-0.160074</td>\n",
       "      <td>-0.471162</td>\n",
       "      <td>-0.247194</td>\n",
       "      <td>-0.539816</td>\n",
       "      <td>-1.224741</td>\n",
       "      <td>7.071045e-01</td>\n",
       "      <td>0.694878</td>\n",
       "      <td>1.259989</td>\n",
       "      <td>-1.192862</td>\n",
       "      <td>-0.683542</td>\n",
       "      <td>0.376224</td>\n",
       "      <td>-0.957662</td>\n",
       "      <td>-0.084250</td>\n",
       "      <td>-0.139395</td>\n",
       "      <td>-0.631485</td>\n",
       "      <td>-0.619237</td>\n",
       "      <td>-1.224741</td>\n",
       "      <td>7.071045e-01</td>\n",
       "      <td>0.694878</td>\n",
       "      <td>1.259989</td>\n",
       "      <td>-1.500800</td>\n",
       "      <td>-0.550392</td>\n",
       "      <td>1.559604</td>\n",
       "      <td>0.773117</td>\n",
       "      <td>0.398755</td>\n",
       "      <td>1.238588</td>\n",
       "      <td>0.235957</td>\n",
       "      <td>0.598276</td>\n",
       "      <td>-1.224741</td>\n",
       "      <td>7.071045e-01</td>\n",
       "      <td>0.694878</td>\n",
       "      <td>1.259989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40910</th>\n",
       "      <td>-0.156793</td>\n",
       "      <td>-1.349294</td>\n",
       "      <td>-2.307057</td>\n",
       "      <td>-0.054647</td>\n",
       "      <td>-0.649444</td>\n",
       "      <td>-0.622462</td>\n",
       "      <td>-1.102000</td>\n",
       "      <td>-0.681155</td>\n",
       "      <td>-0.707105</td>\n",
       "      <td>-1.224741e+00</td>\n",
       "      <td>-1.239085</td>\n",
       "      <td>-0.673889</td>\n",
       "      <td>1.641396</td>\n",
       "      <td>2.112614</td>\n",
       "      <td>-0.400483</td>\n",
       "      <td>0.156057</td>\n",
       "      <td>0.445872</td>\n",
       "      <td>-0.201546</td>\n",
       "      <td>-0.018255</td>\n",
       "      <td>1.679395</td>\n",
       "      <td>-0.707105</td>\n",
       "      <td>-1.224741e+00</td>\n",
       "      <td>-1.239085</td>\n",
       "      <td>-0.673889</td>\n",
       "      <td>1.567244</td>\n",
       "      <td>1.735360</td>\n",
       "      <td>1.123418</td>\n",
       "      <td>-0.214003</td>\n",
       "      <td>2.368556</td>\n",
       "      <td>0.275674</td>\n",
       "      <td>-0.210029</td>\n",
       "      <td>2.975522</td>\n",
       "      <td>-0.707105</td>\n",
       "      <td>-1.224741e+00</td>\n",
       "      <td>-1.239085</td>\n",
       "      <td>-0.673889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40911</th>\n",
       "      <td>-0.150613</td>\n",
       "      <td>-1.227235</td>\n",
       "      <td>-2.278197</td>\n",
       "      <td>-0.054647</td>\n",
       "      <td>-0.670060</td>\n",
       "      <td>-0.568189</td>\n",
       "      <td>-1.102000</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-9.999967e-01</td>\n",
       "      <td>-1.239586</td>\n",
       "      <td>-0.673003</td>\n",
       "      <td>1.655815</td>\n",
       "      <td>2.112614</td>\n",
       "      <td>-0.322324</td>\n",
       "      <td>0.157827</td>\n",
       "      <td>0.487087</td>\n",
       "      <td>-0.302372</td>\n",
       "      <td>0.175005</td>\n",
       "      <td>1.498993</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-9.999967e-01</td>\n",
       "      <td>-1.239586</td>\n",
       "      <td>-0.673003</td>\n",
       "      <td>1.565185</td>\n",
       "      <td>1.846314</td>\n",
       "      <td>1.297892</td>\n",
       "      <td>-0.214003</td>\n",
       "      <td>2.336174</td>\n",
       "      <td>0.269109</td>\n",
       "      <td>-0.210029</td>\n",
       "      <td>2.912714</td>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-9.999967e-01</td>\n",
       "      <td>-1.239586</td>\n",
       "      <td>-0.673003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40912</th>\n",
       "      <td>-0.154733</td>\n",
       "      <td>-1.216144</td>\n",
       "      <td>-2.278197</td>\n",
       "      <td>-0.054647</td>\n",
       "      <td>-0.692143</td>\n",
       "      <td>-0.592699</td>\n",
       "      <td>-1.102000</td>\n",
       "      <td>-0.652222</td>\n",
       "      <td>-1.224741</td>\n",
       "      <td>-7.071045e-01</td>\n",
       "      <td>-1.240085</td>\n",
       "      <td>-0.672117</td>\n",
       "      <td>1.649636</td>\n",
       "      <td>2.245765</td>\n",
       "      <td>-0.276597</td>\n",
       "      <td>0.179074</td>\n",
       "      <td>0.481204</td>\n",
       "      <td>-0.372980</td>\n",
       "      <td>0.384619</td>\n",
       "      <td>1.337474</td>\n",
       "      <td>-1.224741</td>\n",
       "      <td>-7.071045e-01</td>\n",
       "      <td>-1.240085</td>\n",
       "      <td>-0.672117</td>\n",
       "      <td>1.587842</td>\n",
       "      <td>1.879601</td>\n",
       "      <td>1.318646</td>\n",
       "      <td>-0.244988</td>\n",
       "      <td>2.393589</td>\n",
       "      <td>0.027942</td>\n",
       "      <td>-0.210029</td>\n",
       "      <td>2.716426</td>\n",
       "      <td>-1.224741</td>\n",
       "      <td>-7.071045e-01</td>\n",
       "      <td>-1.240085</td>\n",
       "      <td>-0.672117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40913</th>\n",
       "      <td>-0.135165</td>\n",
       "      <td>-1.327098</td>\n",
       "      <td>-2.299601</td>\n",
       "      <td>-0.054647</td>\n",
       "      <td>-0.705393</td>\n",
       "      <td>-0.627714</td>\n",
       "      <td>-1.102000</td>\n",
       "      <td>-0.634717</td>\n",
       "      <td>-1.366021</td>\n",
       "      <td>-3.660242e-01</td>\n",
       "      <td>-1.240585</td>\n",
       "      <td>-0.671231</td>\n",
       "      <td>1.644486</td>\n",
       "      <td>2.245765</td>\n",
       "      <td>-0.244168</td>\n",
       "      <td>0.210946</td>\n",
       "      <td>0.563636</td>\n",
       "      <td>-0.402743</td>\n",
       "      <td>-0.079950</td>\n",
       "      <td>1.234873</td>\n",
       "      <td>-1.366021</td>\n",
       "      <td>-3.660242e-01</td>\n",
       "      <td>-1.240585</td>\n",
       "      <td>-0.671231</td>\n",
       "      <td>1.625948</td>\n",
       "      <td>1.846314</td>\n",
       "      <td>1.153251</td>\n",
       "      <td>-0.284827</td>\n",
       "      <td>2.308190</td>\n",
       "      <td>0.151808</td>\n",
       "      <td>-0.247194</td>\n",
       "      <td>2.377665</td>\n",
       "      <td>-1.366021</td>\n",
       "      <td>-3.660242e-01</td>\n",
       "      <td>-1.240585</td>\n",
       "      <td>-0.671231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40914</th>\n",
       "      <td>-0.125896</td>\n",
       "      <td>-1.338203</td>\n",
       "      <td>-2.308680</td>\n",
       "      <td>-0.054647</td>\n",
       "      <td>-0.703927</td>\n",
       "      <td>-0.618961</td>\n",
       "      <td>-1.102000</td>\n",
       "      <td>-0.608945</td>\n",
       "      <td>-1.414209</td>\n",
       "      <td>-1.920840e-12</td>\n",
       "      <td>-1.241083</td>\n",
       "      <td>-0.670344</td>\n",
       "      <td>1.622858</td>\n",
       "      <td>2.245765</td>\n",
       "      <td>-0.215631</td>\n",
       "      <td>0.174205</td>\n",
       "      <td>0.513587</td>\n",
       "      <td>-0.370354</td>\n",
       "      <td>0.601665</td>\n",
       "      <td>1.214207</td>\n",
       "      <td>-1.414209</td>\n",
       "      <td>-1.920840e-12</td>\n",
       "      <td>-1.241083</td>\n",
       "      <td>-0.670344</td>\n",
       "      <td>1.614620</td>\n",
       "      <td>1.846314</td>\n",
       "      <td>1.149687</td>\n",
       "      <td>-0.174164</td>\n",
       "      <td>2.188942</td>\n",
       "      <td>0.065724</td>\n",
       "      <td>-0.321525</td>\n",
       "      <td>2.132266</td>\n",
       "      <td>-1.414209</td>\n",
       "      <td>-1.920840e-12</td>\n",
       "      <td>-1.241083</td>\n",
       "      <td>-0.670344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5115 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "35800 -1.135188 -0.683542 -0.035966 -0.532714 -0.194921 -0.590213 -0.451605   \n",
       "35801 -1.143427 -0.683542 -0.035966 -0.532714 -0.114747 -0.596201 -0.748928   \n",
       "35802 -1.148577 -0.683542 -0.055424 -0.532714 -0.155677 -0.535362 -0.767511   \n",
       "35803 -1.155786 -0.683542 -0.045695 -0.532714 -0.150010 -0.499034 -0.563101   \n",
       "35804 -1.165055 -0.683542 -0.061911 -0.532714 -0.160074 -0.471162 -0.247194   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "40910 -0.156793 -1.349294 -2.307057 -0.054647 -0.649444 -0.622462 -1.102000   \n",
       "40911 -0.150613 -1.227235 -2.278197 -0.054647 -0.670060 -0.568189 -1.102000   \n",
       "40912 -0.154733 -1.216144 -2.278197 -0.054647 -0.692143 -0.592699 -1.102000   \n",
       "40913 -0.135165 -1.327098 -2.299601 -0.054647 -0.705393 -0.627714 -1.102000   \n",
       "40914 -0.125896 -1.338203 -2.308680 -0.054647 -0.703927 -0.618961 -1.102000   \n",
       "\n",
       "             7         8             9         10        11        12  \\\n",
       "35800 -0.592412 -1.224741 -7.071045e-01  0.691402  1.262056 -1.115392   \n",
       "35801 -0.579040 -1.366021 -3.660242e-01  0.692272  1.261540 -1.180503   \n",
       "35802 -0.553431 -1.414209 -2.493397e-13  0.693141  1.261024 -1.183593   \n",
       "35803 -0.543949 -1.366021  3.660242e-01  0.694010  1.260507 -1.189772   \n",
       "35804 -0.539816 -1.224741  7.071045e-01  0.694878  1.259989 -1.192862   \n",
       "...         ...       ...           ...       ...       ...       ...   \n",
       "40910 -0.681155 -0.707105 -1.224741e+00 -1.239085 -0.673889  1.641396   \n",
       "40911 -0.674834 -0.999997 -9.999967e-01 -1.239586 -0.673003  1.655815   \n",
       "40912 -0.652222 -1.224741 -7.071045e-01 -1.240085 -0.672117  1.649636   \n",
       "40913 -0.634717 -1.366021 -3.660242e-01 -1.240585 -0.671231  1.644486   \n",
       "40914 -0.608945 -1.414209 -1.920840e-12 -1.241083 -0.670344  1.622858   \n",
       "\n",
       "             13        14        15        16        17        18        19  \\\n",
       "35800 -0.602630  0.500946 -1.072765 -0.091048  0.022988 -0.507353 -0.602982   \n",
       "35801 -0.816693  0.392822 -0.957662 -0.008070 -0.045589 -0.544518 -0.609542   \n",
       "35802 -0.694634  0.384331 -0.957662 -0.003291 -0.267059 -0.544518 -0.620292   \n",
       "35803 -0.683542  0.380439 -0.957662 -0.058585 -0.324834 -0.507353 -0.618022   \n",
       "35804 -0.683542  0.376224 -0.957662 -0.084250 -0.139395 -0.631485 -0.619237   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "40910  2.112614 -0.400483  0.156057  0.445872 -0.201546 -0.018255  1.679395   \n",
       "40911  2.112614 -0.322324  0.157827  0.487087 -0.302372  0.175005  1.498993   \n",
       "40912  2.245765 -0.276597  0.179074  0.481204 -0.372980  0.384619  1.337474   \n",
       "40913  2.245765 -0.244168  0.210946  0.563636 -0.402743 -0.079950  1.234873   \n",
       "40914  2.245765 -0.215631  0.174205  0.513587 -0.370354  0.601665  1.214207   \n",
       "\n",
       "             20            21        22        23        24        25  \\\n",
       "35800 -1.224741 -7.071045e-01  0.691402  1.262056 -1.517278 -0.550392   \n",
       "35801 -1.366021 -3.660242e-01  0.692272  1.261540 -1.489471 -0.550392   \n",
       "35802 -1.414209 -2.493397e-13  0.693141  1.261024 -1.489471 -0.550392   \n",
       "35803 -1.366021  3.660242e-01  0.694010  1.260507 -1.489471 -0.550392   \n",
       "35804 -1.224741  7.071045e-01  0.694878  1.259989 -1.500800 -0.550392   \n",
       "...         ...           ...       ...       ...       ...       ...   \n",
       "40910 -0.707105 -1.224741e+00 -1.239085 -0.673889  1.567244  1.735360   \n",
       "40911 -0.999997 -9.999967e-01 -1.239586 -0.673003  1.565185  1.846314   \n",
       "40912 -1.224741 -7.071045e-01 -1.240085 -0.672117  1.587842  1.879601   \n",
       "40913 -1.366021 -3.660242e-01 -1.240585 -0.671231  1.625948  1.846314   \n",
       "40914 -1.414209 -1.920840e-12 -1.241083 -0.670344  1.614620  1.846314   \n",
       "\n",
       "             26        27        28        29        30        31        32  \\\n",
       "35800  1.562846  0.711145  0.398755  1.154230 -0.205267  0.324512 -1.224741   \n",
       "35801  1.592033  0.781970  0.487087  1.213331 -0.161937  0.564968 -1.366021   \n",
       "35802  1.598521  0.764264  0.384039  1.142389 -0.081008  0.548030 -1.414209   \n",
       "35803  1.579062  0.755411  0.398755  1.162190 -0.096142  0.608083 -1.366021   \n",
       "35804  1.559604  0.773117  0.398755  1.238588  0.235957  0.598276 -1.224741   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "40910  1.123418 -0.214003  2.368556  0.275674 -0.210029  2.975522 -0.707105   \n",
       "40911  1.297892 -0.214003  2.336174  0.269109 -0.210029  2.912714 -0.999997   \n",
       "40912  1.318646 -0.244988  2.393589  0.027942 -0.210029  2.716426 -1.224741   \n",
       "40913  1.153251 -0.284827  2.308190  0.151808 -0.247194  2.377665 -1.366021   \n",
       "40914  1.149687 -0.174164  2.188942  0.065724 -0.321525  2.132266 -1.414209   \n",
       "\n",
       "                 33        34        35  \n",
       "35800 -7.071045e-01  0.691402  1.262056  \n",
       "35801 -3.660242e-01  0.692272  1.261540  \n",
       "35802 -2.493397e-13  0.693141  1.261024  \n",
       "35803  3.660242e-01  0.694010  1.260507  \n",
       "35804  7.071045e-01  0.694878  1.259989  \n",
       "...             ...       ...       ...  \n",
       "40910 -1.224741e+00 -1.239085 -0.673889  \n",
       "40911 -9.999967e-01 -1.239586 -0.673003  \n",
       "40912 -7.071045e-01 -1.240085 -0.672117  \n",
       "40913 -3.660242e-01 -1.240585 -0.671231  \n",
       "40914 -1.920840e-12 -1.241083 -0.670344  \n",
       "\n",
       "[5115 rows x 36 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10229, 36)\n",
      "2\n",
      "(415, 168, 36)\n",
      "(415, 120, 1)\n",
      "(415, 120, 1)\n",
      "predictions.shape = (415, 120, 1)\n",
      "pred_day.shape = (415, 5, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847f370f38df4ace9b76a78262a8a236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = 7.03642\n",
      "nse3 = [0.7752466]\n",
      "label_day[predict_day].shape = (5, 1)\n",
      "mean = 7.012401\n",
      "0.7752466201782227\n",
      "2.1803395748138428\n"
     ]
    }
   ],
   "source": [
    "val_nse['GRU'], val_pbias['GRU'], pred, label = compa(model=gru_model,\n",
    "                                                      df=val_df,\n",
    "                                                      plot_col=out_features[0], \n",
    "                                                      target_std=target_std, \n",
    "                                                      target_mean=target_mean,\n",
    "                                                      predict_day = 4)\n",
    "\n",
    "print(val_nse['GRU'])\n",
    "print(val_pbias['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10229, 36)\n",
      "2\n",
      "(415, 168, 36)\n",
      "(415, 120, 1)\n",
      "(415, 120, 1)\n",
      "predictions.shape = (415, 120, 1)\n",
      "pred_day.shape = (415, 5, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d4d959cf0b4d5fbceb31544c9f0077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = 7.03642\n",
      "nse3 = [0.6783006]\n",
      "label_day[predict_day].shape = (5, 1)\n",
      "mean = 7.012401\n",
      "0.6783006191253662\n",
      "4.866395950317383\n"
     ]
    }
   ],
   "source": [
    "val_nse['LSTM'], val_pbias['LSTM'], pred, label = compa(model=multi_lstm_model,\n",
    "                                                      df=val_df,\n",
    "                                                      plot_col=out_features[0], \n",
    "                                                      target_std=target_std, \n",
    "                                                      target_mean=target_mean,\n",
    "                                                      predict_day = 4)\n",
    "\n",
    "print(val_nse['LSTM'])\n",
    "print(val_pbias['LSTM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_nse['CNN'], val_pbias['CNN'], pred, label = compa(model=multi_conv_model,\n",
    "                                                      df=val_df,\n",
    "                                                      plot_col=out_features[0], \n",
    "                                                      target_std=target_std, \n",
    "                                                      target_mean=target_mean,\n",
    "                                                      predict_day = 4)\n",
    "\n",
    "print(val_nse['CNN'])\n",
    "print(val_pbias['CNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_nse['GRU'], val_pbias['GRU'], pred, label = compa(\n",
    "    model=gru_model,df=train_df,\n",
    "    plot_col=out_features[0], target_std=target_std, target_mean=target_mean, predict_day = 4)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(val_nse['GRU'])\n",
    "print(val_pbias['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_nse['GRU'], val_pbias['GRU'], pred, label = compa(\n",
    "    model=gru_model,df=real_df_all,\n",
    "    plot_col=out_features[0], target_std=target_std, target_mean=target_mean, predict_day = 4)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(val_nse['GRU'])\n",
    "print(val_pbias['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72710395], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_nse['GRU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a062297d190748408b910173bb1baeb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(len(val_nse))\n",
    "width = 0.35\n",
    "plt.figure()\n",
    "plt.title(watershed + '['+start_year+','+end_year+']' + rnn_target_column)\n",
    "# plt.bar(x, val_pbias.values(), 0.3, label='PBIAS' )\n",
    "plt.bar(val_nse.keys(), val_nse.values(), 0.3, label='NSE')\n",
    "# plt.xticks(x,val_nse.keys(), rotation=45)\n",
    "_ = plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt['a'] = 0.1\n",
    "tt['b'] =0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc18f52abe5e4aeb96ba570fceba4bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.bar(tt.keys(), tt.values(), 0.3, label='NSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
