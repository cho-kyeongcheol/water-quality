{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from core.gain import *\n",
    "from core.rnn_predic import *\n",
    "from core.models import *\n",
    "from core.util import *\n",
    "#from core.window import WindowGenerator, MissData, make_dataset_water, WaterDataGenerator\n",
    "from core.window import WindowGenerator, make_dataset_gain, make_dataset_water\n",
    "from core.file_open import make_dataframe\n",
    "from core.miss_data import MissData\n",
    "import json\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "parameters_dir = 'input'\n",
    "\n",
    "parameters_file = 'input.json'\n",
    "parameters_path = '{dir}/{file}'.format(dir=parameters_dir, file=parameters_file)\n",
    "\n",
    "with open(parameters_path, encoding='utf8') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "gain_parameters = parameters['gain']\n",
    "rnn_parameters = parameters['rnn']\n",
    "file_parameters = parameters['file']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input json name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_parameters['watershed'] = 'yeongsan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_path = parameters_dir+'/'+ file_parameters['watershed'] + '.json'\n",
    "with open(parameters_path, encoding='utf8') as json_file:\n",
    "    parameters = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parameters = parameters['data']\n",
    "\n",
    "interpolation_option = data_parameters['interpolation']\n",
    "colum_idx = data_parameters['columns']\n",
    "watershed = data_parameters['watershed']\n",
    "file_names = data_parameters['files']\n",
    "folder = data_parameters['directorys']\n",
    "for i in range(len(folder)):\n",
    "    folder[i] = watershed+folder[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "__GAIN_TRAINING__ = gain_parameters['train']\n",
    "gain_epochs = gain_parameters['max_epochs']\n",
    "gain_in_setps = gain_parameters['input_width']\n",
    "gain_out_setps = gain_parameters['label_width']\n",
    "gain_batch_size = gain_parameters['batch_size']\n",
    "gain_fill_no = gain_parameters['fill_width']\n",
    "gain_shift = gain_parameters['shift_width']\n",
    "gain_miss_rate = gain_parameters['miss_rate']\n",
    "\n",
    "__RNN_TRAINING__ = rnn_parameters['train']\n",
    "rnn_epochs = rnn_parameters['max_epochs']\n",
    "rnn_in_setps = rnn_parameters['input_width']\n",
    "rnn_out_steps = rnn_parameters['label_width']\n",
    "rnn_batch_size = rnn_parameters['batch_size']\n",
    "rnn_predict_day = rnn_parameters['predict_day']\n",
    "rnn_target_column = rnn_parameters['target_column']\n",
    "\n",
    "if rnn_predict_day < 3 or rnn_predict_day >5:\n",
    "    print('predict_day err')\n",
    "    exit(88)\n",
    "rnn_predict_day -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gain_epochs , rnn_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_num = range(len(folder))\n",
    "\n",
    "\n",
    "run_num = [0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "real_df_all = pd.DataFrame([])\n",
    "target_all = target_mean = target_std = 0\n",
    "\n",
    "gain_val_performance = {}\n",
    "gain_performance = {}\n",
    "\n",
    "length = len(run_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gain Training Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "__GAIN_TRAINING__ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpol flag :  [False, False]\n",
      "folder :  data/yeong/자동/\n",
      "colum_idx :  :,[26,27,28,29,30,31,32,33]\n",
      "file_names[idx] :  [['나주_2015.xlsx', '나주_2016.xlsx', '나주_2017.xlsx', '나주_2018.xlsx', '나주_2019.xlsx', '나주_2020.xlsx'], ['지석천_2015.xlsx', '지석천_2016.xlsx', '지석천_2017.xlsx', '지석천_2018.xlsx', '지석천_2019.xlsx', '지석천_2020.xlsx'], ['용봉_2015.xlsx', '용봉_2016.xlsx', '용봉_2017.xlsx', '용봉_2018.xlsx', '용봉_2019.xlsx', '용봉_2020.xlsx'], ['우치_2015.xlsx', '우치_2016.xlsx', '우치_2017.xlsx', '우치_2018.xlsx', '우치_2019.xlsx', '우치_2020.xlsx']]\n",
      "data/yeong/자동/나주_2015.xlsx\n",
      "data/yeong/자동/나주_2016.xlsx\n",
      "data/yeong/자동/나주_2017.xlsx\n",
      "data/yeong/자동/나주_2018.xlsx\n",
      "data/yeong/자동/나주_2019.xlsx\n",
      "data/yeong/자동/나주_2020.xlsx\n",
      "time range in files :  2015-01-01 00:00  ~  2020-12-31 23:00\n",
      "data/yeong/자동/지석천_2015.xlsx\n",
      "data/yeong/자동/지석천_2016.xlsx\n",
      "data/yeong/자동/지석천_2017.xlsx\n",
      "data/yeong/자동/지석천_2018.xlsx\n",
      "data/yeong/자동/지석천_2019.xlsx\n",
      "data/yeong/자동/지석천_2020.xlsx\n",
      "time range in files :  2015-01-01 00:00  ~  2020-12-31 23:00\n",
      "data/yeong/자동/용봉_2015.xlsx\n",
      "data/yeong/자동/용봉_2016.xlsx\n",
      "data/yeong/자동/용봉_2017.xlsx\n",
      "data/yeong/자동/용봉_2018.xlsx\n",
      "data/yeong/자동/용봉_2019.xlsx\n",
      "data/yeong/자동/용봉_2020.xlsx\n",
      "time range in files :  2015-01-01 00:00  ~  2020-12-31 23:00\n",
      "data/yeong/자동/우치_2015.xlsx\n",
      "data/yeong/자동/우치_2016.xlsx\n",
      "data/yeong/자동/우치_2017.xlsx\n",
      "data/yeong/자동/우치_2018.xlsx\n",
      "data/yeong/자동/우치_2019.xlsx\n",
      "data/yeong/자동/우치_2020.xlsx\n",
      "time range in files :  2015-01-01 00:00  ~  2020-12-31 23:00\n",
      "MissData :  save/  miss :  (14780, 12)\n",
      "1/1 [==============================] - 1s 668ms/step - loss: 0.2752\n",
      "MissData :  save/  miss :  (14780, 12)\n"
     ]
    }
   ],
   "source": [
    "for i in range(length):\n",
    "\n",
    "    idx = run_num[i]\n",
    "\n",
    "    print('interpol flag : ', interpolation_option[idx])\n",
    "    print('folder : ', data_path + folder[idx])\n",
    "    print('colum_idx : ', colum_idx[idx])\n",
    "    print('file_names[idx] : ', file_names[idx])\n",
    "\n",
    "    #start = time.time()\n",
    "\n",
    "    #if watershed == '한강_12days_test':\n",
    "    #    df, times = make_dataframe_temp_12days(folder[idx], file_names[idx], colum_idx[idx], interpolate=interpolation_option[idx])\n",
    "    #else:\n",
    "    df, times = make_dataframe(data_path+folder[idx], file_names[idx], colum_idx[idx], interpolation=interpolation_option[idx])\n",
    "\n",
    "    df_all, train_mean, train_std, df = normalize(df)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    if i == 0:\n",
    "        dfff = df\n",
    "        target_all = df_all\n",
    "        target_std = train_std\n",
    "        target_mean = train_mean\n",
    "        start_year = str(times.iloc[0].year)\n",
    "        end_year = str(times.iloc[-1].year)\n",
    "\n",
    "    if interpolation_option[idx][0] == False:\n",
    "\n",
    "        loadfiles = ['idx.npy', 'miss.npy', 'discriminator.h5', 'generator.h5']\n",
    "\n",
    "        gain_calc_falg = True\n",
    "\n",
    "        if __GAIN_TRAINING__ == True:\n",
    "            gain_calc_falg = MissData.save(pd.concat(df, axis=0).to_numpy(), max_tseq=24, save_dir='save/')\n",
    "            #print(folder[idx], ': training ', 'Miss date save : ', gain_calc_falg)\n",
    "        else:\n",
    "            for file in loadfiles:\n",
    "                if os.path.isfile('save/' + folder[idx]+file):\n",
    "                    shutil.copyfile('save/' + folder[idx]+file, 'save/'+file)\n",
    "                    #print('load file name : save/' + folder[idx]+file)\n",
    "                else:\n",
    "                    if file == 'miss.npy':\n",
    "                        gain_calc_falg = MissData.save(pd.concat(df, axis=0).to_numpy(), max_tseq=24, save_dir='save/')\n",
    "                        #print(folder[idx], ': is not miss.npy ', 'Miss date save : ', gain_calc_falg)\n",
    "\n",
    "        if gain_calc_falg == True:\n",
    "            #print('GainWindowGenerator in main')\n",
    "            WindowGenerator.make_dataset = make_dataset_gain\n",
    "            wide_window = WindowGenerator(input_width=gain_in_setps, label_width=gain_out_setps, shift=gain_shift,\n",
    "                                          fill_no=gain_fill_no, miss_rate=gain_miss_rate, batch_size=gain_batch_size,\n",
    "                                          train_df = df_all, val_df = df_all, test_df = df_all, df = df)\n",
    "\n",
    "            #gain = model_GAIN(shape=wide_window.dg.shape[1:], gen_sigmoid=False, epochs=gain_epochs, training_flag=__GAIN_TRAINING__, window=wide_window, model_save_path='save/')\n",
    "            gain = model_GAIN(shape=(gain_in_setps, df_all.shape[1]), gen_sigmoid=False, epochs=gain_epochs,\n",
    "                              training_flag=__GAIN_TRAINING__, window=wide_window, model_save_path='save/')\n",
    "\n",
    "            gain_val_performance[str(i)] = gain.evaluate(wide_window.val)\n",
    "            gain_performance[str(i)] = gain.evaluate(wide_window.test, verbose=0)\n",
    "\n",
    "            #print('file proc in main')\n",
    "            if __GAIN_TRAINING__ == True:\n",
    "                #dir = 'save/'+folder[i]\n",
    "                if not os.path.exists('save/' + folder[idx]):\n",
    "                    os.makedirs('save/'+folder[idx])\n",
    "                for file in loadfiles:\n",
    "                    shutil.copyfile('save/' + file, 'save/' + folder[idx] + file)\n",
    "\n",
    "            #print('create_dataset_with_gain in main')\n",
    "            #ori, gan = create_dataset_with_gain(gain=gain, window=wide_window, df=df)\n",
    "            ori, gan = create_dataset_with_gain(gain=gain, shape=(gain_in_setps, df_all.shape[1]), df=df)\n",
    "\n",
    "        else:\n",
    "            gan = create_dataset_interpol(window=gain_in_setps, df=df)\n",
    "    else:\n",
    "        gan = create_dataset_interpol(window=gain_in_setps, df=df)\n",
    "\n",
    "    if i == 0 :\n",
    "#        if i < length -1:\n",
    "#            gan = gan[:,:-4]  #맨마지막전까지 사인코사인삭제\n",
    "#            print(gan.shape)\n",
    "        real_df_all = pd.DataFrame(gan)\n",
    "    else:\n",
    "#        if i < length -1:\n",
    "#            gan = gan[:,:-4]  #맨마지막전까지 사인코사인삭제\n",
    "#            print(gan.shape)\n",
    "        real_df_all = pd.concat([real_df_all, pd.DataFrame(gan)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52608, 48)\n"
     ]
    }
   ],
   "source": [
    "print(real_df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_slice(df, train_ratio, val_ratio, test_ratio):\n",
    "    total_no = df.shape[0]\n",
    "\n",
    "    train_no = int(total_no * train_ratio)\n",
    "    #\n",
    "    val_no = int(total_no * (train_ratio + val_ratio))\n",
    "\n",
    "    # val_no = int(total_no*val_ratio)\n",
    "    # train_no =int(total_no * (train_ratio+val_ratio))\n",
    "    #\n",
    "    # val_slice = slice(0, val_no)\n",
    "    # train_slice = slice(val_no, train_no)\n",
    "    # test_slice = slice(train_no, None)\n",
    "\n",
    "    train_slice = slice(0, train_no)    #0.8\n",
    "    val_slice = slice(train_no, val_no)   #0.1\n",
    "    test_slice = slice(val_no, None)\n",
    "    # val_slice = slice(train_no, val_no)   #0.1\n",
    "    # test_slice = slice(val_no, None)\n",
    "#     train_slice = slice(0, train_no)  # 0.8\n",
    "#     val_slice = slice(train_no, None)  # 0.1\n",
    "#     test_slice = slice(val_no, None)\n",
    "\n",
    "    train = pd.DataFrame(df[train_slice])\n",
    "    val = pd.DataFrame(df[val_slice])\n",
    "    test = pd.DataFrame(df[test_slice])\n",
    "\n",
    "    test_slice2 = slice(total_no - 288, None)\n",
    "    test2 = pd.DataFrame(df[test_slice2])\n",
    "\n",
    "    #print('total_no :1111111111111111111111111 ')\n",
    "    #print('total_no : ', total_no)\n",
    "    #print('train : ', train.shape)\n",
    "    #print('val : ', val.shape)\n",
    "    #print('test : ', test.shape)\n",
    "\n",
    "\n",
    "    return train, val, test, test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# slice DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df=real_df_all.iloc[:-2000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, val_df, train_df, test_df2 = dataset_slice(fake_df, 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5061, 48), (40486, 48), (5061, 48))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape, test_df.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------prediction\n",
      "-------------------prediction\n",
      "-------------------prediction\n",
      "real_df_all.type :  <class 'pandas.core.frame.DataFrame'>\n",
      "train_df.type :  <class 'pandas.core.frame.DataFrame'>\n",
      "train_df.shape :  (5061, 48) val_df.shape :  (5061, 48) test_df.shape: (40486, 48)\n"
     ]
    }
   ],
   "source": [
    "print('-------------------prediction')\n",
    "print('-------------------prediction')\n",
    "print('-------------------prediction')\n",
    "\n",
    "print('real_df_all.type : ', type(real_df_all))\n",
    "print('train_df.type : ', type(train_df))\n",
    "print('train_df.shape : ', train_df.shape, 'val_df.shape : ', val_df.shape, 'test_df.shape:' ,test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_columns_indices:\n",
      "{'tmpr_value': 0, 'ph_value': 1, 'do_value': 2, 'ec_value': 3, 'toc_value': 4, '총질소_값': 5, '총인_값': 6, '클로로필-a_값': 7, 'Day sin': 8, 'Day cos': 9, 'Year sin': 10, 'Year cos': 11}\n",
      "target columns :  tn\n",
      "target_col_idx :  5\n",
      "out_num_features :  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_columns_indices = {name: i for i, name in enumerate(dfff[0])}\n",
    "\n",
    "print(\"label_columns_indices:\")\n",
    "print(label_columns_indices)\n",
    "\n",
    "\n",
    "target_dic = {\"do\":\"do_value\", \"toc\":\"toc_value\", \"tn\":\"총질소_값\", \"tp\":\"총인_값\", \"chl-a\":\"클로로필-a_값\"}\n",
    "\n",
    "rnn_target_column = \"tn\"\n",
    "\n",
    "print('target columns : ', rnn_target_column)\n",
    "num_features = dfff[0].shape[1]\n",
    "\n",
    "\n",
    "\n",
    "target_col_idx = label_columns_indices[target_dic[rnn_target_column]]\n",
    "out_features = [target_col_idx]\n",
    "out_num_features = len(out_features)\n",
    "\n",
    "print(\"target_col_idx : \", target_col_idx)\n",
    "print('out_num_features : ', out_num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model path :  save/yeong/models/nitrogen/\n"
     ]
    }
   ],
   "source": [
    "val_nse = {}\n",
    "val_pbias = {}\n",
    "\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset_water\n",
    "multi_window = WindowGenerator(\n",
    "    input_width=rnn_in_setps,label_width=rnn_out_steps, shift=rnn_out_steps,out_features=out_features,\n",
    "    out_num_features=out_num_features,label_columns=dfff[0].columns, batch_size=rnn_batch_size,\n",
    "    train_df=train_df, val_df=val_df, test_df=test_df, test_df2=test_df2)\n",
    "\n",
    "if __RNN_TRAINING__:\n",
    "    if not os.path.exists('save/' + watershed):\n",
    "        os.makedirs('save/' + watershed)\n",
    "\n",
    "\n",
    "idx = [2, 4, 5, 6, 7]\n",
    "pa = [\"do/\", \"toc/\", \"nitrogen/\", \"phosphorus/\", \"chlorophyll-a/\"]\n",
    "\n",
    "indices = {name: i for i, name in enumerate(idx)}\n",
    "\n",
    "model_path = \"save/\" + watershed + \"models/\" + pa[indices[target_col_idx]]\n",
    "print(\"save model path : \", model_path)\n",
    "\n",
    "val_nse = {}\n",
    "val_pbias = {}\n",
    "\n",
    " # +\"gru.ckpt\" -- path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f4333ed0b6470195bbcd198246476a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(real_df_all.iloc[:,out_features[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc00b162b5ba4566b3a18e4bd8282eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(test_df.iloc[:,out_features[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "__RNN_TRAINING__ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_16 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3339 - mean_absolute_error: 0.4620 - nse: -1.3971e-04 - val_loss: 1.0129 - val_mean_absolute_error: 0.7924 - val_nse: -0.4861\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.01293, saving model to save/yeong/models/nitrogen/gru.ckpt\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 1s 529ms/step - loss: 0.2918 - mean_absolute_error: 0.4423 - nse: 0.0402 - val_loss: 0.9640 - val_mean_absolute_error: 0.7598 - val_nse: -0.4365\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.01293 to 0.96398, saving model to save/yeong/models/nitrogen/gru.ckpt\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 1s 516ms/step - loss: 0.2753 - mean_absolute_error: 0.4239 - nse: 0.1723 - val_loss: 0.7177 - val_mean_absolute_error: 0.6433 - val_nse: -0.1652\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.96398 to 0.71766, saving model to save/yeong/models/nitrogen/gru.ckpt\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 0.2231 - mean_absolute_error: 0.3704 - nse: 0.3246 - val_loss: 0.5700 - val_mean_absolute_error: 0.5732 - val_nse: 0.0237\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.71766 to 0.56997, saving model to save/yeong/models/nitrogen/gru.ckpt\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 1s 601ms/step - loss: 0.1367 - mean_absolute_error: 0.2733 - nse: 0.5326 - val_loss: 0.6390 - val_mean_absolute_error: 0.6314 - val_nse: -0.0474\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.56997\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 0.1358 - mean_absolute_error: 0.2751 - nse: 0.6105 - val_loss: 0.6155 - val_mean_absolute_error: 0.6576 - val_nse: 0.2056\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.56997\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 0.1386 - mean_absolute_error: 0.2961 - nse: 0.5624 - val_loss: 0.5593 - val_mean_absolute_error: 0.6121 - val_nse: 0.1565\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.56997 to 0.55927, saving model to save/yeong/models/nitrogen/gru.ckpt\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 1s 579ms/step - loss: 0.1694 - mean_absolute_error: 0.3203 - nse: 0.3918 - val_loss: 0.6165 - val_mean_absolute_error: 0.6294 - val_nse: -0.0709\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.55927\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 0.1181 - mean_absolute_error: 0.2590 - nse: 0.5603 - val_loss: 0.5456 - val_mean_absolute_error: 0.5655 - val_nse: 0.1669\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.55927 to 0.54563, saving model to save/yeong/models/nitrogen/gru.ckpt\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 1s 535ms/step - loss: 0.0940 - mean_absolute_error: 0.2282 - nse: 0.6810 - val_loss: 0.7526 - val_mean_absolute_error: 0.6753 - val_nse: -0.2214\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.54563\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 1s 557ms/step - loss: 0.1043 - mean_absolute_error: 0.2358 - nse: 0.6832 - val_loss: 0.7525 - val_mean_absolute_error: 0.6741 - val_nse: -0.0727\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.54563\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 0.1354 - mean_absolute_error: 0.2634 - nse: 0.6301 - val_loss: 0.6254 - val_mean_absolute_error: 0.5953 - val_nse: -0.0249\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.54563\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.1137 - mean_absolute_error: 0.2496 - nse: 0.6008 - val_loss: 0.7162 - val_mean_absolute_error: 0.6512 - val_nse: -0.0655\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.54563\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 1s 596ms/step - loss: 0.1078 - mean_absolute_error: 0.2415 - nse: 0.6468 - val_loss: 0.9179 - val_mean_absolute_error: 0.7544 - val_nse: -0.1938\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.54563\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 0.1177 - mean_absolute_error: 0.2444 - nse: 0.6181 - val_loss: 0.6703 - val_mean_absolute_error: 0.6443 - val_nse: -0.0179\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.54563\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 0.0924 - mean_absolute_error: 0.2099 - nse: 0.6761 - val_loss: 0.6887 - val_mean_absolute_error: 0.6499 - val_nse: 0.1063\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.54563\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 1s 582ms/step - loss: 0.0722 - mean_absolute_error: 0.1997 - nse: 0.7067 - val_loss: 0.6135 - val_mean_absolute_error: 0.6069 - val_nse: 0.0509\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.54563\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 1s 617ms/step - loss: 0.0864 - mean_absolute_error: 0.2109 - nse: 0.7037 - val_loss: 0.5122 - val_mean_absolute_error: 0.5575 - val_nse: 0.2556\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.54563 to 0.51215, saving model to save/yeong/models/nitrogen/gru.ckpt\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 0.0966 - mean_absolute_error: 0.2328 - nse: 0.7055 - val_loss: 0.4349 - val_mean_absolute_error: 0.5252 - val_nse: 0.1868\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.51215 to 0.43486, saving model to save/yeong/models/nitrogen/gru.ckpt\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 1s 547ms/step - loss: 0.0999 - mean_absolute_error: 0.2376 - nse: 0.7212 - val_loss: 0.4694 - val_mean_absolute_error: 0.5236 - val_nse: 0.2632\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.43486\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 1s 594ms/step - loss: 0.0877 - mean_absolute_error: 0.2240 - nse: 0.7410 - val_loss: 0.4214 - val_mean_absolute_error: 0.5162 - val_nse: 0.2193\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.43486 to 0.42137, saving model to save/yeong/models/nitrogen/gru.ckpt\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.0916 - mean_absolute_error: 0.2297 - nse: 0.6081 - val_loss: 0.6831 - val_mean_absolute_error: 0.6367 - val_nse: 0.0966\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.42137\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 1s 557ms/step - loss: 0.0646 - mean_absolute_error: 0.1914 - nse: 0.7176 - val_loss: 0.6902 - val_mean_absolute_error: 0.6628 - val_nse: -0.0966\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.42137\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.0770 - mean_absolute_error: 0.2029 - nse: 0.7592 - val_loss: 0.7495 - val_mean_absolute_error: 0.6652 - val_nse: 0.0079\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.42137\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.0759 - mean_absolute_error: 0.2100 - nse: 0.7316 - val_loss: 0.5481 - val_mean_absolute_error: 0.5618 - val_nse: 0.0821\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.42137\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 1s 516ms/step - loss: 0.0804 - mean_absolute_error: 0.2054 - nse: 0.7378 - val_loss: 0.5731 - val_mean_absolute_error: 0.5970 - val_nse: 0.1584\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.42137\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 0.0868 - mean_absolute_error: 0.2106 - nse: 0.7433 - val_loss: 0.6088 - val_mean_absolute_error: 0.5890 - val_nse: 0.2068\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.42137\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.0647 - mean_absolute_error: 0.1849 - nse: 0.7039 - val_loss: 0.5365 - val_mean_absolute_error: 0.5491 - val_nse: 0.2477\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.42137\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 0.0833 - mean_absolute_error: 0.2018 - nse: 0.7356 - val_loss: 0.5626 - val_mean_absolute_error: 0.5924 - val_nse: 0.1544\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.42137\n",
      "Epoch 30/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 567ms/step - loss: 0.0863 - mean_absolute_error: 0.2122 - nse: 0.7264 - val_loss: 0.5662 - val_mean_absolute_error: 0.5940 - val_nse: 0.1587\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.42137\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 1s 562ms/step - loss: 0.0637 - mean_absolute_error: 0.1839 - nse: 0.8107 - val_loss: 0.4343 - val_mean_absolute_error: 0.5080 - val_nse: 0.3372\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.42137\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 1s 545ms/step - loss: 0.0692 - mean_absolute_error: 0.1944 - nse: 0.7567 - val_loss: 0.4273 - val_mean_absolute_error: 0.5044 - val_nse: 0.3505\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.42137\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 0.0635 - mean_absolute_error: 0.1869 - nse: 0.8051 - val_loss: 0.5221 - val_mean_absolute_error: 0.5844 - val_nse: 0.2297\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.42137\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 0.0718 - mean_absolute_error: 0.2049 - nse: 0.8092 - val_loss: 0.3795 - val_mean_absolute_error: 0.4813 - val_nse: 0.3513\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.42137 to 0.37953, saving model to save/yeong/models/nitrogen/gru.ckpt\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 1s 569ms/step - loss: 0.0680 - mean_absolute_error: 0.1984 - nse: 0.7553 - val_loss: 0.4942 - val_mean_absolute_error: 0.5726 - val_nse: 0.2535\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.37953\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.0694 - mean_absolute_error: 0.1913 - nse: 0.7356 - val_loss: 0.4219 - val_mean_absolute_error: 0.5079 - val_nse: 0.3723\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.37953\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.0718 - mean_absolute_error: 0.1976 - nse: 0.7556 - val_loss: 0.4254 - val_mean_absolute_error: 0.5133 - val_nse: 0.3567\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.37953\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.0691 - mean_absolute_error: 0.1945 - nse: 0.7989 - val_loss: 0.5367 - val_mean_absolute_error: 0.5846 - val_nse: 0.2230\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.37953\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 0.0628 - mean_absolute_error: 0.1844 - nse: 0.7842 - val_loss: 0.3868 - val_mean_absolute_error: 0.4873 - val_nse: 0.3923\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.37953\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 0.0746 - mean_absolute_error: 0.2008 - nse: 0.7872 - val_loss: 0.4358 - val_mean_absolute_error: 0.5258 - val_nse: 0.3479\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.37953\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 0.0660 - mean_absolute_error: 0.1881 - nse: 0.7827 - val_loss: 0.3372 - val_mean_absolute_error: 0.4657 - val_nse: 0.3647\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.37953 to 0.33723, saving model to save/yeong/models/nitrogen/gru.ckpt\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 1s 556ms/step - loss: 0.0642 - mean_absolute_error: 0.1872 - nse: 0.7991 - val_loss: 0.3727 - val_mean_absolute_error: 0.4853 - val_nse: 0.4055\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.33723\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.0655 - mean_absolute_error: 0.1902 - nse: 0.8136 - val_loss: 0.5148 - val_mean_absolute_error: 0.6010 - val_nse: 0.3079\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.33723\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.0691 - mean_absolute_error: 0.1958 - nse: 0.8021 - val_loss: 0.4169 - val_mean_absolute_error: 0.5242 - val_nse: 0.3653\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.33723\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.0587 - mean_absolute_error: 0.1824 - nse: 0.7685 - val_loss: 0.4260 - val_mean_absolute_error: 0.5383 - val_nse: 0.2788\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.33723\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 1s 529ms/step - loss: 0.0573 - mean_absolute_error: 0.1797 - nse: 0.7838 - val_loss: 0.3471 - val_mean_absolute_error: 0.4569 - val_nse: 0.4370\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.33723\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 1s 542ms/step - loss: 0.0601 - mean_absolute_error: 0.1827 - nse: 0.7816 - val_loss: 0.3980 - val_mean_absolute_error: 0.5097 - val_nse: 0.4027\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.33723\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.0610 - mean_absolute_error: 0.1829 - nse: 0.8164 - val_loss: 0.3958 - val_mean_absolute_error: 0.5069 - val_nse: 0.4305\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.33723\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 1s 605ms/step - loss: 0.0603 - mean_absolute_error: 0.1785 - nse: 0.8336 - val_loss: 0.4072 - val_mean_absolute_error: 0.5170 - val_nse: 0.3156\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.33723\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 0.0731 - mean_absolute_error: 0.1980 - nse: 0.7699 - val_loss: 0.3770 - val_mean_absolute_error: 0.4792 - val_nse: 0.4471\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.33723\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 1s 592ms/step - loss: 0.0564 - mean_absolute_error: 0.1789 - nse: 0.7936 - val_loss: 0.4459 - val_mean_absolute_error: 0.5479 - val_nse: 0.4013\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.33723\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 1s 617ms/step - loss: 0.0607 - mean_absolute_error: 0.1829 - nse: 0.8182 - val_loss: 0.3411 - val_mean_absolute_error: 0.4579 - val_nse: 0.4806\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.33723\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 1s 575ms/step - loss: 0.0596 - mean_absolute_error: 0.1738 - nse: 0.7927 - val_loss: 0.3674 - val_mean_absolute_error: 0.4824 - val_nse: 0.5254\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.33723\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 1s 524ms/step - loss: 0.0567 - mean_absolute_error: 0.1723 - nse: 0.7659 - val_loss: 0.3112 - val_mean_absolute_error: 0.4370 - val_nse: 0.5215\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.33723 to 0.31124, saving model to save/yeong/models/nitrogen/gru.ckpt\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 1s 568ms/step - loss: 0.0579 - mean_absolute_error: 0.1747 - nse: 0.7967 - val_loss: 0.4247 - val_mean_absolute_error: 0.5320 - val_nse: 0.3416\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.31124\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 0.0707 - mean_absolute_error: 0.1999 - nse: 0.7695 - val_loss: 0.3483 - val_mean_absolute_error: 0.4729 - val_nse: 0.4480\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.31124\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 0.0592 - mean_absolute_error: 0.1865 - nse: 0.8271 - val_loss: 0.3812 - val_mean_absolute_error: 0.4781 - val_nse: 0.3933\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.31124\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 0.0649 - mean_absolute_error: 0.1899 - nse: 0.8136 - val_loss: 0.3492 - val_mean_absolute_error: 0.4724 - val_nse: 0.3929\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.31124\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 1s 547ms/step - loss: 0.0541 - mean_absolute_error: 0.1739 - nse: 0.7986 - val_loss: 0.4086 - val_mean_absolute_error: 0.5033 - val_nse: 0.4139\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.31124\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 1s 538ms/step - loss: 0.0551 - mean_absolute_error: 0.1717 - nse: 0.7346 - val_loss: 0.4508 - val_mean_absolute_error: 0.5505 - val_nse: 0.2907\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.31124\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.0551 - mean_absolute_error: 0.1736 - nse: 0.8312 - val_loss: 0.4298 - val_mean_absolute_error: 0.5341 - val_nse: 0.4084\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.31124\n",
      "Epoch 62/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 531ms/step - loss: 0.0507 - mean_absolute_error: 0.1699 - nse: 0.8175 - val_loss: 0.3819 - val_mean_absolute_error: 0.4959 - val_nse: 0.4271\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.31124\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 0.0543 - mean_absolute_error: 0.1715 - nse: 0.8274 - val_loss: 0.3838 - val_mean_absolute_error: 0.5042 - val_nse: 0.3468\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.31124\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 1s 568ms/step - loss: 0.0532 - mean_absolute_error: 0.1705 - nse: 0.8195 - val_loss: 0.3581 - val_mean_absolute_error: 0.4628 - val_nse: 0.5510\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.31124\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.0522 - mean_absolute_error: 0.1682 - nse: 0.7986 - val_loss: 0.3141 - val_mean_absolute_error: 0.4393 - val_nse: 0.5225\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.31124\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 1s 522ms/step - loss: 0.0496 - mean_absolute_error: 0.1701 - nse: 0.8376 - val_loss: 0.4011 - val_mean_absolute_error: 0.5107 - val_nse: 0.4270\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.31124\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 0.0625 - mean_absolute_error: 0.1843 - nse: 0.8013 - val_loss: 0.4412 - val_mean_absolute_error: 0.5419 - val_nse: 0.3515\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.31124\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 0.0549 - mean_absolute_error: 0.1725 - nse: 0.8292 - val_loss: 0.3459 - val_mean_absolute_error: 0.4642 - val_nse: 0.4599\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.31124\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.0606 - mean_absolute_error: 0.1829 - nse: 0.7974 - val_loss: 0.3300 - val_mean_absolute_error: 0.4390 - val_nse: 0.5089\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.31124\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 0.0512 - mean_absolute_error: 0.1669 - nse: 0.8322 - val_loss: 0.3998 - val_mean_absolute_error: 0.4988 - val_nse: 0.4007\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.31124\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.0462 - mean_absolute_error: 0.1624 - nse: 0.8682 - val_loss: 0.3705 - val_mean_absolute_error: 0.4730 - val_nse: 0.4471\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.31124\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 0.0533 - mean_absolute_error: 0.1735 - nse: 0.8456 - val_loss: 0.4020 - val_mean_absolute_error: 0.4977 - val_nse: 0.3749\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.31124\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 1s 542ms/step - loss: 0.0572 - mean_absolute_error: 0.1747 - nse: 0.8015 - val_loss: 0.3791 - val_mean_absolute_error: 0.4887 - val_nse: 0.3929\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.31124\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 0.0484 - mean_absolute_error: 0.1592 - nse: 0.7993 - val_loss: 0.3472 - val_mean_absolute_error: 0.4687 - val_nse: 0.4729\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.31124\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 1s 512ms/step - loss: 0.0533 - mean_absolute_error: 0.1724 - nse: 0.8504 - val_loss: 0.4807 - val_mean_absolute_error: 0.5511 - val_nse: 0.3317\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.31124\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 0.0412 - mean_absolute_error: 0.1573 - nse: 0.8491 - val_loss: 0.3685 - val_mean_absolute_error: 0.4883 - val_nse: 0.4116\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.31124\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 1s 518ms/step - loss: 0.0523 - mean_absolute_error: 0.1699 - nse: 0.8500 - val_loss: 0.4298 - val_mean_absolute_error: 0.5187 - val_nse: 0.3833\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.31124\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 0.0494 - mean_absolute_error: 0.1628 - nse: 0.8469 - val_loss: 0.3735 - val_mean_absolute_error: 0.4763 - val_nse: 0.3129\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.31124\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 0.0512 - mean_absolute_error: 0.1685 - nse: 0.8467 - val_loss: 0.3309 - val_mean_absolute_error: 0.4487 - val_nse: 0.4565\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.31124\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 0.0424 - mean_absolute_error: 0.1580 - nse: 0.8635 - val_loss: 0.4857 - val_mean_absolute_error: 0.5576 - val_nse: 0.2800\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.31124\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 1s 586ms/step - loss: 0.0549 - mean_absolute_error: 0.1772 - nse: 0.8435 - val_loss: 0.5001 - val_mean_absolute_error: 0.5614 - val_nse: 0.2761\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.31124\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 1s 580ms/step - loss: 0.0455 - mean_absolute_error: 0.1626 - nse: 0.8489 - val_loss: 0.4673 - val_mean_absolute_error: 0.5507 - val_nse: 0.3290\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.31124\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 1s 588ms/step - loss: 0.0407 - mean_absolute_error: 0.1526 - nse: 0.8355 - val_loss: 0.4555 - val_mean_absolute_error: 0.5294 - val_nse: 0.1548\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.31124\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 1s 578ms/step - loss: 0.0485 - mean_absolute_error: 0.1653 - nse: 0.8257 - val_loss: 0.4395 - val_mean_absolute_error: 0.5085 - val_nse: 0.3801\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.31124\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 1s 555ms/step - loss: 0.0497 - mean_absolute_error: 0.1678 - nse: 0.8429 - val_loss: 0.4244 - val_mean_absolute_error: 0.5213 - val_nse: 0.3059\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.31124\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 1s 547ms/step - loss: 0.0447 - mean_absolute_error: 0.1587 - nse: 0.8674 - val_loss: 0.4568 - val_mean_absolute_error: 0.5260 - val_nse: 0.3265\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.31124\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.0444 - mean_absolute_error: 0.1569 - nse: 0.8623 - val_loss: 0.3834 - val_mean_absolute_error: 0.4778 - val_nse: 0.3671\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.31124\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 0.0473 - mean_absolute_error: 0.1650 - nse: 0.8410 - val_loss: 0.5048 - val_mean_absolute_error: 0.5613 - val_nse: 0.3005\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.31124\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 1s 530ms/step - loss: 0.0414 - mean_absolute_error: 0.1513 - nse: 0.8696 - val_loss: 0.4262 - val_mean_absolute_error: 0.5089 - val_nse: 0.3996\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.31124\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.0461 - mean_absolute_error: 0.1613 - nse: 0.8456 - val_loss: 0.4300 - val_mean_absolute_error: 0.5136 - val_nse: 0.4277\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.31124\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 1s 512ms/step - loss: 0.0457 - mean_absolute_error: 0.1495 - nse: 0.8247 - val_loss: 0.4396 - val_mean_absolute_error: 0.5222 - val_nse: 0.3237\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.31124\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 1s 555ms/step - loss: 0.0390 - mean_absolute_error: 0.1476 - nse: 0.8457 - val_loss: 0.5723 - val_mean_absolute_error: 0.5963 - val_nse: 0.1729\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.31124\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 0.0505 - mean_absolute_error: 0.1670 - nse: 0.8488 - val_loss: 0.4399 - val_mean_absolute_error: 0.5195 - val_nse: 0.3514\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.31124\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 1s 558ms/step - loss: 0.0428 - mean_absolute_error: 0.1556 - nse: 0.8628 - val_loss: 0.3985 - val_mean_absolute_error: 0.5019 - val_nse: 0.2688\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.31124\n",
      "Epoch 95/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 528ms/step - loss: 0.0437 - mean_absolute_error: 0.1557 - nse: 0.8743 - val_loss: 0.4779 - val_mean_absolute_error: 0.5357 - val_nse: 0.2565\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.31124\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.0475 - mean_absolute_error: 0.1619 - nse: 0.8510 - val_loss: 0.4124 - val_mean_absolute_error: 0.4917 - val_nse: 0.3217\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.31124\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 1s 506ms/step - loss: 0.0450 - mean_absolute_error: 0.1528 - nse: 0.7914 - val_loss: 0.5285 - val_mean_absolute_error: 0.5807 - val_nse: 0.2239\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.31124\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.0421 - mean_absolute_error: 0.1533 - nse: 0.8335 - val_loss: 0.4879 - val_mean_absolute_error: 0.5594 - val_nse: 0.3128\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.31124\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 0.0408 - mean_absolute_error: 0.1538 - nse: 0.8845 - val_loss: 0.4882 - val_mean_absolute_error: 0.5502 - val_nse: 0.3361\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.31124\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 0.0443 - mean_absolute_error: 0.1588 - nse: 0.8436 - val_loss: 0.4321 - val_mean_absolute_error: 0.5274 - val_nse: 0.1698\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.31124\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.0443 - mean_absolute_error: 0.1558 - nse: 0.8490 - val_loss: 0.4250 - val_mean_absolute_error: 0.5018 - val_nse: 0.4999\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.31124\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 1s 536ms/step - loss: 0.0467 - mean_absolute_error: 0.1605 - nse: 0.8488 - val_loss: 0.3311 - val_mean_absolute_error: 0.4486 - val_nse: 0.4964\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.31124\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 0.0417 - mean_absolute_error: 0.1528 - nse: 0.8388 - val_loss: 0.4547 - val_mean_absolute_error: 0.5236 - val_nse: 0.2981\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.31124\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 0.0466 - mean_absolute_error: 0.1593 - nse: 0.8482 - val_loss: 0.4930 - val_mean_absolute_error: 0.5739 - val_nse: 0.2777\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.31124\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 0.0502 - mean_absolute_error: 0.1646 - nse: 0.8445 - val_loss: 0.4573 - val_mean_absolute_error: 0.5175 - val_nse: 0.3229\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.31124\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.0356 - mean_absolute_error: 0.1452 - nse: 0.8862 - val_loss: 0.3572 - val_mean_absolute_error: 0.4634 - val_nse: 0.4141\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.31124\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.0417 - mean_absolute_error: 0.1518 - nse: 0.8486 - val_loss: 0.4292 - val_mean_absolute_error: 0.4964 - val_nse: 0.3935\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.31124\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 0.0395 - mean_absolute_error: 0.1482 - nse: 0.8927 - val_loss: 0.5376 - val_mean_absolute_error: 0.5761 - val_nse: 0.1683\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.31124\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.0453 - mean_absolute_error: 0.1603 - nse: 0.8756 - val_loss: 0.4230 - val_mean_absolute_error: 0.5138 - val_nse: 0.3437\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.31124\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 1s 582ms/step - loss: 0.0468 - mean_absolute_error: 0.1576 - nse: 0.8191 - val_loss: 0.4491 - val_mean_absolute_error: 0.5160 - val_nse: 0.2591\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.31124\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 1s 590ms/step - loss: 0.0434 - mean_absolute_error: 0.1544 - nse: 0.8303 - val_loss: 0.4399 - val_mean_absolute_error: 0.5131 - val_nse: 0.3886\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.31124\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 1s 542ms/step - loss: 0.0455 - mean_absolute_error: 0.1584 - nse: 0.8615 - val_loss: 0.4895 - val_mean_absolute_error: 0.5522 - val_nse: 0.2040\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.31124\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 1s 542ms/step - loss: 0.0371 - mean_absolute_error: 0.1466 - nse: 0.8848 - val_loss: 0.4370 - val_mean_absolute_error: 0.5171 - val_nse: 0.3546\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.31124\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 1s 567ms/step - loss: 0.0412 - mean_absolute_error: 0.1513 - nse: 0.8707 - val_loss: 0.5146 - val_mean_absolute_error: 0.5568 - val_nse: 0.2331\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.31124\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 1s 587ms/step - loss: 0.0418 - mean_absolute_error: 0.1527 - nse: 0.8726 - val_loss: 0.4091 - val_mean_absolute_error: 0.4826 - val_nse: 0.3562\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.31124\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.0436 - mean_absolute_error: 0.1532 - nse: 0.8735 - val_loss: 0.4094 - val_mean_absolute_error: 0.4911 - val_nse: 0.3123\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.31124\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 1s 569ms/step - loss: 0.0404 - mean_absolute_error: 0.1524 - nse: 0.8656 - val_loss: 0.5725 - val_mean_absolute_error: 0.5845 - val_nse: 0.0826\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.31124\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 0.0435 - mean_absolute_error: 0.1583 - nse: 0.8721 - val_loss: 0.5552 - val_mean_absolute_error: 0.5736 - val_nse: 0.2583\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.31124\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 1s 570ms/step - loss: 0.0385 - mean_absolute_error: 0.1487 - nse: 0.8738 - val_loss: 0.5123 - val_mean_absolute_error: 0.5588 - val_nse: 0.2660\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.31124\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.0408 - mean_absolute_error: 0.1472 - nse: 0.8615 - val_loss: 0.4973 - val_mean_absolute_error: 0.5523 - val_nse: 0.0856\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.31124\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 1s 524ms/step - loss: 0.0395 - mean_absolute_error: 0.1504 - nse: 0.8363 - val_loss: 0.3500 - val_mean_absolute_error: 0.4522 - val_nse: 0.4485\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.31124\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 1s 538ms/step - loss: 0.0453 - mean_absolute_error: 0.1593 - nse: 0.8502 - val_loss: 0.4613 - val_mean_absolute_error: 0.5397 - val_nse: 0.2906\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.31124\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 0.0417 - mean_absolute_error: 0.1528 - nse: 0.8702 - val_loss: 0.4579 - val_mean_absolute_error: 0.5192 - val_nse: 0.3319\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.31124\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 1s 558ms/step - loss: 0.0388 - mean_absolute_error: 0.1472 - nse: 0.8922 - val_loss: 0.3987 - val_mean_absolute_error: 0.4895 - val_nse: 0.3285\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.31124\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 0.0400 - mean_absolute_error: 0.1500 - nse: 0.8648 - val_loss: 0.5431 - val_mean_absolute_error: 0.5735 - val_nse: 0.2107\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.31124\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.0409 - mean_absolute_error: 0.1517 - nse: 0.8664 - val_loss: 0.5925 - val_mean_absolute_error: 0.5967 - val_nse: 0.2192\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.31124\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.0376 - mean_absolute_error: 0.1486 - nse: 0.8794 - val_loss: 0.4690 - val_mean_absolute_error: 0.5395 - val_nse: 0.2835\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.31124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/200\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 0.0434 - mean_absolute_error: 0.1501 - nse: 0.8504 - val_loss: 0.4603 - val_mean_absolute_error: 0.5316 - val_nse: 0.4159\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.31124\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.0385 - mean_absolute_error: 0.1460 - nse: 0.8497 - val_loss: 0.6276 - val_mean_absolute_error: 0.6286 - val_nse: 0.0209\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.31124\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 1s 576ms/step - loss: 0.0423 - mean_absolute_error: 0.1514 - nse: 0.8597 - val_loss: 0.4980 - val_mean_absolute_error: 0.5492 - val_nse: 0.2778\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.31124\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 0.0382 - mean_absolute_error: 0.1444 - nse: 0.8760 - val_loss: 0.4694 - val_mean_absolute_error: 0.5437 - val_nse: 0.1113\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.31124\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 1s 550ms/step - loss: 0.0379 - mean_absolute_error: 0.1483 - nse: 0.8936 - val_loss: 0.5229 - val_mean_absolute_error: 0.5545 - val_nse: 0.1792\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.31124\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 1s 553ms/step - loss: 0.0434 - mean_absolute_error: 0.1557 - nse: 0.8747 - val_loss: 0.4165 - val_mean_absolute_error: 0.4861 - val_nse: 0.2577\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.31124\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 1s 563ms/step - loss: 0.0382 - mean_absolute_error: 0.1460 - nse: 0.8272 - val_loss: 0.6051 - val_mean_absolute_error: 0.5910 - val_nse: 0.1861\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.31124\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 1s 584ms/step - loss: 0.0409 - mean_absolute_error: 0.1518 - nse: 0.8366 - val_loss: 0.5380 - val_mean_absolute_error: 0.5887 - val_nse: 0.1647\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.31124\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 1s 537ms/step - loss: 0.0377 - mean_absolute_error: 0.1488 - nse: 0.8815 - val_loss: 0.6509 - val_mean_absolute_error: 0.6404 - val_nse: 0.1389\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.31124\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.0389 - mean_absolute_error: 0.1477 - nse: 0.8567 - val_loss: 0.4209 - val_mean_absolute_error: 0.5150 - val_nse: 0.2467\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.31124\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 0.0395 - mean_absolute_error: 0.1462 - nse: 0.8683 - val_loss: 0.4412 - val_mean_absolute_error: 0.5336 - val_nse: 0.4001\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.31124\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.0439 - mean_absolute_error: 0.1564 - nse: 0.8704 - val_loss: 0.4937 - val_mean_absolute_error: 0.5341 - val_nse: 0.3135\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.31124\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.0369 - mean_absolute_error: 0.1429 - nse: 0.8409 - val_loss: 0.4870 - val_mean_absolute_error: 0.5321 - val_nse: 0.3205\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.31124\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.0421 - mean_absolute_error: 0.1503 - nse: 0.8631 - val_loss: 0.5423 - val_mean_absolute_error: 0.5972 - val_nse: 0.1595\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.31124\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.0450 - mean_absolute_error: 0.1585 - nse: 0.8598 - val_loss: 0.6065 - val_mean_absolute_error: 0.6079 - val_nse: 0.1607\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.31124\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 0.0366 - mean_absolute_error: 0.1483 - nse: 0.8910 - val_loss: 0.4097 - val_mean_absolute_error: 0.4935 - val_nse: 0.3163\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.31124\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 0.0377 - mean_absolute_error: 0.1450 - nse: 0.8568 - val_loss: 0.4635 - val_mean_absolute_error: 0.5090 - val_nse: 0.3410\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.31124\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.0369 - mean_absolute_error: 0.1464 - nse: 0.9040 - val_loss: 0.6108 - val_mean_absolute_error: 0.6314 - val_nse: 0.0750\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.31124\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 1s 586ms/step - loss: 0.0361 - mean_absolute_error: 0.1436 - nse: 0.8904 - val_loss: 0.4872 - val_mean_absolute_error: 0.5318 - val_nse: 0.1584\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.31124\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 1s 542ms/step - loss: 0.0390 - mean_absolute_error: 0.1456 - nse: 0.8633 - val_loss: 0.4729 - val_mean_absolute_error: 0.5368 - val_nse: 0.2825\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.31124\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 1s 573ms/step - loss: 0.0410 - mean_absolute_error: 0.1458 - nse: 0.8474 - val_loss: 0.4865 - val_mean_absolute_error: 0.5452 - val_nse: 0.2941\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.31124\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.0381 - mean_absolute_error: 0.1445 - nse: 0.8707 - val_loss: 0.4714 - val_mean_absolute_error: 0.5351 - val_nse: 0.2715\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.31124\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 1s 569ms/step - loss: 0.0363 - mean_absolute_error: 0.1479 - nse: 0.8975 - val_loss: 0.6052 - val_mean_absolute_error: 0.6060 - val_nse: 0.1215\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.31124\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 0.0342 - mean_absolute_error: 0.1388 - nse: 0.8765 - val_loss: 0.4756 - val_mean_absolute_error: 0.5275 - val_nse: 0.2483\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.31124\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 0.0336 - mean_absolute_error: 0.1375 - nse: 0.9028 - val_loss: 0.5350 - val_mean_absolute_error: 0.5640 - val_nse: 0.1982\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.31124\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 1s 530ms/step - loss: 0.0380 - mean_absolute_error: 0.1425 - nse: 0.8756 - val_loss: 0.3976 - val_mean_absolute_error: 0.4946 - val_nse: 0.2770\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.31124\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.0384 - mean_absolute_error: 0.1491 - nse: 0.8771 - val_loss: 0.5833 - val_mean_absolute_error: 0.5873 - val_nse: 0.0561\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.31124\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.0365 - mean_absolute_error: 0.1467 - nse: 0.9001 - val_loss: 0.6709 - val_mean_absolute_error: 0.6478 - val_nse: 0.1351\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.31124\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.0350 - mean_absolute_error: 0.1434 - nse: 0.8943 - val_loss: 0.5661 - val_mean_absolute_error: 0.5884 - val_nse: 0.1357\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.31124\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 0.0390 - mean_absolute_error: 0.1498 - nse: 0.8501 - val_loss: 0.6015 - val_mean_absolute_error: 0.6172 - val_nse: 0.0046\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.31124\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 0.0311 - mean_absolute_error: 0.1339 - nse: 0.8779 - val_loss: 0.4740 - val_mean_absolute_error: 0.5315 - val_nse: 0.1940\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.31124\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 1s 506ms/step - loss: 0.0366 - mean_absolute_error: 0.1448 - nse: 0.8781 - val_loss: 0.5620 - val_mean_absolute_error: 0.5967 - val_nse: 0.1277\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.31124\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 0.0359 - mean_absolute_error: 0.1411 - nse: 0.8906 - val_loss: 0.5385 - val_mean_absolute_error: 0.5741 - val_nse: 0.2278\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.31124\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 547ms/step - loss: 0.0341 - mean_absolute_error: 0.1392 - nse: 0.9042 - val_loss: 0.5279 - val_mean_absolute_error: 0.5766 - val_nse: 0.1497\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.31124\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 0.0384 - mean_absolute_error: 0.1446 - nse: 0.8798 - val_loss: 0.5531 - val_mean_absolute_error: 0.5769 - val_nse: 0.1703\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.31124\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 1s 535ms/step - loss: 0.0323 - mean_absolute_error: 0.1370 - nse: 0.8815 - val_loss: 0.6471 - val_mean_absolute_error: 0.6363 - val_nse: 0.1579\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.31124\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 1s 561ms/step - loss: 0.0391 - mean_absolute_error: 0.1485 - nse: 0.8828 - val_loss: 0.5229 - val_mean_absolute_error: 0.5614 - val_nse: 0.2164\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.31124\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.0387 - mean_absolute_error: 0.1443 - nse: 0.8609 - val_loss: 0.6006 - val_mean_absolute_error: 0.6137 - val_nse: 0.2162\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.31124\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 1s 543ms/step - loss: 0.0352 - mean_absolute_error: 0.1396 - nse: 0.8638 - val_loss: 0.5297 - val_mean_absolute_error: 0.5825 - val_nse: 0.1671\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.31124\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.0354 - mean_absolute_error: 0.1388 - nse: 0.8741 - val_loss: 0.6998 - val_mean_absolute_error: 0.6645 - val_nse: -0.0631\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.31124\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 0.0370 - mean_absolute_error: 0.1409 - nse: 0.8839 - val_loss: 0.5058 - val_mean_absolute_error: 0.5547 - val_nse: 0.1601\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.31124\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 0.0349 - mean_absolute_error: 0.1416 - nse: 0.8949 - val_loss: 0.5114 - val_mean_absolute_error: 0.5496 - val_nse: 0.1618\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.31124\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 0.0379 - mean_absolute_error: 0.1465 - nse: 0.8938 - val_loss: 0.4682 - val_mean_absolute_error: 0.5204 - val_nse: 0.2113\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.31124\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.0360 - mean_absolute_error: 0.1403 - nse: 0.8537 - val_loss: 0.5685 - val_mean_absolute_error: 0.5921 - val_nse: 0.1786\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.31124\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.0355 - mean_absolute_error: 0.1381 - nse: 0.8403 - val_loss: 0.6270 - val_mean_absolute_error: 0.6301 - val_nse: -3.5655e-04\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.31124\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 0.0327 - mean_absolute_error: 0.1371 - nse: 0.9010 - val_loss: 0.6620 - val_mean_absolute_error: 0.6329 - val_nse: 0.1447\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.31124\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.0329 - mean_absolute_error: 0.1367 - nse: 0.8750 - val_loss: 0.5429 - val_mean_absolute_error: 0.5773 - val_nse: 0.1446\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.31124\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 1s 578ms/step - loss: 0.0337 - mean_absolute_error: 0.1383 - nse: 0.8914 - val_loss: 0.5147 - val_mean_absolute_error: 0.5841 - val_nse: 0.1829\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.31124\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 1s 522ms/step - loss: 0.0406 - mean_absolute_error: 0.1471 - nse: 0.8763 - val_loss: 0.5475 - val_mean_absolute_error: 0.5657 - val_nse: 0.2655\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.31124\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.0321 - mean_absolute_error: 0.1345 - nse: 0.8638 - val_loss: 0.4608 - val_mean_absolute_error: 0.5203 - val_nse: 0.3587\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.31124\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.0319 - mean_absolute_error: 0.1353 - nse: 0.8933 - val_loss: 0.5929 - val_mean_absolute_error: 0.6059 - val_nse: 0.1134\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.31124\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 0.0414 - mean_absolute_error: 0.1474 - nse: 0.8736 - val_loss: 0.6470 - val_mean_absolute_error: 0.6474 - val_nse: 0.0524\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.31124\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 1s 523ms/step - loss: 0.0322 - mean_absolute_error: 0.1360 - nse: 0.9066 - val_loss: 0.4998 - val_mean_absolute_error: 0.5405 - val_nse: 0.2029\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.31124\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 1s 572ms/step - loss: 0.0360 - mean_absolute_error: 0.1422 - nse: 0.8676 - val_loss: 0.5366 - val_mean_absolute_error: 0.5471 - val_nse: 0.2011\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.31124\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 1s 574ms/step - loss: 0.0316 - mean_absolute_error: 0.1316 - nse: 0.8967 - val_loss: 0.6052 - val_mean_absolute_error: 0.6163 - val_nse: 0.1256\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.31124\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 1s 538ms/step - loss: 0.0336 - mean_absolute_error: 0.1391 - nse: 0.9131 - val_loss: 0.5512 - val_mean_absolute_error: 0.5649 - val_nse: 0.1420\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.31124\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 1s 550ms/step - loss: 0.0329 - mean_absolute_error: 0.1345 - nse: 0.8908 - val_loss: 0.5548 - val_mean_absolute_error: 0.5955 - val_nse: 0.1566\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.31124\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.0384 - mean_absolute_error: 0.1441 - nse: 0.8661 - val_loss: 0.5240 - val_mean_absolute_error: 0.5728 - val_nse: 0.1818\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.31124\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 1s 528ms/step - loss: 0.0322 - mean_absolute_error: 0.1341 - nse: 0.8756 - val_loss: 0.4737 - val_mean_absolute_error: 0.5368 - val_nse: 0.2545\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.31124\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 1s 527ms/step - loss: 0.0336 - mean_absolute_error: 0.1374 - nse: 0.9087 - val_loss: 0.6552 - val_mean_absolute_error: 0.6284 - val_nse: 0.1028\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.31124\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 1s 524ms/step - loss: 0.0309 - mean_absolute_error: 0.1340 - nse: 0.8882 - val_loss: 0.5147 - val_mean_absolute_error: 0.5617 - val_nse: 0.1532\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.31124\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.0337 - mean_absolute_error: 0.1382 - nse: 0.9015 - val_loss: 0.6042 - val_mean_absolute_error: 0.6046 - val_nse: 0.1222\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.31124\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.0317 - mean_absolute_error: 0.1310 - nse: 0.8979 - val_loss: 0.4736 - val_mean_absolute_error: 0.5286 - val_nse: 0.1354\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.31124\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 1s 553ms/step - loss: 0.0360 - mean_absolute_error: 0.1426 - nse: 0.8866 - val_loss: 0.5181 - val_mean_absolute_error: 0.5521 - val_nse: 0.1370\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.31124\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 1s 530ms/step - loss: 0.0310 - mean_absolute_error: 0.1366 - nse: 0.9096 - val_loss: 0.7551 - val_mean_absolute_error: 0.6898 - val_nse: -0.0875\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.31124\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 1s 547ms/step - loss: 0.0362 - mean_absolute_error: 0.1419 - nse: 0.8988 - val_loss: 0.6316 - val_mean_absolute_error: 0.6114 - val_nse: 0.1241\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.31124\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 531ms/step - loss: 0.0324 - mean_absolute_error: 0.1385 - nse: 0.8763 - val_loss: 0.5757 - val_mean_absolute_error: 0.6155 - val_nse: 0.0901\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.31124\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 1s 520ms/step - loss: 0.0292 - mean_absolute_error: 0.1298 - nse: 0.8877 - val_loss: 0.6080 - val_mean_absolute_error: 0.6092 - val_nse: -0.0850\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.31124\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 0.0300 - mean_absolute_error: 0.1300 - nse: 0.8914 - val_loss: 0.5709 - val_mean_absolute_error: 0.5896 - val_nse: 0.1820\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.31124\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 1s 572ms/step - loss: 0.0358 - mean_absolute_error: 0.1401 - nse: 0.8916 - val_loss: 0.5181 - val_mean_absolute_error: 0.5719 - val_nse: 0.1929\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.31124\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 0.0301 - mean_absolute_error: 0.1308 - nse: 0.9135 - val_loss: 0.6138 - val_mean_absolute_error: 0.6143 - val_nse: 0.0436\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.31124\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.0347 - mean_absolute_error: 0.1389 - nse: 0.8868 - val_loss: 0.5484 - val_mean_absolute_error: 0.5826 - val_nse: 0.1351\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.31124\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 1s 579ms/step - loss: 0.0355 - mean_absolute_error: 0.1425 - nse: 0.8716 - val_loss: 0.6173 - val_mean_absolute_error: 0.6271 - val_nse: 0.1489\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.31124\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2697 - mean_absolute_error: 0.4339 - nse: -0.0645 - val_loss: 0.9092 - val_mean_absolute_error: 0.7391 - val_nse: -0.3548\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.90922, saving model to save/yeong/models/nitrogen/multi_lstm.ckpt\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.2557 - mean_absolute_error: 0.4146 - nse: 0.0349 - val_loss: 0.6934 - val_mean_absolute_error: 0.6347 - val_nse: -0.2804\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.90922 to 0.69342, saving model to save/yeong/models/nitrogen/multi_lstm.ckpt\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.2873 - mean_absolute_error: 0.4322 - nse: 0.0740 - val_loss: 0.8899 - val_mean_absolute_error: 0.7399 - val_nse: -0.4489\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.69342\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.2690 - mean_absolute_error: 0.4073 - nse: 0.1394 - val_loss: 0.9906 - val_mean_absolute_error: 0.7931 - val_nse: -0.4632\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.69342\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.2661 - mean_absolute_error: 0.4131 - nse: 0.2133 - val_loss: 0.7797 - val_mean_absolute_error: 0.6557 - val_nse: -0.1744\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69342\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.2023 - mean_absolute_error: 0.3521 - nse: 0.2897 - val_loss: 0.8588 - val_mean_absolute_error: 0.7053 - val_nse: -0.2562\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69342\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.2432 - mean_absolute_error: 0.3585 - nse: 0.3430 - val_loss: 0.8940 - val_mean_absolute_error: 0.7349 - val_nse: -0.2779\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.69342\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.1753 - mean_absolute_error: 0.3071 - nse: 0.4967 - val_loss: 0.9028 - val_mean_absolute_error: 0.7159 - val_nse: -0.2572\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.69342\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.1133 - mean_absolute_error: 0.2503 - nse: 0.5233 - val_loss: 0.5915 - val_mean_absolute_error: 0.5522 - val_nse: 0.0754\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.69342 to 0.59153, saving model to save/yeong/models/nitrogen/multi_lstm.ckpt\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.1205 - mean_absolute_error: 0.2488 - nse: 0.5693 - val_loss: 0.7751 - val_mean_absolute_error: 0.6902 - val_nse: -0.0994\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.59153\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.1387 - mean_absolute_error: 0.2699 - nse: 0.5988 - val_loss: 0.5795 - val_mean_absolute_error: 0.5818 - val_nse: 0.0568\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.59153 to 0.57947, saving model to save/yeong/models/nitrogen/multi_lstm.ckpt\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1248 - mean_absolute_error: 0.2501 - nse: 0.6118 - val_loss: 0.5853 - val_mean_absolute_error: 0.5815 - val_nse: 0.2132\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.57947\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.1143 - mean_absolute_error: 0.2454 - nse: 0.6031 - val_loss: 0.5494 - val_mean_absolute_error: 0.5667 - val_nse: 0.1769\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.57947 to 0.54943, saving model to save/yeong/models/nitrogen/multi_lstm.ckpt\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.1129 - mean_absolute_error: 0.2410 - nse: 0.6660 - val_loss: 0.4185 - val_mean_absolute_error: 0.5081 - val_nse: 0.2345\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.54943 to 0.41847, saving model to save/yeong/models/nitrogen/multi_lstm.ckpt\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.1208 - mean_absolute_error: 0.2427 - nse: 0.5778 - val_loss: 0.4418 - val_mean_absolute_error: 0.5341 - val_nse: 0.1697\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.41847\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.1392 - mean_absolute_error: 0.2633 - nse: 0.5245 - val_loss: 0.5115 - val_mean_absolute_error: 0.5603 - val_nse: 0.1449\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.41847\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1005 - mean_absolute_error: 0.2233 - nse: 0.6457 - val_loss: 0.7484 - val_mean_absolute_error: 0.6837 - val_nse: -0.0389\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.41847\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.1110 - mean_absolute_error: 0.2363 - nse: 0.6220 - val_loss: 0.7396 - val_mean_absolute_error: 0.6731 - val_nse: -0.1119\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.41847\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.1052 - mean_absolute_error: 0.2321 - nse: 0.6842 - val_loss: 0.7224 - val_mean_absolute_error: 0.6510 - val_nse: 0.1250\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.41847\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0921 - mean_absolute_error: 0.2133 - nse: 0.7217 - val_loss: 0.6526 - val_mean_absolute_error: 0.6355 - val_nse: 0.0253\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.41847\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0820 - mean_absolute_error: 0.2081 - nse: 0.6829 - val_loss: 0.6166 - val_mean_absolute_error: 0.6242 - val_nse: 0.0086\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.41847\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0765 - mean_absolute_error: 0.1935 - nse: 0.7279 - val_loss: 0.6670 - val_mean_absolute_error: 0.6278 - val_nse: 0.0267\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.41847\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0702 - mean_absolute_error: 0.1909 - nse: 0.7422 - val_loss: 0.6739 - val_mean_absolute_error: 0.6333 - val_nse: 0.0103\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.41847\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0845 - mean_absolute_error: 0.2113 - nse: 0.6849 - val_loss: 0.6015 - val_mean_absolute_error: 0.5980 - val_nse: 0.0284\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.41847\n",
      "Epoch 25/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0998 - mean_absolute_error: 0.2227 - nse: 0.7004 - val_loss: 0.5920 - val_mean_absolute_error: 0.5926 - val_nse: 0.0654\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.41847\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0653 - mean_absolute_error: 0.1863 - nse: 0.7313 - val_loss: 0.6574 - val_mean_absolute_error: 0.6237 - val_nse: 0.1378\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.41847\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0792 - mean_absolute_error: 0.2039 - nse: 0.6982 - val_loss: 0.5450 - val_mean_absolute_error: 0.5664 - val_nse: 0.1602\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.41847\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.0602 - mean_absolute_error: 0.1796 - nse: 0.7115 - val_loss: 0.5977 - val_mean_absolute_error: 0.5863 - val_nse: 0.2293\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.41847\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1000 - mean_absolute_error: 0.2282 - nse: 0.7395 - val_loss: 0.5469 - val_mean_absolute_error: 0.5766 - val_nse: 0.1289\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.41847\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0897 - mean_absolute_error: 0.2135 - nse: 0.7406 - val_loss: 0.5031 - val_mean_absolute_error: 0.5411 - val_nse: 0.1936\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.41847\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0714 - mean_absolute_error: 0.1925 - nse: 0.7483 - val_loss: 0.5937 - val_mean_absolute_error: 0.6109 - val_nse: 0.1693\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.41847\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0840 - mean_absolute_error: 0.2157 - nse: 0.7335 - val_loss: 0.4666 - val_mean_absolute_error: 0.5445 - val_nse: 0.2599\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.41847\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0663 - mean_absolute_error: 0.1889 - nse: 0.7925 - val_loss: 0.4961 - val_mean_absolute_error: 0.5546 - val_nse: 0.2001\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.41847\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0814 - mean_absolute_error: 0.2096 - nse: 0.7693 - val_loss: 0.4604 - val_mean_absolute_error: 0.5295 - val_nse: 0.2796\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.41847\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0953 - mean_absolute_error: 0.2376 - nse: 0.7438 - val_loss: 0.5784 - val_mean_absolute_error: 0.6035 - val_nse: 0.1977\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.41847\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0646 - mean_absolute_error: 0.1823 - nse: 0.7905 - val_loss: 0.6208 - val_mean_absolute_error: 0.6211 - val_nse: 0.0957\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.41847\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0729 - mean_absolute_error: 0.1994 - nse: 0.7835 - val_loss: 0.5679 - val_mean_absolute_error: 0.5919 - val_nse: 0.1963\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.41847\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0543 - mean_absolute_error: 0.1707 - nse: 0.7869 - val_loss: 0.5011 - val_mean_absolute_error: 0.5497 - val_nse: 0.2014\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.41847\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0576 - mean_absolute_error: 0.1692 - nse: 0.7949 - val_loss: 0.4749 - val_mean_absolute_error: 0.5377 - val_nse: 0.1735\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.41847\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.0592 - mean_absolute_error: 0.1780 - nse: 0.8023 - val_loss: 0.5527 - val_mean_absolute_error: 0.5718 - val_nse: 0.1229\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.41847\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0567 - mean_absolute_error: 0.1755 - nse: 0.8172 - val_loss: 0.6590 - val_mean_absolute_error: 0.6383 - val_nse: 0.0519\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.41847\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.0593 - mean_absolute_error: 0.1806 - nse: 0.8280 - val_loss: 0.5699 - val_mean_absolute_error: 0.5810 - val_nse: 0.1312\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.41847\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.0541 - mean_absolute_error: 0.1690 - nse: 0.8062 - val_loss: 0.5969 - val_mean_absolute_error: 0.5929 - val_nse: 0.0946\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.41847\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0587 - mean_absolute_error: 0.1796 - nse: 0.8248 - val_loss: 0.7472 - val_mean_absolute_error: 0.6912 - val_nse: -0.1126\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.41847\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0562 - mean_absolute_error: 0.1737 - nse: 0.8538 - val_loss: 0.7928 - val_mean_absolute_error: 0.7005 - val_nse: -0.0207\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.41847\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0520 - mean_absolute_error: 0.1705 - nse: 0.7818 - val_loss: 0.5396 - val_mean_absolute_error: 0.5577 - val_nse: 0.1594\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.41847\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.0538 - mean_absolute_error: 0.1726 - nse: 0.7983 - val_loss: 0.5902 - val_mean_absolute_error: 0.6087 - val_nse: 0.0280\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.41847\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0470 - mean_absolute_error: 0.1603 - nse: 0.8570 - val_loss: 0.6619 - val_mean_absolute_error: 0.6518 - val_nse: 0.0641\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.41847\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0575 - mean_absolute_error: 0.1773 - nse: 0.8352 - val_loss: 0.5877 - val_mean_absolute_error: 0.5995 - val_nse: 0.1968\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.41847\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0613 - mean_absolute_error: 0.1764 - nse: 0.7907 - val_loss: 0.5257 - val_mean_absolute_error: 0.5651 - val_nse: 0.1966\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.41847\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0559 - mean_absolute_error: 0.1756 - nse: 0.8134 - val_loss: 0.4556 - val_mean_absolute_error: 0.5222 - val_nse: 0.2727\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.41847\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0544 - mean_absolute_error: 0.1678 - nse: 0.8408 - val_loss: 0.4222 - val_mean_absolute_error: 0.5139 - val_nse: 0.1621\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.41847\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0441 - mean_absolute_error: 0.1521 - nse: 0.8315 - val_loss: 0.5044 - val_mean_absolute_error: 0.5632 - val_nse: 0.1251\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.41847\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0583 - mean_absolute_error: 0.1739 - nse: 0.8019 - val_loss: 0.6137 - val_mean_absolute_error: 0.6327 - val_nse: -0.0185\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.41847\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0532 - mean_absolute_error: 0.1690 - nse: 0.8310 - val_loss: 0.7728 - val_mean_absolute_error: 0.7129 - val_nse: -0.0369\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.41847\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0499 - mean_absolute_error: 0.1643 - nse: 0.8408 - val_loss: 0.7248 - val_mean_absolute_error: 0.6837 - val_nse: 0.0803\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.41847\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0511 - mean_absolute_error: 0.1681 - nse: 0.8382 - val_loss: 0.6256 - val_mean_absolute_error: 0.6401 - val_nse: 0.0882\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.41847\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0524 - mean_absolute_error: 0.1718 - nse: 0.8291 - val_loss: 0.6418 - val_mean_absolute_error: 0.6359 - val_nse: 0.0091\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.41847\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0395 - mean_absolute_error: 0.1462 - nse: 0.8496 - val_loss: 0.6382 - val_mean_absolute_error: 0.6365 - val_nse: 0.0320\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.41847\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0419 - mean_absolute_error: 0.1537 - nse: 0.8307 - val_loss: 0.6170 - val_mean_absolute_error: 0.6359 - val_nse: 0.1204\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.41847\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0469 - mean_absolute_error: 0.1607 - nse: 0.8345 - val_loss: 0.6023 - val_mean_absolute_error: 0.6138 - val_nse: 0.0307\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.41847\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0514 - mean_absolute_error: 0.1654 - nse: 0.8434 - val_loss: 0.6395 - val_mean_absolute_error: 0.6325 - val_nse: -0.0469\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.41847\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0434 - mean_absolute_error: 0.1558 - nse: 0.8411 - val_loss: 0.6902 - val_mean_absolute_error: 0.6603 - val_nse: 0.0773\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.41847\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.0422 - mean_absolute_error: 0.1525 - nse: 0.8325 - val_loss: 0.4935 - val_mean_absolute_error: 0.5468 - val_nse: 0.2527\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.41847\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0404 - mean_absolute_error: 0.1458 - nse: 0.8172 - val_loss: 0.6008 - val_mean_absolute_error: 0.6059 - val_nse: 0.1907\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.41847\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0418 - mean_absolute_error: 0.1522 - nse: 0.8607 - val_loss: 0.5392 - val_mean_absolute_error: 0.5793 - val_nse: 0.1866\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.41847\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0488 - mean_absolute_error: 0.1611 - nse: 0.8659 - val_loss: 0.6337 - val_mean_absolute_error: 0.6237 - val_nse: 0.0154\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.41847\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0509 - mean_absolute_error: 0.1587 - nse: 0.8314 - val_loss: 0.5982 - val_mean_absolute_error: 0.6051 - val_nse: 0.1350\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.41847\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0511 - mean_absolute_error: 0.1702 - nse: 0.8508 - val_loss: 0.6184 - val_mean_absolute_error: 0.6235 - val_nse: 0.0757\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.41847\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0391 - mean_absolute_error: 0.1477 - nse: 0.8674 - val_loss: 0.5587 - val_mean_absolute_error: 0.5792 - val_nse: 0.0734\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.41847\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0488 - mean_absolute_error: 0.1622 - nse: 0.8609 - val_loss: 0.5584 - val_mean_absolute_error: 0.5744 - val_nse: 0.1517\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.41847\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0521 - mean_absolute_error: 0.1698 - nse: 0.8643 - val_loss: 0.5846 - val_mean_absolute_error: 0.5985 - val_nse: 0.1122\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.41847\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0506 - mean_absolute_error: 0.1627 - nse: 0.8454 - val_loss: 0.7346 - val_mean_absolute_error: 0.6589 - val_nse: 0.0854\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.41847\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0468 - mean_absolute_error: 0.1581 - nse: 0.8281 - val_loss: 0.5968 - val_mean_absolute_error: 0.6136 - val_nse: 0.0557\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.41847\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0442 - mean_absolute_error: 0.1504 - nse: 0.8640 - val_loss: 0.5139 - val_mean_absolute_error: 0.5474 - val_nse: 0.2212\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.41847\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0467 - mean_absolute_error: 0.1551 - nse: 0.8401 - val_loss: 0.4862 - val_mean_absolute_error: 0.5418 - val_nse: 0.1514\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.41847\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0372 - mean_absolute_error: 0.1406 - nse: 0.8483 - val_loss: 0.5415 - val_mean_absolute_error: 0.5722 - val_nse: 0.1361\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.41847\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.0390 - mean_absolute_error: 0.1490 - nse: 0.8777 - val_loss: 0.5806 - val_mean_absolute_error: 0.5942 - val_nse: 0.0947\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.41847\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.0415 - mean_absolute_error: 0.1512 - nse: 0.8713 - val_loss: 0.5415 - val_mean_absolute_error: 0.5643 - val_nse: 0.2044\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.41847\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.0398 - mean_absolute_error: 0.1462 - nse: 0.8795 - val_loss: 0.5018 - val_mean_absolute_error: 0.5524 - val_nse: 0.2171\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.41847\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0400 - mean_absolute_error: 0.1507 - nse: 0.8608 - val_loss: 0.7294 - val_mean_absolute_error: 0.6759 - val_nse: -0.0484\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.41847\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0428 - mean_absolute_error: 0.1495 - nse: 0.8988 - val_loss: 0.7227 - val_mean_absolute_error: 0.6697 - val_nse: 0.0473\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.41847\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0368 - mean_absolute_error: 0.1431 - nse: 0.8525 - val_loss: 0.6464 - val_mean_absolute_error: 0.6288 - val_nse: 0.0378\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.41847\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0370 - mean_absolute_error: 0.1449 - nse: 0.8599 - val_loss: 0.4957 - val_mean_absolute_error: 0.5546 - val_nse: 0.1343\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.41847\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0379 - mean_absolute_error: 0.1424 - nse: 0.8802 - val_loss: 0.7394 - val_mean_absolute_error: 0.6938 - val_nse: 0.0497\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.41847\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0381 - mean_absolute_error: 0.1446 - nse: 0.8800 - val_loss: 0.5531 - val_mean_absolute_error: 0.5847 - val_nse: 0.1316\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.41847\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0428 - mean_absolute_error: 0.1503 - nse: 0.8703 - val_loss: 0.5133 - val_mean_absolute_error: 0.5508 - val_nse: 0.1931\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.41847\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0450 - mean_absolute_error: 0.1522 - nse: 0.8587 - val_loss: 0.5518 - val_mean_absolute_error: 0.5755 - val_nse: 0.1793\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.41847\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0401 - mean_absolute_error: 0.1490 - nse: 0.8760 - val_loss: 0.3841 - val_mean_absolute_error: 0.4850 - val_nse: 0.2414\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.41847 to 0.38413, saving model to save/yeong/models/nitrogen/multi_lstm.ckpt\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0360 - mean_absolute_error: 0.1384 - nse: 0.8607 - val_loss: 0.4825 - val_mean_absolute_error: 0.5526 - val_nse: 0.1446\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.38413\n",
      "Epoch 91/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0388 - mean_absolute_error: 0.1453 - nse: 0.8684 - val_loss: 0.5826 - val_mean_absolute_error: 0.6108 - val_nse: -0.0034\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.38413\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0381 - mean_absolute_error: 0.1470 - nse: 0.8804 - val_loss: 0.7954 - val_mean_absolute_error: 0.7136 - val_nse: -0.0650\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.38413\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0376 - mean_absolute_error: 0.1437 - nse: 0.8623 - val_loss: 0.7550 - val_mean_absolute_error: 0.6914 - val_nse: -0.0592\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.38413\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0374 - mean_absolute_error: 0.1445 - nse: 0.8847 - val_loss: 0.6599 - val_mean_absolute_error: 0.6468 - val_nse: 0.1337\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.38413\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0401 - mean_absolute_error: 0.1503 - nse: 0.8778 - val_loss: 0.6292 - val_mean_absolute_error: 0.6256 - val_nse: 0.0655\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.38413\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0320 - mean_absolute_error: 0.1322 - nse: 0.8674 - val_loss: 0.6137 - val_mean_absolute_error: 0.6213 - val_nse: 0.0366\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.38413\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.0320 - mean_absolute_error: 0.1329 - nse: 0.8882 - val_loss: 0.5742 - val_mean_absolute_error: 0.6099 - val_nse: 0.1488\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.38413\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0353 - mean_absolute_error: 0.1393 - nse: 0.8677 - val_loss: 0.6405 - val_mean_absolute_error: 0.6321 - val_nse: 0.0201\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.38413\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0365 - mean_absolute_error: 0.1393 - nse: 0.8731 - val_loss: 0.5905 - val_mean_absolute_error: 0.6172 - val_nse: 0.0291\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.38413\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0318 - mean_absolute_error: 0.1332 - nse: 0.8978 - val_loss: 0.6466 - val_mean_absolute_error: 0.6306 - val_nse: 0.0448\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.38413\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0340 - mean_absolute_error: 0.1386 - nse: 0.8669 - val_loss: 0.5991 - val_mean_absolute_error: 0.5962 - val_nse: 0.2191\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.38413\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.0316 - mean_absolute_error: 0.1290 - nse: 0.8528 - val_loss: 0.5958 - val_mean_absolute_error: 0.6077 - val_nse: 0.1163\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.38413\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0292 - mean_absolute_error: 0.1276 - nse: 0.8926 - val_loss: 0.5778 - val_mean_absolute_error: 0.5903 - val_nse: 0.1955\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.38413\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0318 - mean_absolute_error: 0.1298 - nse: 0.9107 - val_loss: 0.6074 - val_mean_absolute_error: 0.6160 - val_nse: 0.0278\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.38413\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0409 - mean_absolute_error: 0.1430 - nse: 0.8740 - val_loss: 0.5556 - val_mean_absolute_error: 0.5807 - val_nse: 0.1316\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.38413\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0371 - mean_absolute_error: 0.1409 - nse: 0.8860 - val_loss: 0.6502 - val_mean_absolute_error: 0.6327 - val_nse: 0.1132\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.38413\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0325 - mean_absolute_error: 0.1316 - nse: 0.8854 - val_loss: 0.4932 - val_mean_absolute_error: 0.5593 - val_nse: 0.1261\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.38413\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.0317 - mean_absolute_error: 0.1339 - nse: 0.9113 - val_loss: 0.5711 - val_mean_absolute_error: 0.5947 - val_nse: 0.1256\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.38413\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0371 - mean_absolute_error: 0.1421 - nse: 0.9004 - val_loss: 0.5455 - val_mean_absolute_error: 0.5793 - val_nse: 0.2138\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.38413\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0390 - mean_absolute_error: 0.1448 - nse: 0.8954 - val_loss: 0.6380 - val_mean_absolute_error: 0.6177 - val_nse: 0.1515\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.38413\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.0339 - mean_absolute_error: 0.1331 - nse: 0.8756 - val_loss: 0.6667 - val_mean_absolute_error: 0.6590 - val_nse: 0.0245\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.38413\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0352 - mean_absolute_error: 0.1378 - nse: 0.8932 - val_loss: 0.4748 - val_mean_absolute_error: 0.5323 - val_nse: 0.2494\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.38413\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0344 - mean_absolute_error: 0.1334 - nse: 0.8627 - val_loss: 0.4847 - val_mean_absolute_error: 0.5542 - val_nse: 0.2168\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.38413\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0320 - mean_absolute_error: 0.1295 - nse: 0.8816 - val_loss: 0.4870 - val_mean_absolute_error: 0.5450 - val_nse: 0.1761\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.38413\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0275 - mean_absolute_error: 0.1273 - nse: 0.9139 - val_loss: 0.5232 - val_mean_absolute_error: 0.5740 - val_nse: 0.1651\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.38413\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0330 - mean_absolute_error: 0.1359 - nse: 0.8992 - val_loss: 0.5351 - val_mean_absolute_error: 0.5700 - val_nse: 0.1725\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.38413\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.0296 - mean_absolute_error: 0.1273 - nse: 0.9112 - val_loss: 0.5496 - val_mean_absolute_error: 0.5881 - val_nse: 0.2179\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.38413\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.0295 - mean_absolute_error: 0.1273 - nse: 0.8877 - val_loss: 0.5881 - val_mean_absolute_error: 0.5958 - val_nse: 0.1017\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.38413\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0299 - mean_absolute_error: 0.1282 - nse: 0.9260 - val_loss: 0.6417 - val_mean_absolute_error: 0.6364 - val_nse: 0.1018\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.38413\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0276 - mean_absolute_error: 0.1256 - nse: 0.9136 - val_loss: 0.7421 - val_mean_absolute_error: 0.6834 - val_nse: -0.0439\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.38413\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0286 - mean_absolute_error: 0.1284 - nse: 0.8722 - val_loss: 0.4781 - val_mean_absolute_error: 0.5353 - val_nse: 0.2323\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.38413\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0291 - mean_absolute_error: 0.1253 - nse: 0.9010 - val_loss: 0.6655 - val_mean_absolute_error: 0.6623 - val_nse: 0.0938\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.38413\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0269 - mean_absolute_error: 0.1259 - nse: 0.9237 - val_loss: 0.5492 - val_mean_absolute_error: 0.5955 - val_nse: 0.1012\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.38413\n",
      "Epoch 124/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0312 - mean_absolute_error: 0.1323 - nse: 0.9000 - val_loss: 0.5199 - val_mean_absolute_error: 0.5490 - val_nse: 0.2770\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.38413\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0322 - mean_absolute_error: 0.1302 - nse: 0.8960 - val_loss: 0.5320 - val_mean_absolute_error: 0.5684 - val_nse: 0.2074\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.38413\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0319 - mean_absolute_error: 0.1342 - nse: 0.8970 - val_loss: 0.3930 - val_mean_absolute_error: 0.5013 - val_nse: 0.2477\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.38413\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0282 - mean_absolute_error: 0.1256 - nse: 0.8995 - val_loss: 0.4528 - val_mean_absolute_error: 0.5316 - val_nse: 0.1960\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.38413\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0335 - mean_absolute_error: 0.1317 - nse: 0.8881 - val_loss: 0.5049 - val_mean_absolute_error: 0.5682 - val_nse: 0.1353\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.38413\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0278 - mean_absolute_error: 0.1250 - nse: 0.9106 - val_loss: 0.7476 - val_mean_absolute_error: 0.6900 - val_nse: 5.7334e-04\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.38413\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0335 - mean_absolute_error: 0.1350 - nse: 0.8715 - val_loss: 0.6980 - val_mean_absolute_error: 0.6782 - val_nse: -0.0425\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.38413\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0308 - mean_absolute_error: 0.1290 - nse: 0.9066 - val_loss: 0.6926 - val_mean_absolute_error: 0.6669 - val_nse: 0.1321\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.38413\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0300 - mean_absolute_error: 0.1309 - nse: 0.9128 - val_loss: 0.6017 - val_mean_absolute_error: 0.6191 - val_nse: 0.0792\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.38413\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0282 - mean_absolute_error: 0.1244 - nse: 0.8974 - val_loss: 0.6303 - val_mean_absolute_error: 0.6269 - val_nse: 0.0171\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.38413\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0246 - mean_absolute_error: 0.1175 - nse: 0.9010 - val_loss: 0.6178 - val_mean_absolute_error: 0.6245 - val_nse: 0.1023\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.38413\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0274 - mean_absolute_error: 0.1238 - nse: 0.9022 - val_loss: 0.5859 - val_mean_absolute_error: 0.6076 - val_nse: 0.1314\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.38413\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0305 - mean_absolute_error: 0.1286 - nse: 0.8872 - val_loss: 0.6022 - val_mean_absolute_error: 0.6158 - val_nse: 0.0098\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.38413\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.0249 - mean_absolute_error: 0.1170 - nse: 0.9250 - val_loss: 0.5535 - val_mean_absolute_error: 0.5869 - val_nse: 0.1574\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.38413\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0283 - mean_absolute_error: 0.1236 - nse: 0.8893 - val_loss: 0.6191 - val_mean_absolute_error: 0.6116 - val_nse: 0.1580\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.38413\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0291 - mean_absolute_error: 0.1226 - nse: 0.8754 - val_loss: 0.5794 - val_mean_absolute_error: 0.6021 - val_nse: 0.1480\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.38413\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0250 - mean_absolute_error: 0.1166 - nse: 0.8898 - val_loss: 0.5301 - val_mean_absolute_error: 0.5638 - val_nse: 0.2977\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.38413\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0258 - mean_absolute_error: 0.1197 - nse: 0.9321 - val_loss: 0.5858 - val_mean_absolute_error: 0.6022 - val_nse: 0.0256\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.38413\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0327 - mean_absolute_error: 0.1286 - nse: 0.9031 - val_loss: 0.5264 - val_mean_absolute_error: 0.5586 - val_nse: 0.1531\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.38413\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0301 - mean_absolute_error: 0.1261 - nse: 0.8956 - val_loss: 0.6794 - val_mean_absolute_error: 0.6461 - val_nse: 0.0780\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.38413\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0334 - mean_absolute_error: 0.1348 - nse: 0.8940 - val_loss: 0.5703 - val_mean_absolute_error: 0.5984 - val_nse: 0.1182\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.38413\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0246 - mean_absolute_error: 0.1175 - nse: 0.9246 - val_loss: 0.5622 - val_mean_absolute_error: 0.5839 - val_nse: 0.1076\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.38413\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0323 - mean_absolute_error: 0.1303 - nse: 0.9115 - val_loss: 0.5311 - val_mean_absolute_error: 0.5629 - val_nse: 0.1644\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.38413\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.0315 - mean_absolute_error: 0.1319 - nse: 0.9127 - val_loss: 0.6045 - val_mean_absolute_error: 0.6041 - val_nse: 0.1664\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.38413\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0324 - mean_absolute_error: 0.1275 - nse: 0.8908 - val_loss: 0.6602 - val_mean_absolute_error: 0.6379 - val_nse: 0.0232\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.38413\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0315 - mean_absolute_error: 0.1300 - nse: 0.9073 - val_loss: 0.5353 - val_mean_absolute_error: 0.5642 - val_nse: 0.2416\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.38413\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0296 - mean_absolute_error: 0.1242 - nse: 0.8794 - val_loss: 0.5365 - val_mean_absolute_error: 0.5699 - val_nse: 0.1693\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.38413\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0289 - mean_absolute_error: 0.1221 - nse: 0.8984 - val_loss: 0.4270 - val_mean_absolute_error: 0.5080 - val_nse: 0.2328\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.38413\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0279 - mean_absolute_error: 0.1266 - nse: 0.9061 - val_loss: 0.5357 - val_mean_absolute_error: 0.5648 - val_nse: 0.1688\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.38413\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0286 - mean_absolute_error: 0.1286 - nse: 0.9100 - val_loss: 0.5805 - val_mean_absolute_error: 0.5958 - val_nse: 0.1209\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.38413\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.0257 - mean_absolute_error: 0.1201 - nse: 0.9235 - val_loss: 0.4910 - val_mean_absolute_error: 0.5491 - val_nse: 0.2649\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.38413\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 0.0248 - mean_absolute_error: 0.1174 - nse: 0.9083 - val_loss: 0.5591 - val_mean_absolute_error: 0.5798 - val_nse: 0.1580\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.38413\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0284 - mean_absolute_error: 0.1252 - nse: 0.9190 - val_loss: 0.6384 - val_mean_absolute_error: 0.6418 - val_nse: 0.0824\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.38413\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0249 - mean_absolute_error: 0.1191 - nse: 0.9369 - val_loss: 0.7142 - val_mean_absolute_error: 0.6661 - val_nse: 0.0647\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.38413\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0264 - mean_absolute_error: 0.1202 - nse: 0.8787 - val_loss: 0.4746 - val_mean_absolute_error: 0.5336 - val_nse: 0.2198\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.38413\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0263 - mean_absolute_error: 0.1210 - nse: 0.9041 - val_loss: 0.5491 - val_mean_absolute_error: 0.5891 - val_nse: 0.1406\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.38413\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.0243 - mean_absolute_error: 0.1184 - nse: 0.9263 - val_loss: 0.6116 - val_mean_absolute_error: 0.6258 - val_nse: 0.1559\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.38413\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0250 - mean_absolute_error: 0.1181 - nse: 0.9274 - val_loss: 0.5307 - val_mean_absolute_error: 0.5580 - val_nse: 0.2476\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.38413\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0294 - mean_absolute_error: 0.1226 - nse: 0.8955 - val_loss: 0.5130 - val_mean_absolute_error: 0.5470 - val_nse: 0.2484\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.38413\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0282 - mean_absolute_error: 0.1273 - nse: 0.9157 - val_loss: 0.4344 - val_mean_absolute_error: 0.5203 - val_nse: 0.2454\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.38413\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.0261 - mean_absolute_error: 0.1179 - nse: 0.9130 - val_loss: 0.3806 - val_mean_absolute_error: 0.4826 - val_nse: 0.2704\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.38413 to 0.38062, saving model to save/yeong/models/nitrogen/multi_lstm.ckpt\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0284 - mean_absolute_error: 0.1207 - nse: 0.8909 - val_loss: 0.4809 - val_mean_absolute_error: 0.5499 - val_nse: 0.1573\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.38062\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 0.0258 - mean_absolute_error: 0.1192 - nse: 0.9158 - val_loss: 0.5741 - val_mean_absolute_error: 0.6018 - val_nse: 0.0735\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.38062\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0294 - mean_absolute_error: 0.1239 - nse: 0.9057 - val_loss: 0.7236 - val_mean_absolute_error: 0.6801 - val_nse: 0.0348\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.38062\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0305 - mean_absolute_error: 0.1261 - nse: 0.9017 - val_loss: 0.7160 - val_mean_absolute_error: 0.6827 - val_nse: 0.1221\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.38062\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0253 - mean_absolute_error: 0.1198 - nse: 0.9223 - val_loss: 0.5380 - val_mean_absolute_error: 0.5895 - val_nse: 0.1727\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.38062\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0254 - mean_absolute_error: 0.1190 - nse: 0.9129 - val_loss: 0.6307 - val_mean_absolute_error: 0.6417 - val_nse: 0.0175\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.38062\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0222 - mean_absolute_error: 0.1110 - nse: 0.9183 - val_loss: 0.5867 - val_mean_absolute_error: 0.5973 - val_nse: 0.1287\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.38062\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0246 - mean_absolute_error: 0.1173 - nse: 0.9086 - val_loss: 0.5626 - val_mean_absolute_error: 0.5985 - val_nse: 0.1649\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.38062\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.0268 - mean_absolute_error: 0.1189 - nse: 0.9036 - val_loss: 0.5553 - val_mean_absolute_error: 0.5854 - val_nse: 0.1158\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.38062\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0234 - mean_absolute_error: 0.1145 - nse: 0.9226 - val_loss: 0.6425 - val_mean_absolute_error: 0.6346 - val_nse: 0.0072\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.38062\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0250 - mean_absolute_error: 0.1162 - nse: 0.9069 - val_loss: 0.6204 - val_mean_absolute_error: 0.6199 - val_nse: 0.1814\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.38062\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0252 - mean_absolute_error: 0.1162 - nse: 0.9037 - val_loss: 0.4703 - val_mean_absolute_error: 0.5293 - val_nse: 0.2480\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.38062\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0237 - mean_absolute_error: 0.1091 - nse: 0.8892 - val_loss: 0.5955 - val_mean_absolute_error: 0.6031 - val_nse: 0.2349\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.38062\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0227 - mean_absolute_error: 0.1133 - nse: 0.9318 - val_loss: 0.5083 - val_mean_absolute_error: 0.5596 - val_nse: 0.1962\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.38062\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0265 - mean_absolute_error: 0.1167 - nse: 0.9318 - val_loss: 0.5434 - val_mean_absolute_error: 0.5759 - val_nse: 0.1263\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.38062\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0289 - mean_absolute_error: 0.1196 - nse: 0.8913 - val_loss: 0.5826 - val_mean_absolute_error: 0.6006 - val_nse: 0.2008\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.38062\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0265 - mean_absolute_error: 0.1209 - nse: 0.9192 - val_loss: 0.5701 - val_mean_absolute_error: 0.6084 - val_nse: 0.1268\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.38062\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0235 - mean_absolute_error: 0.1137 - nse: 0.9264 - val_loss: 0.5193 - val_mean_absolute_error: 0.5575 - val_nse: 0.1498\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.38062\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0258 - mean_absolute_error: 0.1184 - nse: 0.9246 - val_loss: 0.5112 - val_mean_absolute_error: 0.5519 - val_nse: 0.2003\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.38062\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.0254 - mean_absolute_error: 0.1167 - nse: 0.9330 - val_loss: 0.5479 - val_mean_absolute_error: 0.5823 - val_nse: 0.1894\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.38062\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0280 - mean_absolute_error: 0.1187 - nse: 0.9090 - val_loss: 0.6612 - val_mean_absolute_error: 0.6281 - val_nse: 0.1540\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.38062\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0261 - mean_absolute_error: 0.1165 - nse: 0.9173 - val_loss: 0.5219 - val_mean_absolute_error: 0.5701 - val_nse: 0.1707\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.38062\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0239 - mean_absolute_error: 0.1123 - nse: 0.9150 - val_loss: 0.4886 - val_mean_absolute_error: 0.5377 - val_nse: 0.2696\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.38062\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0273 - mean_absolute_error: 0.1172 - nse: 0.9113 - val_loss: 0.4617 - val_mean_absolute_error: 0.5293 - val_nse: 0.2207\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.38062\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0225 - mean_absolute_error: 0.1124 - nse: 0.9107 - val_loss: 0.5164 - val_mean_absolute_error: 0.5613 - val_nse: 0.1822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00189: val_loss did not improve from 0.38062\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0216 - mean_absolute_error: 0.1124 - nse: 0.9274 - val_loss: 0.5312 - val_mean_absolute_error: 0.5668 - val_nse: 0.1826\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.38062\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.0247 - mean_absolute_error: 0.1161 - nse: 0.9321 - val_loss: 0.4955 - val_mean_absolute_error: 0.5497 - val_nse: 0.2461\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.38062\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0210 - mean_absolute_error: 0.1074 - nse: 0.9297 - val_loss: 0.5083 - val_mean_absolute_error: 0.5555 - val_nse: 0.2376\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.38062\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0232 - mean_absolute_error: 0.1120 - nse: 0.9239 - val_loss: 0.6446 - val_mean_absolute_error: 0.6338 - val_nse: 0.0514\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.38062\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0231 - mean_absolute_error: 0.1123 - nse: 0.9423 - val_loss: 0.7114 - val_mean_absolute_error: 0.6694 - val_nse: 0.0732\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.38062\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.0219 - mean_absolute_error: 0.1097 - nse: 0.9157 - val_loss: 0.5329 - val_mean_absolute_error: 0.5563 - val_nse: 0.2225\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.38062\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0211 - mean_absolute_error: 0.1077 - nse: 0.9134 - val_loss: 0.4985 - val_mean_absolute_error: 0.5669 - val_nse: 0.0973\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.38062\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0221 - mean_absolute_error: 0.1098 - nse: 0.9362 - val_loss: 0.6916 - val_mean_absolute_error: 0.6689 - val_nse: 0.1187\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.38062\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0188 - mean_absolute_error: 0.1043 - nse: 0.9414 - val_loss: 0.5256 - val_mean_absolute_error: 0.5602 - val_nse: 0.1941\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.38062\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.0250 - mean_absolute_error: 0.1136 - nse: 0.9246 - val_loss: 0.4957 - val_mean_absolute_error: 0.5406 - val_nse: 0.2706\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.38062\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0258 - mean_absolute_error: 0.1162 - nse: 0.9118 - val_loss: 0.4667 - val_mean_absolute_error: 0.5277 - val_nse: 0.2723\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.38062\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 571ms/step - loss: 0.3517 - mean_absolute_error: 0.4830 - nse: -2.8300e-04 - val_loss: 0.9378 - val_mean_absolute_error: 0.7593 - val_nse: -0.4613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93782, saving model to save/yeong/models/nitrogen/multi_conv.ckpt\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.2336 - mean_absolute_error: 0.3844 - nse: 0.1315 - val_loss: 0.9005 - val_mean_absolute_error: 0.7409 - val_nse: -0.3751\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.93782 to 0.90052, saving model to save/yeong/models/nitrogen/multi_conv.ckpt\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.2063 - mean_absolute_error: 0.3456 - nse: 0.3860 - val_loss: 0.6491 - val_mean_absolute_error: 0.6046 - val_nse: 0.0106\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.90052 to 0.64908, saving model to save/yeong/models/nitrogen/multi_conv.ckpt\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.1219 - mean_absolute_error: 0.2565 - nse: 0.6019 - val_loss: 0.3794 - val_mean_absolute_error: 0.4606 - val_nse: 0.4122\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.64908 to 0.37937, saving model to save/yeong/models/nitrogen/multi_conv.ckpt\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.1270 - mean_absolute_error: 0.2702 - nse: 0.5726 - val_loss: 0.4405 - val_mean_absolute_error: 0.4993 - val_nse: 0.4144\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.37937\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1654 - mean_absolute_error: 0.3052 - nse: 0.4983 - val_loss: 0.4000 - val_mean_absolute_error: 0.4729 - val_nse: 0.3645\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.37937\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0819 - mean_absolute_error: 0.2115 - nse: 0.7164 - val_loss: 0.4468 - val_mean_absolute_error: 0.5142 - val_nse: 0.3239\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.37937\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0919 - mean_absolute_error: 0.2205 - nse: 0.6871 - val_loss: 0.5237 - val_mean_absolute_error: 0.5558 - val_nse: 0.1621\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.37937\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0844 - mean_absolute_error: 0.2116 - nse: 0.7280 - val_loss: 0.6880 - val_mean_absolute_error: 0.6475 - val_nse: 0.0359\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.37937\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0766 - mean_absolute_error: 0.1988 - nse: 0.6888 - val_loss: 0.6655 - val_mean_absolute_error: 0.6310 - val_nse: 0.0652\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37937\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.1122 - mean_absolute_error: 0.2402 - nse: 0.6948 - val_loss: 0.6068 - val_mean_absolute_error: 0.6087 - val_nse: 0.1446\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.37937\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0934 - mean_absolute_error: 0.2142 - nse: 0.6895 - val_loss: 0.4569 - val_mean_absolute_error: 0.5165 - val_nse: 0.2872\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.37937\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0789 - mean_absolute_error: 0.2066 - nse: 0.7267 - val_loss: 0.5321 - val_mean_absolute_error: 0.5659 - val_nse: 0.2552\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.37937\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0699 - mean_absolute_error: 0.2004 - nse: 0.7804 - val_loss: 0.3421 - val_mean_absolute_error: 0.4615 - val_nse: 0.4345\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.37937 to 0.34207, saving model to save/yeong/models/nitrogen/multi_conv.ckpt\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0672 - mean_absolute_error: 0.1909 - nse: 0.8138 - val_loss: 0.4154 - val_mean_absolute_error: 0.5189 - val_nse: 0.3810\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.34207\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0815 - mean_absolute_error: 0.2122 - nse: 0.7320 - val_loss: 0.3907 - val_mean_absolute_error: 0.5105 - val_nse: 0.3604\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.34207\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0724 - mean_absolute_error: 0.1992 - nse: 0.7160 - val_loss: 0.4075 - val_mean_absolute_error: 0.5184 - val_nse: 0.4263\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.34207\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0835 - mean_absolute_error: 0.2184 - nse: 0.7635 - val_loss: 0.4361 - val_mean_absolute_error: 0.5413 - val_nse: 0.3390\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.34207\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0558 - mean_absolute_error: 0.1824 - nse: 0.8108 - val_loss: 0.3594 - val_mean_absolute_error: 0.4765 - val_nse: 0.4003\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.34207\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0637 - mean_absolute_error: 0.1844 - nse: 0.7541 - val_loss: 0.4743 - val_mean_absolute_error: 0.5333 - val_nse: 0.3247\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.34207\n",
      "Epoch 21/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0578 - mean_absolute_error: 0.1753 - nse: 0.8052 - val_loss: 0.4243 - val_mean_absolute_error: 0.5180 - val_nse: 0.3175\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.34207\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0569 - mean_absolute_error: 0.1797 - nse: 0.8121 - val_loss: 0.4096 - val_mean_absolute_error: 0.5184 - val_nse: 0.3613\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.34207\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0653 - mean_absolute_error: 0.1870 - nse: 0.8013 - val_loss: 0.3758 - val_mean_absolute_error: 0.4924 - val_nse: 0.3556\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.34207\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0595 - mean_absolute_error: 0.1834 - nse: 0.7902 - val_loss: 0.4356 - val_mean_absolute_error: 0.5272 - val_nse: 0.4001\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.34207\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0556 - mean_absolute_error: 0.1791 - nse: 0.8243 - val_loss: 0.4051 - val_mean_absolute_error: 0.5138 - val_nse: 0.3944\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.34207\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.0558 - mean_absolute_error: 0.1752 - nse: 0.8303 - val_loss: 0.3491 - val_mean_absolute_error: 0.4852 - val_nse: 0.3833\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.34207\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0543 - mean_absolute_error: 0.1686 - nse: 0.8030 - val_loss: 0.3340 - val_mean_absolute_error: 0.4594 - val_nse: 0.4933\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.34207 to 0.33402, saving model to save/yeong/models/nitrogen/multi_conv.ckpt\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0507 - mean_absolute_error: 0.1636 - nse: 0.8357 - val_loss: 0.4163 - val_mean_absolute_error: 0.5176 - val_nse: 0.3991\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.33402\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.0644 - mean_absolute_error: 0.1863 - nse: 0.8323 - val_loss: 0.3596 - val_mean_absolute_error: 0.4743 - val_nse: 0.4466\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.33402\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0528 - mean_absolute_error: 0.1652 - nse: 0.7817 - val_loss: 0.3498 - val_mean_absolute_error: 0.4850 - val_nse: 0.4009\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.33402\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0492 - mean_absolute_error: 0.1629 - nse: 0.7907 - val_loss: 0.3307 - val_mean_absolute_error: 0.4547 - val_nse: 0.4571\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.33402 to 0.33070, saving model to save/yeong/models/nitrogen/multi_conv.ckpt\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.0499 - mean_absolute_error: 0.1648 - nse: 0.8196 - val_loss: 0.4453 - val_mean_absolute_error: 0.5384 - val_nse: 0.3904\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.33070\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0467 - mean_absolute_error: 0.1614 - nse: 0.8602 - val_loss: 0.4278 - val_mean_absolute_error: 0.5042 - val_nse: 0.4330\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.33070\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0557 - mean_absolute_error: 0.1739 - nse: 0.8279 - val_loss: 0.4479 - val_mean_absolute_error: 0.5286 - val_nse: 0.3875\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.33070\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0463 - mean_absolute_error: 0.1625 - nse: 0.8349 - val_loss: 0.3693 - val_mean_absolute_error: 0.4838 - val_nse: 0.3908\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.33070\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0501 - mean_absolute_error: 0.1657 - nse: 0.8186 - val_loss: 0.4387 - val_mean_absolute_error: 0.5407 - val_nse: 0.3306\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.33070\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0470 - mean_absolute_error: 0.1590 - nse: 0.8613 - val_loss: 0.4106 - val_mean_absolute_error: 0.5013 - val_nse: 0.4760\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.33070\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0453 - mean_absolute_error: 0.1498 - nse: 0.8580 - val_loss: 0.4021 - val_mean_absolute_error: 0.5011 - val_nse: 0.3616\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.33070\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.0476 - mean_absolute_error: 0.1616 - nse: 0.8604 - val_loss: 0.4402 - val_mean_absolute_error: 0.5322 - val_nse: 0.3190\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.33070\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0517 - mean_absolute_error: 0.1619 - nse: 0.8132 - val_loss: 0.4217 - val_mean_absolute_error: 0.5121 - val_nse: 0.3922\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.33070\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0431 - mean_absolute_error: 0.1530 - nse: 0.8793 - val_loss: 0.3799 - val_mean_absolute_error: 0.4732 - val_nse: 0.4050\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.33070\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0365 - mean_absolute_error: 0.1420 - nse: 0.8715 - val_loss: 0.3704 - val_mean_absolute_error: 0.4699 - val_nse: 0.4748\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.33070\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0492 - mean_absolute_error: 0.1612 - nse: 0.8514 - val_loss: 0.4105 - val_mean_absolute_error: 0.5069 - val_nse: 0.3846\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.33070\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0430 - mean_absolute_error: 0.1558 - nse: 0.8606 - val_loss: 0.4186 - val_mean_absolute_error: 0.5034 - val_nse: 0.3478\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.33070\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0358 - mean_absolute_error: 0.1396 - nse: 0.8610 - val_loss: 0.4458 - val_mean_absolute_error: 0.5305 - val_nse: 0.2990\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.33070\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0505 - mean_absolute_error: 0.1644 - nse: 0.8539 - val_loss: 0.4654 - val_mean_absolute_error: 0.5424 - val_nse: 0.3067\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.33070\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0355 - mean_absolute_error: 0.1409 - nse: 0.8527 - val_loss: 0.5657 - val_mean_absolute_error: 0.5967 - val_nse: 0.2419\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.33070\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0410 - mean_absolute_error: 0.1474 - nse: 0.8918 - val_loss: 0.4641 - val_mean_absolute_error: 0.5260 - val_nse: 0.3516\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.33070\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0409 - mean_absolute_error: 0.1463 - nse: 0.8577 - val_loss: 0.4284 - val_mean_absolute_error: 0.5073 - val_nse: 0.3511\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.33070\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0419 - mean_absolute_error: 0.1474 - nse: 0.8629 - val_loss: 0.5642 - val_mean_absolute_error: 0.5762 - val_nse: 0.2088\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.33070\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0395 - mean_absolute_error: 0.1459 - nse: 0.8701 - val_loss: 0.3957 - val_mean_absolute_error: 0.4914 - val_nse: 0.3351\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.33070\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0362 - mean_absolute_error: 0.1406 - nse: 0.8902 - val_loss: 0.5018 - val_mean_absolute_error: 0.5607 - val_nse: 0.2380\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.33070\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0412 - mean_absolute_error: 0.1488 - nse: 0.8743 - val_loss: 0.5421 - val_mean_absolute_error: 0.5802 - val_nse: 0.1120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00053: val_loss did not improve from 0.33070\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0346 - mean_absolute_error: 0.1397 - nse: 0.8788 - val_loss: 0.5684 - val_mean_absolute_error: 0.5838 - val_nse: 0.1955\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.33070\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0394 - mean_absolute_error: 0.1473 - nse: 0.8811 - val_loss: 0.5093 - val_mean_absolute_error: 0.5558 - val_nse: 0.2280\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.33070\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0440 - mean_absolute_error: 0.1542 - nse: 0.8598 - val_loss: 0.4776 - val_mean_absolute_error: 0.5434 - val_nse: 0.2743\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.33070\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.0369 - mean_absolute_error: 0.1425 - nse: 0.8539 - val_loss: 0.4827 - val_mean_absolute_error: 0.5250 - val_nse: 0.2252\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.33070\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0402 - mean_absolute_error: 0.1443 - nse: 0.8632 - val_loss: 0.4721 - val_mean_absolute_error: 0.5246 - val_nse: 0.2807\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.33070\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0372 - mean_absolute_error: 0.1450 - nse: 0.8711 - val_loss: 0.4722 - val_mean_absolute_error: 0.5345 - val_nse: 0.2786\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.33070\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.0435 - mean_absolute_error: 0.1517 - nse: 0.8729 - val_loss: 0.4549 - val_mean_absolute_error: 0.5305 - val_nse: 0.2126\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.33070\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0395 - mean_absolute_error: 0.1448 - nse: 0.8469 - val_loss: 0.5060 - val_mean_absolute_error: 0.5564 - val_nse: 0.2938\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.33070\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0363 - mean_absolute_error: 0.1387 - nse: 0.8943 - val_loss: 0.5784 - val_mean_absolute_error: 0.5872 - val_nse: 0.1493\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.33070\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0376 - mean_absolute_error: 0.1435 - nse: 0.8788 - val_loss: 0.4869 - val_mean_absolute_error: 0.5592 - val_nse: 0.1961\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.33070\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0394 - mean_absolute_error: 0.1443 - nse: 0.8717 - val_loss: 0.5326 - val_mean_absolute_error: 0.5541 - val_nse: 0.2131\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.33070\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0378 - mean_absolute_error: 0.1453 - nse: 0.8648 - val_loss: 0.5881 - val_mean_absolute_error: 0.5995 - val_nse: 0.1380\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.33070\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0414 - mean_absolute_error: 0.1508 - nse: 0.8934 - val_loss: 0.4844 - val_mean_absolute_error: 0.5482 - val_nse: 0.2046\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.33070\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0386 - mean_absolute_error: 0.1398 - nse: 0.8512 - val_loss: 0.5595 - val_mean_absolute_error: 0.5872 - val_nse: 0.1395\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.33070\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.0378 - mean_absolute_error: 0.1405 - nse: 0.8420 - val_loss: 0.5201 - val_mean_absolute_error: 0.5633 - val_nse: 0.1248\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.33070\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0393 - mean_absolute_error: 0.1454 - nse: 0.8652 - val_loss: 0.5332 - val_mean_absolute_error: 0.5632 - val_nse: 0.2218\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.33070\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0351 - mean_absolute_error: 0.1423 - nse: 0.8787 - val_loss: 0.6037 - val_mean_absolute_error: 0.5869 - val_nse: 0.2402\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.33070\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0372 - mean_absolute_error: 0.1417 - nse: 0.8936 - val_loss: 0.5151 - val_mean_absolute_error: 0.5485 - val_nse: 0.2408\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.33070\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0357 - mean_absolute_error: 0.1413 - nse: 0.8718 - val_loss: 0.5330 - val_mean_absolute_error: 0.5720 - val_nse: 0.1995\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.33070\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0337 - mean_absolute_error: 0.1333 - nse: 0.8756 - val_loss: 0.5100 - val_mean_absolute_error: 0.5618 - val_nse: 0.1938\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.33070\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0331 - mean_absolute_error: 0.1359 - nse: 0.8858 - val_loss: 0.6044 - val_mean_absolute_error: 0.6163 - val_nse: 0.1887\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.33070\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0345 - mean_absolute_error: 0.1340 - nse: 0.9016 - val_loss: 0.5926 - val_mean_absolute_error: 0.5863 - val_nse: 0.1507\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.33070\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0372 - mean_absolute_error: 0.1424 - nse: 0.8992 - val_loss: 0.5947 - val_mean_absolute_error: 0.6049 - val_nse: 0.0405\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.33070\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0369 - mean_absolute_error: 0.1395 - nse: 0.8663 - val_loss: 0.6286 - val_mean_absolute_error: 0.6260 - val_nse: 0.0922\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.33070\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0350 - mean_absolute_error: 0.1386 - nse: 0.8989 - val_loss: 0.5816 - val_mean_absolute_error: 0.5799 - val_nse: 0.0671\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.33070\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.0286 - mean_absolute_error: 0.1280 - nse: 0.8999 - val_loss: 0.4723 - val_mean_absolute_error: 0.5295 - val_nse: 0.2956\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.33070\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0339 - mean_absolute_error: 0.1353 - nse: 0.8878 - val_loss: 0.6208 - val_mean_absolute_error: 0.6049 - val_nse: 0.1686\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.33070\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0347 - mean_absolute_error: 0.1372 - nse: 0.8886 - val_loss: 0.4471 - val_mean_absolute_error: 0.5089 - val_nse: 0.2286\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.33070\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0312 - mean_absolute_error: 0.1310 - nse: 0.8960 - val_loss: 0.5467 - val_mean_absolute_error: 0.5796 - val_nse: 0.1414\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.33070\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0353 - mean_absolute_error: 0.1365 - nse: 0.8779 - val_loss: 0.5694 - val_mean_absolute_error: 0.5893 - val_nse: 0.1118\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.33070\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0320 - mean_absolute_error: 0.1337 - nse: 0.8858 - val_loss: 0.6204 - val_mean_absolute_error: 0.6155 - val_nse: 0.1880\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.33070\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0316 - mean_absolute_error: 0.1304 - nse: 0.9153 - val_loss: 0.6214 - val_mean_absolute_error: 0.6011 - val_nse: 0.1160\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.33070\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0323 - mean_absolute_error: 0.1313 - nse: 0.8944 - val_loss: 0.6729 - val_mean_absolute_error: 0.6267 - val_nse: 0.0547\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.33070\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 240ms/step - loss: 0.0375 - mean_absolute_error: 0.1400 - nse: 0.8722 - val_loss: 0.7063 - val_mean_absolute_error: 0.6309 - val_nse: -0.0807\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.33070\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0311 - mean_absolute_error: 0.1295 - nse: 0.8883 - val_loss: 0.5581 - val_mean_absolute_error: 0.5722 - val_nse: 0.1236\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.33070\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0342 - mean_absolute_error: 0.1361 - nse: 0.9057 - val_loss: 0.6439 - val_mean_absolute_error: 0.6234 - val_nse: -0.0451\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.33070\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.0284 - mean_absolute_error: 0.1256 - nse: 0.9083 - val_loss: 0.6577 - val_mean_absolute_error: 0.6349 - val_nse: 0.0214\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.33070\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0330 - mean_absolute_error: 0.1341 - nse: 0.8860 - val_loss: 0.6366 - val_mean_absolute_error: 0.6268 - val_nse: -0.0125\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.33070\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0276 - mean_absolute_error: 0.1248 - nse: 0.9090 - val_loss: 0.6498 - val_mean_absolute_error: 0.6117 - val_nse: 0.0961\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.33070\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0363 - mean_absolute_error: 0.1411 - nse: 0.8907 - val_loss: 0.6188 - val_mean_absolute_error: 0.6189 - val_nse: 0.0307\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.33070\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0325 - mean_absolute_error: 0.1328 - nse: 0.8714 - val_loss: 0.5907 - val_mean_absolute_error: 0.5765 - val_nse: 0.0823\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.33070\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0324 - mean_absolute_error: 0.1307 - nse: 0.8876 - val_loss: 0.6430 - val_mean_absolute_error: 0.6023 - val_nse: -6.3455e-04\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.33070\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0322 - mean_absolute_error: 0.1348 - nse: 0.8814 - val_loss: 0.5627 - val_mean_absolute_error: 0.5803 - val_nse: 0.1244\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.33070\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0358 - mean_absolute_error: 0.1387 - nse: 0.9009 - val_loss: 0.6086 - val_mean_absolute_error: 0.6210 - val_nse: 0.0091\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.33070\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0328 - mean_absolute_error: 0.1327 - nse: 0.8819 - val_loss: 0.5087 - val_mean_absolute_error: 0.5460 - val_nse: 0.1791\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.33070\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0302 - mean_absolute_error: 0.1293 - nse: 0.9073 - val_loss: 0.7194 - val_mean_absolute_error: 0.6609 - val_nse: 0.0657\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.33070\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0333 - mean_absolute_error: 0.1343 - nse: 0.8873 - val_loss: 0.6172 - val_mean_absolute_error: 0.6287 - val_nse: -0.0486\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.33070\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0314 - mean_absolute_error: 0.1321 - nse: 0.8958 - val_loss: 0.5261 - val_mean_absolute_error: 0.5457 - val_nse: 0.1191\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.33070\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0331 - mean_absolute_error: 0.1339 - nse: 0.8969 - val_loss: 0.6793 - val_mean_absolute_error: 0.6287 - val_nse: -0.0190\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.33070\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0301 - mean_absolute_error: 0.1311 - nse: 0.9052 - val_loss: 0.7411 - val_mean_absolute_error: 0.6798 - val_nse: -0.0912\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.33070\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0361 - mean_absolute_error: 0.1373 - nse: 0.8972 - val_loss: 0.6695 - val_mean_absolute_error: 0.6309 - val_nse: -0.0772\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.33070\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0300 - mean_absolute_error: 0.1287 - nse: 0.8706 - val_loss: 0.6760 - val_mean_absolute_error: 0.6467 - val_nse: -0.0693\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.33070\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0321 - mean_absolute_error: 0.1293 - nse: 0.8851 - val_loss: 0.5542 - val_mean_absolute_error: 0.5746 - val_nse: 0.0947\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.33070\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0305 - mean_absolute_error: 0.1318 - nse: 0.8752 - val_loss: 0.8236 - val_mean_absolute_error: 0.7014 - val_nse: -0.0879\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.33070\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0280 - mean_absolute_error: 0.1273 - nse: 0.9265 - val_loss: 0.6527 - val_mean_absolute_error: 0.5932 - val_nse: 0.0800\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.33070\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0338 - mean_absolute_error: 0.1337 - nse: 0.8784 - val_loss: 0.6767 - val_mean_absolute_error: 0.6521 - val_nse: -0.1053\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.33070\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0298 - mean_absolute_error: 0.1272 - nse: 0.8970 - val_loss: 0.6476 - val_mean_absolute_error: 0.6142 - val_nse: 0.0459\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.33070\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0303 - mean_absolute_error: 0.1278 - nse: 0.8896 - val_loss: 0.7286 - val_mean_absolute_error: 0.6805 - val_nse: -0.0067\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.33070\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0291 - mean_absolute_error: 0.1272 - nse: 0.9184 - val_loss: 0.6935 - val_mean_absolute_error: 0.6251 - val_nse: 0.0200\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.33070\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.0344 - mean_absolute_error: 0.1359 - nse: 0.9073 - val_loss: 0.7034 - val_mean_absolute_error: 0.6562 - val_nse: -0.1281\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.33070\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0285 - mean_absolute_error: 0.1249 - nse: 0.8842 - val_loss: 0.8567 - val_mean_absolute_error: 0.7326 - val_nse: -0.2199\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.33070\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0335 - mean_absolute_error: 0.1333 - nse: 0.9059 - val_loss: 0.7662 - val_mean_absolute_error: 0.6684 - val_nse: -0.2330\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.33070\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0252 - mean_absolute_error: 0.1202 - nse: 0.9137 - val_loss: 0.5546 - val_mean_absolute_error: 0.5663 - val_nse: 0.1279\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.33070\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0273 - mean_absolute_error: 0.1240 - nse: 0.9063 - val_loss: 0.7597 - val_mean_absolute_error: 0.6690 - val_nse: -0.0025\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.33070\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0300 - mean_absolute_error: 0.1283 - nse: 0.9131 - val_loss: 0.5908 - val_mean_absolute_error: 0.5776 - val_nse: 0.0416\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.33070\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0282 - mean_absolute_error: 0.1221 - nse: 0.8986 - val_loss: 0.6196 - val_mean_absolute_error: 0.6062 - val_nse: 0.0345\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.33070\n",
      "Epoch 120/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 235ms/step - loss: 0.0285 - mean_absolute_error: 0.1246 - nse: 0.9014 - val_loss: 0.7545 - val_mean_absolute_error: 0.6797 - val_nse: -0.1744\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.33070\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0289 - mean_absolute_error: 0.1254 - nse: 0.9048 - val_loss: 0.7442 - val_mean_absolute_error: 0.6550 - val_nse: -0.0245\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.33070\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0291 - mean_absolute_error: 0.1234 - nse: 0.8877 - val_loss: 0.8354 - val_mean_absolute_error: 0.6883 - val_nse: -0.1375\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.33070\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.0261 - mean_absolute_error: 0.1200 - nse: 0.9309 - val_loss: 0.8035 - val_mean_absolute_error: 0.6715 - val_nse: -0.1182\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.33070\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0312 - mean_absolute_error: 0.1288 - nse: 0.9037 - val_loss: 0.6830 - val_mean_absolute_error: 0.6303 - val_nse: -0.1616\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.33070\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0295 - mean_absolute_error: 0.1270 - nse: 0.8880 - val_loss: 0.8750 - val_mean_absolute_error: 0.7067 - val_nse: -0.1448\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.33070\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0257 - mean_absolute_error: 0.1214 - nse: 0.9224 - val_loss: 0.6280 - val_mean_absolute_error: 0.6104 - val_nse: -0.0962\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.33070\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0270 - mean_absolute_error: 0.1211 - nse: 0.9176 - val_loss: 0.7740 - val_mean_absolute_error: 0.6844 - val_nse: -0.1347\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.33070\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0268 - mean_absolute_error: 0.1224 - nse: 0.9121 - val_loss: 0.8538 - val_mean_absolute_error: 0.7163 - val_nse: -0.3523\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.33070\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0239 - mean_absolute_error: 0.1164 - nse: 0.9196 - val_loss: 0.7675 - val_mean_absolute_error: 0.6689 - val_nse: -0.1529\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.33070\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0290 - mean_absolute_error: 0.1255 - nse: 0.9114 - val_loss: 0.8122 - val_mean_absolute_error: 0.6900 - val_nse: -0.1772\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.33070\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.0284 - mean_absolute_error: 0.1269 - nse: 0.9029 - val_loss: 0.7138 - val_mean_absolute_error: 0.6345 - val_nse: -0.1314\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.33070\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.0306 - mean_absolute_error: 0.1253 - nse: 0.8891 - val_loss: 0.7731 - val_mean_absolute_error: 0.6609 - val_nse: -0.1855\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.33070\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.0278 - mean_absolute_error: 0.1260 - nse: 0.9041 - val_loss: 0.6170 - val_mean_absolute_error: 0.5977 - val_nse: 0.0133\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.33070\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0249 - mean_absolute_error: 0.1200 - nse: 0.9188 - val_loss: 0.6966 - val_mean_absolute_error: 0.6414 - val_nse: -0.1317\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.33070\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0302 - mean_absolute_error: 0.1259 - nse: 0.9029 - val_loss: 0.6492 - val_mean_absolute_error: 0.6157 - val_nse: -0.0690\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.33070\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0286 - mean_absolute_error: 0.1240 - nse: 0.9026 - val_loss: 0.8089 - val_mean_absolute_error: 0.6884 - val_nse: -0.1137\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.33070\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0262 - mean_absolute_error: 0.1230 - nse: 0.9201 - val_loss: 0.8513 - val_mean_absolute_error: 0.7282 - val_nse: -0.3298\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.33070\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0300 - mean_absolute_error: 0.1275 - nse: 0.9020 - val_loss: 0.6536 - val_mean_absolute_error: 0.6120 - val_nse: -0.0652\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.33070\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0279 - mean_absolute_error: 0.1242 - nse: 0.8996 - val_loss: 0.7742 - val_mean_absolute_error: 0.6669 - val_nse: -0.1740\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.33070\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0259 - mean_absolute_error: 0.1206 - nse: 0.9181 - val_loss: 0.8753 - val_mean_absolute_error: 0.7334 - val_nse: -0.3135\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.33070\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0279 - mean_absolute_error: 0.1245 - nse: 0.9250 - val_loss: 0.7261 - val_mean_absolute_error: 0.6455 - val_nse: -0.1037\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.33070\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0284 - mean_absolute_error: 0.1226 - nse: 0.8927 - val_loss: 0.7292 - val_mean_absolute_error: 0.6681 - val_nse: -0.2261\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.33070\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0274 - mean_absolute_error: 0.1212 - nse: 0.8849 - val_loss: 0.6454 - val_mean_absolute_error: 0.6138 - val_nse: -0.0827\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.33070\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0325 - mean_absolute_error: 0.1337 - nse: 0.8727 - val_loss: 1.0043 - val_mean_absolute_error: 0.7790 - val_nse: -0.3767\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.33070\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.0249 - mean_absolute_error: 0.1188 - nse: 0.9252 - val_loss: 0.9064 - val_mean_absolute_error: 0.6998 - val_nse: -0.1646\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.33070\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.0301 - mean_absolute_error: 0.1298 - nse: 0.9158 - val_loss: 0.8513 - val_mean_absolute_error: 0.7120 - val_nse: -0.2604\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.33070\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0272 - mean_absolute_error: 0.1215 - nse: 0.8948 - val_loss: 0.6855 - val_mean_absolute_error: 0.6322 - val_nse: -0.0782\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.33070\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0277 - mean_absolute_error: 0.1218 - nse: 0.8943 - val_loss: 0.7883 - val_mean_absolute_error: 0.6835 - val_nse: -0.1733\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.33070\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.0286 - mean_absolute_error: 0.1253 - nse: 0.9172 - val_loss: 0.8541 - val_mean_absolute_error: 0.6961 - val_nse: -0.1530\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.33070\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0309 - mean_absolute_error: 0.1271 - nse: 0.9141 - val_loss: 0.8269 - val_mean_absolute_error: 0.6944 - val_nse: -0.2961\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.33070\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.0256 - mean_absolute_error: 0.1215 - nse: 0.9150 - val_loss: 1.0130 - val_mean_absolute_error: 0.7761 - val_nse: -0.5800\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.33070\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.0328 - mean_absolute_error: 0.1287 - nse: 0.8984 - val_loss: 0.9326 - val_mean_absolute_error: 0.7375 - val_nse: -0.3401\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.33070\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0222 - mean_absolute_error: 0.1140 - nse: 0.9311 - val_loss: 0.6990 - val_mean_absolute_error: 0.6167 - val_nse: -0.0736\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.33070\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.0233 - mean_absolute_error: 0.1155 - nse: 0.9169 - val_loss: 0.7632 - val_mean_absolute_error: 0.6669 - val_nse: -0.0799\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.33070\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0286 - mean_absolute_error: 0.1231 - nse: 0.9155 - val_loss: 0.8093 - val_mean_absolute_error: 0.6779 - val_nse: -0.2042\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.33070\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0253 - mean_absolute_error: 0.1185 - nse: 0.9178 - val_loss: 0.6654 - val_mean_absolute_error: 0.6138 - val_nse: -0.0527\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.33070\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0238 - mean_absolute_error: 0.1118 - nse: 0.9028 - val_loss: 0.7983 - val_mean_absolute_error: 0.6750 - val_nse: -0.2890\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.33070\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0272 - mean_absolute_error: 0.1219 - nse: 0.9208 - val_loss: 0.8810 - val_mean_absolute_error: 0.7074 - val_nse: -0.2623\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.33070\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0248 - mean_absolute_error: 0.1158 - nse: 0.9020 - val_loss: 0.9388 - val_mean_absolute_error: 0.7324 - val_nse: -0.3432\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.33070\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0249 - mean_absolute_error: 0.1143 - nse: 0.9339 - val_loss: 0.8153 - val_mean_absolute_error: 0.6735 - val_nse: -0.1084\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.33070\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0276 - mean_absolute_error: 0.1204 - nse: 0.9021 - val_loss: 0.7793 - val_mean_absolute_error: 0.6596 - val_nse: -0.1511\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.33070\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0254 - mean_absolute_error: 0.1201 - nse: 0.9160 - val_loss: 0.9850 - val_mean_absolute_error: 0.7506 - val_nse: -0.3878\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.33070\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0244 - mean_absolute_error: 0.1159 - nse: 0.9188 - val_loss: 0.7240 - val_mean_absolute_error: 0.6536 - val_nse: -0.2216\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.33070\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0231 - mean_absolute_error: 0.1137 - nse: 0.9324 - val_loss: 0.8999 - val_mean_absolute_error: 0.7300 - val_nse: -0.3491\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.33070\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0262 - mean_absolute_error: 0.1191 - nse: 0.9175 - val_loss: 0.8200 - val_mean_absolute_error: 0.6907 - val_nse: -0.3966\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.33070\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0210 - mean_absolute_error: 0.1098 - nse: 0.9258 - val_loss: 0.9232 - val_mean_absolute_error: 0.7154 - val_nse: -0.2882\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.33070\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0243 - mean_absolute_error: 0.1148 - nse: 0.9292 - val_loss: 0.9214 - val_mean_absolute_error: 0.7352 - val_nse: -0.3613\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.33070\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0264 - mean_absolute_error: 0.1227 - nse: 0.9153 - val_loss: 0.7193 - val_mean_absolute_error: 0.6369 - val_nse: -0.1848\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.33070\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0262 - mean_absolute_error: 0.1182 - nse: 0.8943 - val_loss: 0.8223 - val_mean_absolute_error: 0.6657 - val_nse: -0.2117\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.33070\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0251 - mean_absolute_error: 0.1171 - nse: 0.9148 - val_loss: 0.7632 - val_mean_absolute_error: 0.6538 - val_nse: -0.1982\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.33070\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.0244 - mean_absolute_error: 0.1182 - nse: 0.9220 - val_loss: 0.7631 - val_mean_absolute_error: 0.6580 - val_nse: -0.1709\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.33070\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0259 - mean_absolute_error: 0.1181 - nse: 0.9186 - val_loss: 0.8064 - val_mean_absolute_error: 0.6905 - val_nse: -0.3892\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.33070\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.0260 - mean_absolute_error: 0.1175 - nse: 0.9126 - val_loss: 0.9461 - val_mean_absolute_error: 0.7387 - val_nse: -0.2923\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.33070\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.0237 - mean_absolute_error: 0.1162 - nse: 0.9231 - val_loss: 0.8674 - val_mean_absolute_error: 0.7120 - val_nse: -0.2880\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.33070\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 0.0234 - mean_absolute_error: 0.1145 - nse: 0.9254 - val_loss: 0.7987 - val_mean_absolute_error: 0.6823 - val_nse: -0.4129\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.33070\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0267 - mean_absolute_error: 0.1180 - nse: 0.9107 - val_loss: 0.7266 - val_mean_absolute_error: 0.6334 - val_nse: -0.0936\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.33070\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0239 - mean_absolute_error: 0.1165 - nse: 0.9165 - val_loss: 0.9496 - val_mean_absolute_error: 0.7560 - val_nse: -0.3972\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.33070\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0243 - mean_absolute_error: 0.1185 - nse: 0.9377 - val_loss: 0.7419 - val_mean_absolute_error: 0.6466 - val_nse: -0.1787\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.33070\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0264 - mean_absolute_error: 0.1144 - nse: 0.8899 - val_loss: 0.9123 - val_mean_absolute_error: 0.7447 - val_nse: -0.5060\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.33070\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0254 - mean_absolute_error: 0.1173 - nse: 0.9020 - val_loss: 0.7779 - val_mean_absolute_error: 0.6720 - val_nse: -0.2606\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.33070\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0274 - mean_absolute_error: 0.1207 - nse: 0.9015 - val_loss: 0.9694 - val_mean_absolute_error: 0.7456 - val_nse: -0.3691\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.33070\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0231 - mean_absolute_error: 0.1145 - nse: 0.9262 - val_loss: 0.9837 - val_mean_absolute_error: 0.7314 - val_nse: -0.2646\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.33070\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.0251 - mean_absolute_error: 0.1183 - nse: 0.9240 - val_loss: 0.9140 - val_mean_absolute_error: 0.7166 - val_nse: -0.3038\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.33070\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0264 - mean_absolute_error: 0.1200 - nse: 0.9086 - val_loss: 0.7910 - val_mean_absolute_error: 0.6765 - val_nse: -0.2313\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.33070\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0240 - mean_absolute_error: 0.1130 - nse: 0.9111 - val_loss: 0.8220 - val_mean_absolute_error: 0.6884 - val_nse: -0.2759\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.33070\n",
      "Epoch 186/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0265 - mean_absolute_error: 0.1188 - nse: 0.9065 - val_loss: 0.9199 - val_mean_absolute_error: 0.7181 - val_nse: -0.2191\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.33070\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0243 - mean_absolute_error: 0.1138 - nse: 0.9341 - val_loss: 1.0111 - val_mean_absolute_error: 0.7600 - val_nse: -0.4793\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.33070\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0251 - mean_absolute_error: 0.1199 - nse: 0.9309 - val_loss: 1.0239 - val_mean_absolute_error: 0.7749 - val_nse: -0.5611\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.33070\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0269 - mean_absolute_error: 0.1171 - nse: 0.8957 - val_loss: 1.0257 - val_mean_absolute_error: 0.7877 - val_nse: -0.5784\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.33070\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0238 - mean_absolute_error: 0.1137 - nse: 0.9294 - val_loss: 0.8362 - val_mean_absolute_error: 0.6760 - val_nse: -0.3088\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.33070\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 0.0213 - mean_absolute_error: 0.1105 - nse: 0.9307 - val_loss: 0.7404 - val_mean_absolute_error: 0.6408 - val_nse: -0.0701\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.33070\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0249 - mean_absolute_error: 0.1153 - nse: 0.9193 - val_loss: 0.9277 - val_mean_absolute_error: 0.7327 - val_nse: -0.3273\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.33070\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0235 - mean_absolute_error: 0.1130 - nse: 0.9232 - val_loss: 0.6703 - val_mean_absolute_error: 0.6056 - val_nse: -0.0943\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.33070\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0220 - mean_absolute_error: 0.1093 - nse: 0.9203 - val_loss: 0.8783 - val_mean_absolute_error: 0.7214 - val_nse: -0.3908\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.33070\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.0253 - mean_absolute_error: 0.1158 - nse: 0.9280 - val_loss: 0.8266 - val_mean_absolute_error: 0.6751 - val_nse: -0.2699\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.33070\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0217 - mean_absolute_error: 0.1095 - nse: 0.9072 - val_loss: 1.0168 - val_mean_absolute_error: 0.7514 - val_nse: -0.3097\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.33070\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.0235 - mean_absolute_error: 0.1117 - nse: 0.9355 - val_loss: 0.9268 - val_mean_absolute_error: 0.7208 - val_nse: -0.3309\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.33070\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0238 - mean_absolute_error: 0.1120 - nse: 0.9218 - val_loss: 0.8689 - val_mean_absolute_error: 0.6922 - val_nse: -0.2800\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.33070\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.0243 - mean_absolute_error: 0.1143 - nse: 0.9202 - val_loss: 0.9769 - val_mean_absolute_error: 0.7395 - val_nse: -0.3852\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.33070\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0229 - mean_absolute_error: 0.1110 - nse: 0.9142 - val_loss: 0.7833 - val_mean_absolute_error: 0.6659 - val_nse: -0.1841\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.33070\n"
     ]
    }
   ],
   "source": [
    "rnn_epochs = 200\n",
    "# multi_linear_model = model_multi_linear(\n",
    "#     window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "#     #training_flag=__RNN_TRAINING__, checkpoint_path=\"save/\"+watershed+\"models/multi_linear.ckpt\")\n",
    "#     training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"multi_linear.ckpt\")\n",
    "# elman_model = model_elman(\n",
    "#     window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "#     training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"elman.ckpt\")\n",
    "gru_model = model_gru(\n",
    "    window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "    training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"gru.ckpt\")\n",
    "multi_lstm_model = model_multi_lstm(\n",
    "    window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "    training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"multi_lstm.ckpt\")\n",
    "multi_conv_model = model_multi_conv(\n",
    "    window=multi_window, OUT_STEPS=rnn_out_steps, out_num_features=out_num_features, epochs=rnn_epochs,\n",
    "    training_flag=__RNN_TRAINING__, checkpoint_path=model_path+\"multi_conv.ckpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## core / window.py / "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_to_day_mean(array):\n",
    "    time = 24\n",
    "    array = array.reshape((array.shape[0], array.shape[1] // time, time, array.shape[2]))\n",
    "    array = array.mean(2)\n",
    "    return array\n",
    "\n",
    "def compa(model=None,df = None, plot_col=0, input_width=7*24, label_width=5*24, target_std=None, target_mean=None, predict_day=4):\n",
    "    \n",
    "    print(df.shape)\n",
    "    print(plot_col)\n",
    "    \n",
    "    width = input_width + label_width\n",
    "    \n",
    "    length = df.shape[0]\n",
    "    length -= width\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "   \n",
    "    for i in range(0,length,24):\n",
    "#         print('i = ', i)\n",
    "        dataset = df.iloc[i:i+width].to_numpy()\n",
    "        input = dataset[:input_width]\n",
    "        label = dataset[input_width:, plot_col:plot_col+1]\n",
    "        \n",
    "        input = input.reshape((-1,)+input.shape)\n",
    "        label = label.reshape((-1,)+label.shape)\n",
    "        \n",
    "        inputs.append(input)\n",
    "        labels.append(label)\n",
    "        \n",
    "    inputs = np.concatenate(inputs, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "#     print('labels.mean(axis=1)')\n",
    "#     print(labels.mean(axis=1).shape)\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "\n",
    "    predictions = model(inputs)\n",
    "    print(predictions.shape)\n",
    "    \n",
    "    predictions = predictions.numpy() * target_std[plot_col] + target_mean[plot_col]\n",
    "    labels = labels * target_std[plot_col] + target_mean[plot_col]\n",
    "\n",
    "    print('predictions.shape =', predictions.shape)\n",
    "#     pred_day = predictions\n",
    "    pred_day = hour_to_day_mean(predictions)\n",
    "    print('pred_day.shape =', pred_day.shape)\n",
    "#     label_day= labels\n",
    "    label_day = hour_to_day_mean(labels)\n",
    "    \n",
    "    inputs_target = inputs[:,:,plot_col:plot_col+1]\n",
    "    inputs_target = inputs_target * target_std[plot_col] + target_mean[plot_col]\n",
    "#     inputs_day = inputs_target\n",
    "    inputs_day = hour_to_day_mean(inputs_target)\n",
    "    \n",
    "    #plt.figure(figsize=(10, 800))\n",
    "    \n",
    "#     input_index = np.array(range(0,length,24))\n",
    "#     label_index = input_index + 24*7\n",
    "    \n",
    "#     print(input_index)\n",
    "#     print(label_index)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(label_day[:, predict_day, :], label='label')\n",
    "    plt.plot(pred_day[:, predict_day, :], label='pred')\n",
    "#     plt.plot(input_index, inputs_day[:, 0, :], label='input')\n",
    "#     plt.plot(label_index, label_day[:, 0, :], label='label')\n",
    "#     plt.plot(label_index, pred_day[:, 0, :], label='pred')\n",
    "    plt.legend()\n",
    "        \n",
    "        \n",
    "        \n",
    "#     o1 = np.mean(labels)\n",
    "# #     print('labels =', labels)\n",
    "#     print('o1 =', o1)\n",
    "#     nse1 = ((labels - predictions)**2).sum()\n",
    "#     nse2 = ((labels - o1)**2).sum()\n",
    "#     nse3 = 1 - (nse1/nse2)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print('pred_day.shape =',pred_day.shape)\n",
    "#     print('label_day.shape =',label_day.shape)\n",
    "    #print('label_index =', label_index)\n",
    "    \n",
    "    o1 = np.mean(label_day[:,predict_day,0])\n",
    "#     o1 = np.mean(label_day)\n",
    "#     print('labels =', labels)\n",
    "#     o1 = target_mean[plot_col]\n",
    "    print('o1 =', o1)\n",
    "    nse1 = ((pred_day-label_day)**2).sum(axis=0)\n",
    "    nse2 = ((label_day - o1)**2).sum(axis=0)\n",
    "    nse3 = 1 - (nse1[predict_day]/nse2[predict_day])\n",
    "    print('nse3 =', nse3)\n",
    "    #mean = np.mean(label_day[predict_day])\n",
    "    print('label_day[predict_day].shape =', label_day[predict_day].shape)\n",
    "#     print(type(pred_day))\n",
    "#     print(pred_day[:20, 0, 0])\n",
    "#     print(label_day[:20, 0, 0])\n",
    "    o = label_day[:, 0, 0]\n",
    "    s = pred_day[:, 0, 0]\n",
    "    mean = np.mean(o)\n",
    "    print('mean =', mean)\n",
    "    nse = 1. - np.sum((o-s)**2)/np.sum((o-mean)**2)\n",
    "#     print('nse =', nse)\n",
    "    \n",
    "    \n",
    "#     pbias1 = (label_day - pred_day).sum(axis=0)\n",
    "#     pbias2 = (label_day).sum(axis=0)\n",
    "#     pbias3 = (pbias1[predict_day]/pbias2[predict_day])*100\n",
    "    \n",
    "    \n",
    "#     o1 = np.mean(label_day)\n",
    "# #     print('labels =', labels)\n",
    "#     print('o1 =', o1)\n",
    "#     nse1 = ((label_day - pred_day)**2).sum()\n",
    "#     nse2 = ((label_day - o1)**2).sum()\n",
    "#     nse3 = 1 - (nse1/nse2)\n",
    "#     print('nse3 =', nse3)\n",
    "    #mean = np.mean(label_day[predict_day])\n",
    "#     print(type(pred_day))\n",
    "#     print(pred_day[:20, 0, 0])\n",
    "#     print(label_day[:20, 0, 0])\n",
    "#     o = label_day[:, 0, 0]\n",
    "#     s = pred_day[:, 0, 0]\n",
    "#     mean = np.mean(o)\n",
    "#     print('mean =', mean)\n",
    "#     nse = 1. - np.sum((o-s)**2)/np.sum((o-mean)**2)\n",
    "#     print('nse =', nse)\n",
    "    \n",
    "    \n",
    "    pbias1 = (label_day - pred_day).sum(axis=0)\n",
    "    pbias2 = (label_day).sum(axis=0)\n",
    "    pbias3 = (pbias1[predict_day]/pbias2[predict_day])*100\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return nse3, np.abs(pbias3), pred_day, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.506584</td>\n",
       "      <td>-0.781585</td>\n",
       "      <td>0.609397</td>\n",
       "      <td>1.684159</td>\n",
       "      <td>-0.388460</td>\n",
       "      <td>0.914476</td>\n",
       "      <td>-0.137610</td>\n",
       "      <td>-0.525071</td>\n",
       "      <td>1.747686e-12</td>\n",
       "      <td>1.414210</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>1.413681</td>\n",
       "      <td>-1.689783</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>0.994299</td>\n",
       "      <td>-0.463754</td>\n",
       "      <td>-1.254349</td>\n",
       "      <td>0.019940</td>\n",
       "      <td>-1.432135</td>\n",
       "      <td>-0.901498</td>\n",
       "      <td>1.747686e-12</td>\n",
       "      <td>1.414210</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>1.413681</td>\n",
       "      <td>-1.356208</td>\n",
       "      <td>-0.588429</td>\n",
       "      <td>0.154454</td>\n",
       "      <td>1.470927</td>\n",
       "      <td>-1.231157</td>\n",
       "      <td>1.157017</td>\n",
       "      <td>-0.740015</td>\n",
       "      <td>-0.865323</td>\n",
       "      <td>1.747686e-12</td>\n",
       "      <td>1.414210</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>1.413681</td>\n",
       "      <td>-1.507453</td>\n",
       "      <td>0.396912</td>\n",
       "      <td>0.801488</td>\n",
       "      <td>-0.042523</td>\n",
       "      <td>-1.445851</td>\n",
       "      <td>-0.430887</td>\n",
       "      <td>-0.021002</td>\n",
       "      <td>-0.837544</td>\n",
       "      <td>1.747686e-12</td>\n",
       "      <td>1.414210</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>1.413681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.510089</td>\n",
       "      <td>-0.797194</td>\n",
       "      <td>0.583524</td>\n",
       "      <td>1.684159</td>\n",
       "      <td>-0.339509</td>\n",
       "      <td>0.940824</td>\n",
       "      <td>-0.116095</td>\n",
       "      <td>-0.537748</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>1.366022</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>1.413679</td>\n",
       "      <td>-1.703138</td>\n",
       "      <td>-0.056581</td>\n",
       "      <td>0.980826</td>\n",
       "      <td>-0.440851</td>\n",
       "      <td>-1.252129</td>\n",
       "      <td>0.066557</td>\n",
       "      <td>-1.446334</td>\n",
       "      <td>-0.901628</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>1.366022</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>1.413679</td>\n",
       "      <td>-1.367382</td>\n",
       "      <td>-0.588860</td>\n",
       "      <td>0.147153</td>\n",
       "      <td>1.470137</td>\n",
       "      <td>-1.172248</td>\n",
       "      <td>1.150839</td>\n",
       "      <td>-0.761529</td>\n",
       "      <td>-0.863685</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>1.366022</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>1.413679</td>\n",
       "      <td>-1.530273</td>\n",
       "      <td>0.212962</td>\n",
       "      <td>0.685987</td>\n",
       "      <td>-0.009650</td>\n",
       "      <td>-1.419883</td>\n",
       "      <td>-0.440716</td>\n",
       "      <td>-0.030038</td>\n",
       "      <td>-0.838586</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>1.366022</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>1.413679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.514180</td>\n",
       "      <td>-0.801723</td>\n",
       "      <td>0.557911</td>\n",
       "      <td>1.679321</td>\n",
       "      <td>-0.397348</td>\n",
       "      <td>0.916532</td>\n",
       "      <td>-0.116095</td>\n",
       "      <td>-0.548444</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>1.224742</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>1.413677</td>\n",
       "      <td>-1.721294</td>\n",
       "      <td>-0.082758</td>\n",
       "      <td>0.978333</td>\n",
       "      <td>-0.447071</td>\n",
       "      <td>-1.250806</td>\n",
       "      <td>0.057947</td>\n",
       "      <td>-1.503778</td>\n",
       "      <td>-0.900590</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>1.224742</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>1.413677</td>\n",
       "      <td>-1.375527</td>\n",
       "      <td>-0.588900</td>\n",
       "      <td>0.148847</td>\n",
       "      <td>1.459278</td>\n",
       "      <td>-1.273414</td>\n",
       "      <td>1.088776</td>\n",
       "      <td>-0.818973</td>\n",
       "      <td>-0.870471</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>1.224742</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>1.413677</td>\n",
       "      <td>-1.551150</td>\n",
       "      <td>0.065689</td>\n",
       "      <td>0.593695</td>\n",
       "      <td>-0.020410</td>\n",
       "      <td>-1.389266</td>\n",
       "      <td>-0.424052</td>\n",
       "      <td>-0.042516</td>\n",
       "      <td>-0.839976</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>1.224742</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>1.413677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.515415</td>\n",
       "      <td>-0.815917</td>\n",
       "      <td>0.552050</td>\n",
       "      <td>1.673497</td>\n",
       "      <td>-0.220514</td>\n",
       "      <td>0.915128</td>\n",
       "      <td>-0.116095</td>\n",
       "      <td>-0.551085</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>1.413674</td>\n",
       "      <td>-1.736147</td>\n",
       "      <td>-0.104176</td>\n",
       "      <td>0.983347</td>\n",
       "      <td>-0.488828</td>\n",
       "      <td>-1.255655</td>\n",
       "      <td>0.106997</td>\n",
       "      <td>-1.514535</td>\n",
       "      <td>-0.899293</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>1.413674</td>\n",
       "      <td>-1.381628</td>\n",
       "      <td>-0.588456</td>\n",
       "      <td>0.155937</td>\n",
       "      <td>1.467275</td>\n",
       "      <td>-1.280091</td>\n",
       "      <td>1.089618</td>\n",
       "      <td>-0.847587</td>\n",
       "      <td>-0.868599</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>1.413674</td>\n",
       "      <td>-1.573655</td>\n",
       "      <td>-0.036403</td>\n",
       "      <td>0.538223</td>\n",
       "      <td>-0.068585</td>\n",
       "      <td>-1.483433</td>\n",
       "      <td>-0.405517</td>\n",
       "      <td>-0.051552</td>\n",
       "      <td>-0.839976</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>1.413674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.518370</td>\n",
       "      <td>-0.796628</td>\n",
       "      <td>0.560990</td>\n",
       "      <td>1.676459</td>\n",
       "      <td>-0.406037</td>\n",
       "      <td>0.915223</td>\n",
       "      <td>-0.094581</td>\n",
       "      <td>-0.542766</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>0.707105</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>1.413669</td>\n",
       "      <td>-1.745111</td>\n",
       "      <td>-0.117089</td>\n",
       "      <td>0.990270</td>\n",
       "      <td>-0.501366</td>\n",
       "      <td>-1.248377</td>\n",
       "      <td>0.123470</td>\n",
       "      <td>-1.424820</td>\n",
       "      <td>-0.899163</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>0.707105</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>1.413669</td>\n",
       "      <td>-1.389343</td>\n",
       "      <td>-0.588900</td>\n",
       "      <td>0.160290</td>\n",
       "      <td>1.494817</td>\n",
       "      <td>-1.296875</td>\n",
       "      <td>1.167127</td>\n",
       "      <td>-0.847587</td>\n",
       "      <td>-0.866727</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>0.707105</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>1.413669</td>\n",
       "      <td>-1.589207</td>\n",
       "      <td>-0.100847</td>\n",
       "      <td>0.504041</td>\n",
       "      <td>-0.044103</td>\n",
       "      <td>-1.431635</td>\n",
       "      <td>-0.403551</td>\n",
       "      <td>-0.064030</td>\n",
       "      <td>-0.839281</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>0.707105</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>1.413669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41281</th>\n",
       "      <td>1.173564</td>\n",
       "      <td>1.327393</td>\n",
       "      <td>-0.175488</td>\n",
       "      <td>-0.919043</td>\n",
       "      <td>0.143182</td>\n",
       "      <td>0.091736</td>\n",
       "      <td>-0.653957</td>\n",
       "      <td>0.093643</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>1.366022</td>\n",
       "      <td>-1.368961</td>\n",
       "      <td>-0.355847</td>\n",
       "      <td>1.013118</td>\n",
       "      <td>0.923018</td>\n",
       "      <td>-0.592118</td>\n",
       "      <td>-1.107595</td>\n",
       "      <td>0.671316</td>\n",
       "      <td>-0.860180</td>\n",
       "      <td>0.572367</td>\n",
       "      <td>0.280485</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>1.366022</td>\n",
       "      <td>-1.368961</td>\n",
       "      <td>-0.355847</td>\n",
       "      <td>1.177576</td>\n",
       "      <td>-0.649557</td>\n",
       "      <td>0.238414</td>\n",
       "      <td>0.257186</td>\n",
       "      <td>0.167853</td>\n",
       "      <td>0.694025</td>\n",
       "      <td>-0.973016</td>\n",
       "      <td>1.827672</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>1.366022</td>\n",
       "      <td>-1.368961</td>\n",
       "      <td>-0.355847</td>\n",
       "      <td>0.881833</td>\n",
       "      <td>-0.154268</td>\n",
       "      <td>-0.190047</td>\n",
       "      <td>-0.883157</td>\n",
       "      <td>-0.002714</td>\n",
       "      <td>-1.056228</td>\n",
       "      <td>-0.279055</td>\n",
       "      <td>-0.325991</td>\n",
       "      <td>3.660245e-01</td>\n",
       "      <td>1.366022</td>\n",
       "      <td>-1.368961</td>\n",
       "      <td>-0.355847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41282</th>\n",
       "      <td>1.172562</td>\n",
       "      <td>1.259997</td>\n",
       "      <td>-0.172312</td>\n",
       "      <td>-0.915094</td>\n",
       "      <td>0.106184</td>\n",
       "      <td>0.109243</td>\n",
       "      <td>-0.691607</td>\n",
       "      <td>0.066306</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>1.224742</td>\n",
       "      <td>-1.369215</td>\n",
       "      <td>-0.354866</td>\n",
       "      <td>0.998077</td>\n",
       "      <td>0.765756</td>\n",
       "      <td>-0.626175</td>\n",
       "      <td>-1.111544</td>\n",
       "      <td>0.585716</td>\n",
       "      <td>-0.839960</td>\n",
       "      <td>0.545474</td>\n",
       "      <td>0.013794</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>1.224742</td>\n",
       "      <td>-1.369215</td>\n",
       "      <td>-0.354866</td>\n",
       "      <td>1.161531</td>\n",
       "      <td>-0.806818</td>\n",
       "      <td>0.076758</td>\n",
       "      <td>0.262615</td>\n",
       "      <td>0.156240</td>\n",
       "      <td>0.763295</td>\n",
       "      <td>-0.955159</td>\n",
       "      <td>1.696780</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>1.224742</td>\n",
       "      <td>-1.369215</td>\n",
       "      <td>-0.354866</td>\n",
       "      <td>0.787135</td>\n",
       "      <td>-0.068091</td>\n",
       "      <td>-0.236341</td>\n",
       "      <td>-0.809628</td>\n",
       "      <td>-0.075252</td>\n",
       "      <td>-1.137870</td>\n",
       "      <td>-0.152700</td>\n",
       "      <td>-0.374598</td>\n",
       "      <td>7.071051e-01</td>\n",
       "      <td>1.224742</td>\n",
       "      <td>-1.369215</td>\n",
       "      <td>-0.354866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41283</th>\n",
       "      <td>1.156518</td>\n",
       "      <td>1.147661</td>\n",
       "      <td>-0.161413</td>\n",
       "      <td>-0.916081</td>\n",
       "      <td>0.141006</td>\n",
       "      <td>-0.040720</td>\n",
       "      <td>-0.711400</td>\n",
       "      <td>0.051942</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-1.369469</td>\n",
       "      <td>-0.353885</td>\n",
       "      <td>0.984037</td>\n",
       "      <td>0.586038</td>\n",
       "      <td>-0.661820</td>\n",
       "      <td>-1.099697</td>\n",
       "      <td>0.460931</td>\n",
       "      <td>-0.839960</td>\n",
       "      <td>0.471895</td>\n",
       "      <td>0.052215</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-1.369469</td>\n",
       "      <td>-0.353885</td>\n",
       "      <td>1.146489</td>\n",
       "      <td>-0.851744</td>\n",
       "      <td>-0.033131</td>\n",
       "      <td>0.261628</td>\n",
       "      <td>0.172929</td>\n",
       "      <td>0.774623</td>\n",
       "      <td>-0.951502</td>\n",
       "      <td>1.647435</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-1.369469</td>\n",
       "      <td>-0.353885</td>\n",
       "      <td>0.910474</td>\n",
       "      <td>-0.253643</td>\n",
       "      <td>-0.414961</td>\n",
       "      <td>-0.818247</td>\n",
       "      <td>0.041153</td>\n",
       "      <td>-0.981876</td>\n",
       "      <td>-0.177948</td>\n",
       "      <td>-0.385199</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-1.369469</td>\n",
       "      <td>-0.353885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41284</th>\n",
       "      <td>1.144484</td>\n",
       "      <td>1.170131</td>\n",
       "      <td>-0.150515</td>\n",
       "      <td>-0.922004</td>\n",
       "      <td>0.014054</td>\n",
       "      <td>0.047275</td>\n",
       "      <td>-0.761529</td>\n",
       "      <td>0.126771</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>0.707105</td>\n",
       "      <td>-1.369722</td>\n",
       "      <td>-0.352903</td>\n",
       "      <td>0.966991</td>\n",
       "      <td>0.395079</td>\n",
       "      <td>-0.723123</td>\n",
       "      <td>-1.093774</td>\n",
       "      <td>0.351391</td>\n",
       "      <td>-0.848104</td>\n",
       "      <td>0.619054</td>\n",
       "      <td>0.080158</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>0.707105</td>\n",
       "      <td>-1.369722</td>\n",
       "      <td>-0.352903</td>\n",
       "      <td>1.131447</td>\n",
       "      <td>-0.952838</td>\n",
       "      <td>-0.127128</td>\n",
       "      <td>0.260937</td>\n",
       "      <td>0.172206</td>\n",
       "      <td>0.851099</td>\n",
       "      <td>-0.929987</td>\n",
       "      <td>1.590445</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>0.707105</td>\n",
       "      <td>-1.369722</td>\n",
       "      <td>-0.352903</td>\n",
       "      <td>0.920593</td>\n",
       "      <td>-0.329609</td>\n",
       "      <td>-0.383732</td>\n",
       "      <td>-0.776735</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>-1.031615</td>\n",
       "      <td>-0.263660</td>\n",
       "      <td>-0.395460</td>\n",
       "      <td>1.224742e+00</td>\n",
       "      <td>0.707105</td>\n",
       "      <td>-1.369722</td>\n",
       "      <td>-0.352903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41285</th>\n",
       "      <td>1.136462</td>\n",
       "      <td>1.057809</td>\n",
       "      <td>-0.145973</td>\n",
       "      <td>-0.919043</td>\n",
       "      <td>0.030742</td>\n",
       "      <td>0.023686</td>\n",
       "      <td>-0.783044</td>\n",
       "      <td>0.181445</td>\n",
       "      <td>1.366022e+00</td>\n",
       "      <td>0.366025</td>\n",
       "      <td>-1.369975</td>\n",
       "      <td>-0.351921</td>\n",
       "      <td>0.948940</td>\n",
       "      <td>0.192891</td>\n",
       "      <td>-0.806450</td>\n",
       "      <td>-1.074031</td>\n",
       "      <td>0.314392</td>\n",
       "      <td>-0.854563</td>\n",
       "      <td>0.635189</td>\n",
       "      <td>0.016266</td>\n",
       "      <td>1.366022e+00</td>\n",
       "      <td>0.366025</td>\n",
       "      <td>-1.369975</td>\n",
       "      <td>-0.351921</td>\n",
       "      <td>1.116405</td>\n",
       "      <td>-0.952838</td>\n",
       "      <td>-0.216131</td>\n",
       "      <td>0.261036</td>\n",
       "      <td>0.217910</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>-0.901373</td>\n",
       "      <td>1.486658</td>\n",
       "      <td>1.366022e+00</td>\n",
       "      <td>0.366025</td>\n",
       "      <td>-1.369975</td>\n",
       "      <td>-0.351921</td>\n",
       "      <td>0.826482</td>\n",
       "      <td>-0.349724</td>\n",
       "      <td>-0.350119</td>\n",
       "      <td>-0.827089</td>\n",
       "      <td>-0.070874</td>\n",
       "      <td>-1.062078</td>\n",
       "      <td>-0.291240</td>\n",
       "      <td>-0.402846</td>\n",
       "      <td>1.366022e+00</td>\n",
       "      <td>0.366025</td>\n",
       "      <td>-1.369975</td>\n",
       "      <td>-0.351921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41286 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -1.506584 -0.781585  0.609397  1.684159 -0.388460  0.914476 -0.137610   \n",
       "1     -1.510089 -0.797194  0.583524  1.684159 -0.339509  0.940824 -0.116095   \n",
       "2     -1.514180 -0.801723  0.557911  1.679321 -0.397348  0.916532 -0.116095   \n",
       "3     -1.515415 -0.815917  0.552050  1.673497 -0.220514  0.915128 -0.116095   \n",
       "4     -1.518370 -0.796628  0.560990  1.676459 -0.406037  0.915223 -0.094581   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "41281  1.173564  1.327393 -0.175488 -0.919043  0.143182  0.091736 -0.653957   \n",
       "41282  1.172562  1.259997 -0.172312 -0.915094  0.106184  0.109243 -0.691607   \n",
       "41283  1.156518  1.147661 -0.161413 -0.916081  0.141006 -0.040720 -0.711400   \n",
       "41284  1.144484  1.170131 -0.150515 -0.922004  0.014054  0.047275 -0.761529   \n",
       "41285  1.136462  1.057809 -0.145973 -0.919043  0.030742  0.023686 -0.783044   \n",
       "\n",
       "             7             8         9         10        11        12  \\\n",
       "0     -0.525071  1.747686e-12  1.414210  0.002127  1.413681 -1.689783   \n",
       "1     -0.537748  3.660245e-01  1.366022  0.003141  1.413679 -1.703138   \n",
       "2     -0.548444  7.071051e-01  1.224742  0.004155  1.413677 -1.721294   \n",
       "3     -0.551085  9.999976e-01  0.999998  0.005168  1.413674 -1.736147   \n",
       "4     -0.542766  1.224742e+00  0.707105  0.006182  1.413669 -1.745111   \n",
       "...         ...           ...       ...       ...       ...       ...   \n",
       "41281  0.093643  3.660245e-01  1.366022 -1.368961 -0.355847  1.013118   \n",
       "41282  0.066306  7.071051e-01  1.224742 -1.369215 -0.354866  0.998077   \n",
       "41283  0.051942  9.999976e-01  0.999998 -1.369469 -0.353885  0.984037   \n",
       "41284  0.126771  1.224742e+00  0.707105 -1.369722 -0.352903  0.966991   \n",
       "41285  0.181445  1.366022e+00  0.366025 -1.369975 -0.351921  0.948940   \n",
       "\n",
       "             13        14        15        16        17        18        19  \\\n",
       "0     -0.004983  0.994299 -0.463754 -1.254349  0.019940 -1.432135 -0.901498   \n",
       "1     -0.056581  0.980826 -0.440851 -1.252129  0.066557 -1.446334 -0.901628   \n",
       "2     -0.082758  0.978333 -0.447071 -1.250806  0.057947 -1.503778 -0.900590   \n",
       "3     -0.104176  0.983347 -0.488828 -1.255655  0.106997 -1.514535 -0.899293   \n",
       "4     -0.117089  0.990270 -0.501366 -1.248377  0.123470 -1.424820 -0.899163   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "41281  0.923018 -0.592118 -1.107595  0.671316 -0.860180  0.572367  0.280485   \n",
       "41282  0.765756 -0.626175 -1.111544  0.585716 -0.839960  0.545474  0.013794   \n",
       "41283  0.586038 -0.661820 -1.099697  0.460931 -0.839960  0.471895  0.052215   \n",
       "41284  0.395079 -0.723123 -1.093774  0.351391 -0.848104  0.619054  0.080158   \n",
       "41285  0.192891 -0.806450 -1.074031  0.314392 -0.854563  0.635189  0.016266   \n",
       "\n",
       "                 20        21        22        23        24        25  \\\n",
       "0      1.747686e-12  1.414210  0.002127  1.413681 -1.356208 -0.588429   \n",
       "1      3.660245e-01  1.366022  0.003141  1.413679 -1.367382 -0.588860   \n",
       "2      7.071051e-01  1.224742  0.004155  1.413677 -1.375527 -0.588900   \n",
       "3      9.999976e-01  0.999998  0.005168  1.413674 -1.381628 -0.588456   \n",
       "4      1.224742e+00  0.707105  0.006182  1.413669 -1.389343 -0.588900   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "41281  3.660245e-01  1.366022 -1.368961 -0.355847  1.177576 -0.649557   \n",
       "41282  7.071051e-01  1.224742 -1.369215 -0.354866  1.161531 -0.806818   \n",
       "41283  9.999976e-01  0.999998 -1.369469 -0.353885  1.146489 -0.851744   \n",
       "41284  1.224742e+00  0.707105 -1.369722 -0.352903  1.131447 -0.952838   \n",
       "41285  1.366022e+00  0.366025 -1.369975 -0.351921  1.116405 -0.952838   \n",
       "\n",
       "             26        27        28        29        30        31  \\\n",
       "0      0.154454  1.470927 -1.231157  1.157017 -0.740015 -0.865323   \n",
       "1      0.147153  1.470137 -1.172248  1.150839 -0.761529 -0.863685   \n",
       "2      0.148847  1.459278 -1.273414  1.088776 -0.818973 -0.870471   \n",
       "3      0.155937  1.467275 -1.280091  1.089618 -0.847587 -0.868599   \n",
       "4      0.160290  1.494817 -1.296875  1.167127 -0.847587 -0.866727   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "41281  0.238414  0.257186  0.167853  0.694025 -0.973016  1.827672   \n",
       "41282  0.076758  0.262615  0.156240  0.763295 -0.955159  1.696780   \n",
       "41283 -0.033131  0.261628  0.172929  0.774623 -0.951502  1.647435   \n",
       "41284 -0.127128  0.260937  0.172206  0.851099 -0.929987  1.590445   \n",
       "41285 -0.216131  0.261036  0.217910  0.903333 -0.901373  1.486658   \n",
       "\n",
       "                 32        33        34        35        36        37  \\\n",
       "0      1.747686e-12  1.414210  0.002127  1.413681 -1.507453  0.396912   \n",
       "1      3.660245e-01  1.366022  0.003141  1.413679 -1.530273  0.212962   \n",
       "2      7.071051e-01  1.224742  0.004155  1.413677 -1.551150  0.065689   \n",
       "3      9.999976e-01  0.999998  0.005168  1.413674 -1.573655 -0.036403   \n",
       "4      1.224742e+00  0.707105  0.006182  1.413669 -1.589207 -0.100847   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "41281  3.660245e-01  1.366022 -1.368961 -0.355847  0.881833 -0.154268   \n",
       "41282  7.071051e-01  1.224742 -1.369215 -0.354866  0.787135 -0.068091   \n",
       "41283  9.999976e-01  0.999998 -1.369469 -0.353885  0.910474 -0.253643   \n",
       "41284  1.224742e+00  0.707105 -1.369722 -0.352903  0.920593 -0.329609   \n",
       "41285  1.366022e+00  0.366025 -1.369975 -0.351921  0.826482 -0.349724   \n",
       "\n",
       "             38        39        40        41        42        43  \\\n",
       "0      0.801488 -0.042523 -1.445851 -0.430887 -0.021002 -0.837544   \n",
       "1      0.685987 -0.009650 -1.419883 -0.440716 -0.030038 -0.838586   \n",
       "2      0.593695 -0.020410 -1.389266 -0.424052 -0.042516 -0.839976   \n",
       "3      0.538223 -0.068585 -1.483433 -0.405517 -0.051552 -0.839976   \n",
       "4      0.504041 -0.044103 -1.431635 -0.403551 -0.064030 -0.839281   \n",
       "...         ...       ...       ...       ...       ...       ...   \n",
       "41281 -0.190047 -0.883157 -0.002714 -1.056228 -0.279055 -0.325991   \n",
       "41282 -0.236341 -0.809628 -0.075252 -1.137870 -0.152700 -0.374598   \n",
       "41283 -0.414961 -0.818247  0.041153 -0.981876 -0.177948 -0.385199   \n",
       "41284 -0.383732 -0.776735 -0.012037 -1.031615 -0.263660 -0.395460   \n",
       "41285 -0.350119 -0.827089 -0.070874 -1.062078 -0.291240 -0.402846   \n",
       "\n",
       "                 44        45        46        47  \n",
       "0      1.747686e-12  1.414210  0.002127  1.413681  \n",
       "1      3.660245e-01  1.366022  0.003141  1.413679  \n",
       "2      7.071051e-01  1.224742  0.004155  1.413677  \n",
       "3      9.999976e-01  0.999998  0.005168  1.413674  \n",
       "4      1.224742e+00  0.707105  0.006182  1.413669  \n",
       "...             ...       ...       ...       ...  \n",
       "41281  3.660245e-01  1.366022 -1.368961 -0.355847  \n",
       "41282  7.071051e-01  1.224742 -1.369215 -0.354866  \n",
       "41283  9.999976e-01  0.999998 -1.369469 -0.353885  \n",
       "41284  1.224742e+00  0.707105 -1.369722 -0.352903  \n",
       "41285  1.366022e+00  0.366025 -1.369975 -0.351921  \n",
       "\n",
       "[41286 rows x 48 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41286, 48)\n",
      "5\n",
      "(1709, 168, 48)\n",
      "(1709, 120, 1)\n",
      "(1709, 120, 1)\n",
      "predictions.shape = (1709, 120, 1)\n",
      "pred_day.shape = (1709, 5, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19b215721354f5dafb26217d171f391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = 4.264279\n",
      "nse3 = [0.3554865]\n",
      "label_day[predict_day].shape = (5, 1)\n",
      "mean = 4.266996\n",
      "[12.418887]\n"
     ]
    }
   ],
   "source": [
    "val_nse['GRU'], val_pbias['GRU'], pred, label = compa(model=gru_model,\n",
    "                                                      df=test_df,\n",
    "                                                      plot_col=out_features[0], \n",
    "                                                      target_std=target_std, \n",
    "                                                      target_mean=target_mean,\n",
    "                                                      predict_day = 4)\n",
    "\n",
    "(val_nse['GRU'])\n",
    "print(val_pbias['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41286, 48)\n",
      "5\n",
      "(1709, 168, 48)\n",
      "(1709, 120, 1)\n",
      "(1709, 120, 1)\n",
      "predictions.shape = (1709, 120, 1)\n",
      "pred_day.shape = (1709, 5, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59df9a8b849043dc8ebf372b156689ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = 4.264279\n",
      "nse3 = [0.13001621]\n",
      "label_day[predict_day].shape = (5, 1)\n",
      "mean = 4.266996\n",
      "[15.465405]\n"
     ]
    }
   ],
   "source": [
    "val_nse['LSTM'], val_pbias['LSTM'], pred, label = compa(model=multi_lstm_model,\n",
    "                                                      df=test_df,\n",
    "                                                      plot_col=out_features[0], \n",
    "                                                      target_std=target_std, \n",
    "                                                      target_mean=target_mean,\n",
    "                                                      predict_day = 4)\n",
    "\n",
    "(val_nse['LSTM'])\n",
    "print(val_pbias['LSTM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41286, 48)\n",
      "5\n",
      "(1709, 168, 48)\n",
      "(1709, 120, 1)\n",
      "(1709, 120, 1)\n",
      "predictions.shape = (1709, 120, 1)\n",
      "pred_day.shape = (1709, 5, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3ab3506ce1484aa9a798362d413ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = 4.264279\n",
      "nse3 = [0.41306144]\n",
      "label_day[predict_day].shape = (5, 1)\n",
      "mean = 4.266996\n",
      "[10.212532]\n"
     ]
    }
   ],
   "source": [
    "val_nse['CNN'], val_pbias['CNN'], pred, label = compa(model=multi_conv_model,\n",
    "                                                      df=test_df,\n",
    "                                                      plot_col=out_features[0], \n",
    "                                                      target_std=target_std, \n",
    "                                                      target_mean=target_mean,\n",
    "                                                      predict_day = 4)\n",
    "\n",
    "(val_nse['CNN'])\n",
    "print(val_pbias['CNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42086, 48)\n",
      "4\n",
      "(1742, 168, 48)\n",
      "(1742, 120, 1)\n",
      "(1742, 120, 1)\n",
      "predictions.shape = (1742, 120, 1)\n",
      "pred_day.shape = (1742, 5, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7983f97472e649d0b49ed683e67b2102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = 4.2740355\n",
      "nse3 = [0.70065904]\n",
      "label_day[predict_day].shape = (5, 1)\n",
      "mean = 4.2766314\n",
      "\n",
      "\n",
      "[0.70065904]\n",
      "[0.6245054]\n"
     ]
    }
   ],
   "source": [
    "val_nse['GRU'], val_pbias['GRU'], pred, label = compa(\n",
    "    model=gru_model,df=train_df,\n",
    "    plot_col=out_features[0], target_std=target_std, target_mean=target_mean, predict_day = 4)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(val_nse['GRU'])\n",
    "print(val_pbias['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52608, 48)\n",
      "4\n",
      "(2180, 168, 48)\n",
      "(2180, 120, 1)\n",
      "(2180, 120, 1)\n",
      "predictions.shape = (2180, 120, 1)\n",
      "pred_day.shape = (2180, 5, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e922bc8878e4bad838c7d1a578d5dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1 = 4.27052\n",
      "nse3 = [0.6760868]\n",
      "label_day[predict_day].shape = (5, 1)\n",
      "mean = 4.2712\n",
      "\n",
      "\n",
      "[0.6760868]\n",
      "[0.57269627]\n"
     ]
    }
   ],
   "source": [
    "val_nse['GRU'], val_pbias['GRU'], pred, label = compa(\n",
    "    model=gru_model,df=real_df_all,\n",
    "    plot_col=out_features[0], target_std=target_std, target_mean=target_mean, predict_day = 4)\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(val_nse['GRU'])\n",
    "print(val_pbias['GRU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
